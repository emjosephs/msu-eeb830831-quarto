---
title: "P values"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
```

## What are we doing here?
For the last few weeks we've been building linear models to estimate population parameters. Now we'll discuss how to test for whether these estimates are **significant**. And, if you're wondering what we mean by significant, that's a good thing to wonder -- stay tuned.

As we work through this section we'll return to our penguin data. We'll start by thinking about how to tell if the difference in body mass between penguins of different species is significant.

We previously used a linear model to estimate the conditional mean body mass of each species using the following code:

```{r}
## filter out chinstraps and anything with missing data
penguins2 <- penguins |>
  filter(!is.na(body_mass) ,species != "Chinstrap") |> 
  droplevels()

model1 <- lm(body_mass ~ species, data=penguins2)
summary(model1)

plot(penguins2$body_mass, bty="n", ylab = "body mass", 
     col=penguins2$species, lwd=2)
legend('topleft',levels(penguins2$species),bty="n", pch=1, pt.lwd=2, col=palette())
```


```{r}
#| echo: false
#| column: margin
#| fig-alt: "Two penguins on a rock"
#| fig-cap: "Chinstrap Penguins -- Christopher Michel, CC BY 2.0 <https://creativecommons.org/licenses/by/2.0>, via Wikimedia Commons" 
library(knitr)
include_graphics("figs/Chinstrap-penguin.jpg")
```

## Learning goals
- Apply and interpret different ways to measure variation
- Define a scientific hypothesis, determine appropriate null hypotheses.
- Calculate and interpret p values for a linear model based on t and F distributions.
- Calculate and interpet p values from likelihood ratio tests.

## The population vs the sample recap

The population we are studying has true values and parameters. However, without measuring every individual in the population, we will not get to know these true parameters. Instead, we sample the population and use statistics to make inferences about the population.

The **sampling distribution** is the distribution of parameters we would get from sampling multiple times. It tells us about the amount of error generated from taking a sample. We need to know this to determine how much certainty we can get from the sample.

## Measuring variation

There are various ways to measure variation in a data set.

The **variance** is the expectation of the differences between individuals and the population mean. For a set of data points $X$ with mean $\mu$, $\text{Var}(X) = E[(X - \mu)^2]$. (Remember that the $E$, or 'expectation', is a fancy math way of describing the average). Practically we calculate this as $\frac{\Sigma (X - \mu)^2}{n-1}$ where $n$ is the number of individuals in the sample. 
[Dividing by $n-1$ and not $n$ deals with bias that arises because the sample mean is calculated from the sample, so all the points in the sample mean are a bit closer to it than they are to the true mean.]{.aside}

The code below will calculate the variance and standard deviation of our Adelie penguin body masses.

```{r}
myN <- penguins2 |> filter(species=="Adelie") |> count() #get n

AdelieVar <- penguins2 |> filter(species=="Adelie") |>#filter only Adelies
  mutate(diffs = (body_mass - mean(body_mass))^2)  |> #calculate sqaured difference between each penguin's body mass and mean body mass
  summarise(var = sum(diffs))/(myN-1) #get the expectation

AdelieVar
```

You can also use a built in R function var().

```{r}
penguins2 |> filter(species=="Adelie") |> select(body_mass) |> var()
```

The **standard deviation** is the square root of the variance. The standard deviation can be useful because it is measured in the same units as the original data (while the variance is in squared units). This property makes it a bit easier to interpret. The standard deviation is sometimes written as $\sigma$ compared to the variance, which is written as $\sigma^2$ so we can express the standard deviation kind of funnily as: $\sigma = \sqrt{\sigma^2}$. When we calculate the standard deviation of a sample, it is often refered to as $s$ to distinguish it from the true standard deviation of the population ($\sigma$). 

We can get the standard deviation by taking the square root of the variance we just calculated.

```{r}
sqrt(AdelieVar$var)
```

or by using the built in R function sd().

```{r}
adelieMasses <- penguins2 |> filter(species=="Adelie") |> select(body_mass) 
sd(adelieMasses$body_mass)
```

### Standard error

To quantify the variance in the sampling distribution, we need to account for the size of the sample. We expect larger samples to have less variation (due to the law of large numbers!).

The **standard error** is the standard deviation  adjusted for the sample size. So for a sample size of $n$ and a sample standard deviation of $s$, the standard error ($SE$) equals $\frac{s}{\sqrt{n-1}}$.

Here is code cor calculating the standard error of our sample of Adelie penguins.

```{r}
adelieMasses <- penguins2 |> filter(species=="Adelie") |> select(body_mass) 
sdAdelies = sd(adelieMasses$body_mass)/(sqrt(nrow(adelieMasses)-1))
sdAdelies
```

And here is a plot of how the standard error scales with sample size (from increasingly large samples of penguins)

```{r}
set.seed(1000)
sampleAdelies <- function(n){
  mySample = sample(adelieMasses$body_mass, size=n,replace=F)
  return(sd(mySample)/(sqrt(n-1)))
}

myStderrors <- sapply(2:150, sampleAdelies)

plot(2:150, myStderrors, col = palette()[2], bty="n", xlab="n", ylab = "standard error")
```

## Hypothesis testing
Much of statistics uses a hypothesis testing framework whether we try to distinguish between two possibilities: a null hypothesis and an alternative hypothesis. 

Statistical hypotheses differ from the scientific hypotheses you may be more familiar with. Scientific hypotheses make claims about phenomena and the processes that cause them. For example, a scientific hypothesis might be "Plants from the desert are adapted to drought conditions" while a statistical hypothesis would be "The mean fitness of desert genotypes is higher than the mean fitness of grassland genotypes in dry conditions". Part of designing a good experiment is figuring out how to link statistical hypotheses to scientific hypotheses, and then do the experiments to test the statistical hypotheses.

### Null hypothesis
The **null hypothesis** is a claim about a population parameter that tells us what we expect if the process from a scientific hypothesis is not occurring. The null hypothesis should be something that would be interesting to reject. In the previous example, a null hypothesis would be that plants from different environments do not have different fitnesses in dry conditions. We often abbreviate the null hypothesis as $H_0$.

A null hypothesis needs a corresponding **alternative hypothesis** ($H_A$). These alternative hypotheses are usually more biologically interesting. In our previous example, $H_A$ might be that "Desert genotypes have higher fitness than grassland genotypes in dry conditions". 

Not all science relies upon a null and alternative hypothesis testing framework -- this is just one potential way to do statistics. Some scientists may be more interested in making specific parameter estimates, or in exploring which of many factors are important for a process. It's also often good scientific strategy to set up experiments where instead of rejecting a null hypothesis, your results will let you distinguish among multiple interesting potential hypotheses (making your results exciting no matter what happens).

### Frameworks for rejecting the null hypothesis
Now, how do we distinguish betwen the null hypothesis and alternative hypothesis? We often use a **test statistic**, which is calculated from the data, and compare it to a **null distribution** of our expectation for the test statistic under the hypothesis. Next we'll lay out some of the statistics and distributions that are commonly used.

## T distribution 

So we have a null hypothesis and we have a sample mean. How do we test for whether our sample is expected under the null hypothesis?

The **t distribution** tells us about the distributions of sample means. We can use it to determine how likely it is that our sample mean falls into the distribution of sample means expected under the null hypothesis. 

We can use this distribution to calculate a **t value** describing the number of standard errors between our sample mean ($x_0$) and the mean of the null distribution ($\mu_0$). We calculate this with the following equation:

$$ t = \frac{\bar{x}-\mu_0}{SE_x}$$

Returning to our penguins, we might have a somewhat contrived null hypothesis that the mean body mass of Adelie penguins is 4000g and want to test to see if our sample mean is consistent with the null hypothesis. We can calculate $t$
using the following code:

```{r}
xbar = mean(adelieMasses$body_mass)
mu0 = 4000

myT = (xbar-mu0)/sdAdelies
myT
```

This tells us that the mean body mass of our Adelie penguin sample is 5.3 standard errors below 3500g. This kind of makes sense -- we can see that if we add the mean mass of the Adelie penguins in the sample to the product of our t statistic and the standard error of the Adelie masses, we get 3500.

```{r}
mean(adelieMasses$body_mass)-myT*sdAdelies
```

This seems pretty different! But how different?

### Degrees of freedom

First we need to figure out the *degrees of freedom* that help us interpret $t$. In this case, the degrees of freedom are equal to $n-1$ for a sample size of $n$. 

### Calculating a p value

*p values* tell us about the proportion of the null distribution that is more extreme than our observed test statistic. So in a one-tailed test, a p value of 0.01 tells us that 1\% of the null distribution is more extreme than the observed data. We can translate this into telling us that we expect to see data like ours 1\% of the time even if the null hypothesis is true.

To calculate a p value for a t statistic, we can use the pt() function. The pt() function is analagous to pnorm() and other related functions.

```{r}
df = nrow(adelieMasses)-1
p_val <- 2*pt(q = myT, df=df, lower.tail=T) 
p_val
```

Congrats -- you've just calculated a p value for a t test!

### P values in linear regression

You can use the same procedure to calculate the p values for the slope of a linear regression. Generally, our null expectation for a linear regression is that the slope of the regression line (or the effect size corresponding to our predictor variable) is zero. We can think of our null hypothesis being that there is no relationship between the predictor and the response variable so that the slope of the linear regression line is 0. Our alternative hypothesis is that there the predictor variable does predict the response variable, and the slope of the linear regression line is not equal to 0. 

Instead of testing for a difference in sample means, we are now testing for a difference between the slope of the regression line and 0. Fortunately, this value, divided by its standard error, will follow a t distribution and we can use the same procedure as above to calculate a p value.

Let's try an example, testing a model where the predictor variable is bill length and the response variable is body mass.

```{r}
penguins3 <- penguins |> filter(!is.na(body_mass)) #filter out NAs

model2 <- lm(body_mass ~ bill_len, data=penguins3)
model2
```

The effect size of bill length ($b$), which is the same as the slope of the regression line, is 87.42. But can we reject the null hypothesis that the slope is 0?

First we calculate the standard error of the slope ($SE_b$). 

$$ SE_b = \sqrt{\frac{\Sigma(Y_i-\hat{Y_i})^2}{\Sigma(X_i - \hat{X_i})^2}} \times \frac{1}{\sqrt{(n-2)}}$$

This equation is kind of gnarly. Here.s how I interpret it. $\Sigma(Y_i-\hat{Y_i})^2$ is the variance of the response variable, and the square root of it is the standard deviation of that response variable. Similarly, $\Sigma(X_i-\hat{X_i})^2$ is the variance of the explanatory variable the the square root of it is the standard deviation of the response variable. By dividing these two values we get the standard deviation of the slope. Finally, to get the standard error, we divide the standard deviation by the square root of the degrees of freedom, which in this case is $n-2$ since have used up two degrees of freedom in the calculation of slope and intercept.

```{r}
varY <- penguins3 |> mutate(diffs = (body_mass - mean(body_mass))^2) |>
  summarise(var=sum(diffs))
varX <- penguins3 |> mutate(diffs = (bill_len - mean(bill_len))^2) |>
  summarise(var=sum(diffs))
dfb <- nrow(penguins3)-2
SEb <- sqrt(varY/(varX*dfb))[[1]]

```

Now we can calculate t as the difference between our observed slope or effect size (b) and the mean of the null (0), divided by the standard error.

$$ t = \frac{b-0}{SE_b}$$

```{r}
myT = model2$coefficients[2][[1]]/SEb
myT
```

And then we calculate p from the t distribution.

```{r}
p_val <- 2*pt(q=myT, df=dfb, lower.tail=F)
p_val

```

### Using R built in functions
Of course, R has a built in function for doing this. 

```{r}
model1<-t.test(adelieMasses, mu=4000, alternative="two.sided")
model1$p.value
```

We can also do this for a linear model

```{r}
summary(model2)

summary(model2)[[4]][2,4] ##
```


### F distribution

### Likelihood ratio tests

Likelihood ratio tests compare the likelihood of the best model given the data with the likeliood of the model under the null hypothesis.

The difference in log likelihoods ($D$) will follow a $\chi ^2$ distribution with degrees of freedom equal to the number of parameters tested.


## Review questions

The following code looks at variability in penguin bill length. 

```{r}
myN <- nrow(penguins2)

sumSquares <- penguins2 |> 
  mutate(diffs = (bill_len - mean(bill_len))^2)  |> 
  summarise(sum(diffs))

valueA <- sqrt(sumSquares[1,1]/(myN-1))
valueB <- sumSquares[1,1]/(myN-1)
valueC <- sqrt(sumSquares[1,1]/(myN-1))/sqrt(myN-1)
```

Match 'valueA','valueB', and 'valueC' with the variance, standard deviation, and standard error.

