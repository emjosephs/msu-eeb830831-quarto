---
title: "Linear models with continuous variables"
bibliography: references.bib
format: html
editor: source
reference-location: margin
citation-location: margin
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library('DT')
library(LaCroixColoR)
palette(lacroix_palette("Mango", type = "discrete"))
library(broom)
```

## What are we doing here?

Many of the variables you are interested are continuous, not categorical. In this section we'll extend our linear models to incorporate continuous predictor variables.

## Motivating example

Continuing with the penguin example data, imagine that we are interested in understanding how bill length determines body mass -- do penguins with longer bills also have larger bodies?

```{r}
penguins2 <- penguins |> filter(!is.na(body_mass) & !is.na(bill_len)) ##filter out ones with NAs
plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])

```

```{r}
#| echo: false
#| column: margin
#| fig-alt: "A cartoon of two penguins wearing hats"
#| fig-cap: "But do the penguins have hats? (Jeanette Warmuth, Public domain, via Wikimedia Commons)" 
library(knitr)
include_graphics("figs/hatpenguins.png")
```


We can plot out the data and see that there is potentially a relationship between bill length and body mass. But, imagine we have a biological reason for wanting to model body mass as a function of bill length. How would we do that?

## Linear models with continuous predictors

Instead of letting the mean of the model vary between categories, we now build our model with the mean as a linear function of the predictor variable. This model is called **linear regression** and it has two components: the **deterministic function** and the **stochastic function**

### The deterministic function

The deterministic function describes how the explanatory variables relate to the conditional mean of the response variable. In this case, since this is a linear model, we are modelling the response variable as a line:

$$\hat{y}_i = b_0 + b_1 \times x_i$$
As before, $\hat{y_i}$ is the conditional mean of the response variable for an individual with value $x_i$, $b_0$ is the intercept or the conditional mean of an individual with a value of $x_i = 0$ (although this value will not be meaningful if it lies far outside the range of the data). 

However, we can think of $b_1$ now as the slope of the relationship between the predictor and response variable. This means that for every increase of 1 unit in $x$, $y$ increases by $b_1$.

### The stochastic function

The **stochastic function** explains how the residuals are distributed around the conditional means predicted by the deterministic function. Since we are working with regular linear models, we will assume that the residuals are normally distributed.

$$ e_i \sim N(0, \sigma^2)$$

We can combine the deterministic and stochastic functions into one model:

$$\hat{y_i} = b_0 + b_1 \times x_i + N(0,\sigma^2)$$

or

$$\hat{y_i} \sim N(b_0 + b_1 \times x_i, \sigma^2)$$

## Estimating parameters of the determinstic function

Here is some math for estimating these parameters.

First, we start with the *covariance* between our response variable $y$ and our explanatory variable $x$. The covariance tells us about how much $x$ and $y$ jointly deviate from their means -- a high covariance means that an individual with a high value of $x$ likely also has a high value of $y$ while a low covariance tells us that an individual with a high value of $x$ could have any value of $y$.

We write the covariance of $x$ and $y$ as $\text{cov}_{x,y}$ and calculate it using the following equation:

$$ \text{cov}_{x,y} = \frac{1}{n-1} \sum(x_i - \bar{x})\times(y_i-\bar{y})$$

where $n$ is the number of individuals, $x_i$ and $y_i$ are the values for the $i^{th}$ individual, and $\bar{x}$ and $\bar{y}$ are the mean values of $x$ and $y$. 

Notice that for a covariance, which variable is the explanatory variable and which is the response variable doesn't matter -- you could switch $x$ and $y$ and the equation would remain the same.

Covariance is related to a correlation ($r_{x,y}$), which tells us how reliably $x$ and $y$ covary by standardizing the covariance by how much $x$ and $y$ themselves vary, quantified using the standard deviations of $x$ ($s_x$) and $y$ ($s_y$. 

$$ r_{x,y} = \frac{cov_{x,y}}{s_{x} \times s_{y}}$$

Again, the order of variables doesn't matter. $r_{x,y} = r_{y,x}$

In contrast, linear regression distinguishes between the response and explanatory variables because, here, the goal is to build a model that reduces residuals.

However, the equation for calculating the slope of the linear regression line ($b_1$) looks somewhat similar to the correlation but this time we only standardize by $s_x$.

$$b_1 = \frac{cov_{x,y}}{s^2_x}  =  \frac{\frac{1}{(n-1)}\sum(x_i-\bar{x})(y_i-\bar{y})}{\frac{1}{(n-1)}\sum(x_i-\bar{x})^2}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}$$ 

Note that $x$ and $y$ are no longer interchangeable.

### Some notes on intuition

While the equations for correlation and for the regression coefficient are similar, it's possible to have a large correlation and a small regression slope (and vice versa). 

In general, lots of variation within $x$ will increase $s_x$ and reduce the regression coefficient.

```{r, echo=F}
set.seed(4)
x <- runif(100) #simulate the starting variable
y <- x # simulate the response, slope is 1

y1 <- y + rnorm(n=100,sd=.3) ## add in noise
y2 <- 1.6*x + rnorm(n=100,sd=.36)

#lm(y1~x)
#lm(y~x)
#cor.test(x,y1)
#cor.test(x,y2)
#lm(y2~x)

par(mfrow=c(1,3))
plot(x, y, bty="n", ylab="y", 
     main="(A) r=1, b=1", ylim=c(0,2.5), col="gray"
     )
abline(lm(y~x), col=palette()[2], lwd=2, lty=2)

plot(x, y1, bty="n", ylab="y", 
     main="(B) r=.75, b=1", ylim=c(0,2.5), col="gray"
     )
abline(lm(y1~x), col=palette()[2], lwd=2, lty=2)


plot(x, y2, bty="n", ylab="y", 
     main="(C) r=.75, b=1.5", ylim=c(0,2.5), col="gray"
     )
abline(lm(y2~x), col=palette()[2], lwd=2, lty=2)

```


### Fitting with lm()

Let's try applying a linear model to our penguin data. We'll use the lm() function in R.

$$\text{Body mass}_i = b_0 + b_1 \times \text{Bill length}_i $$

```{r}
model1 <- lm(body_mass ~ bill_len, data=penguins2)
model1
```

Based on these estimates, we can now write out our model as


$$\text{Body mass}_i = 362.31 + 87.42 \times \text{Bill length}_i $$

We can interpret this as for every mm of bill length increase, body mass increases by 87 grams.

We can also estimate $\sigma^2$, the variance of the residuals.

```{r}
sigma(model1)

```

This allows us to write our model including the stochastic function as:

$$ \text{Body mass}_i \sim N(362.31 + 87.42 \times \text{Bill length}_i, 645.43) $$ 

We can also plot the predictions from this model over the raw data.

```{r}
plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])
abline(model1, col=palette()[3], lwd=2)

```


```{r}
#| echo: false
#| column: margin
#| fig-alt: "A black and white photo of a group of penguins in a bowling alley, standing where the pins would be"
#| fig-cap: "Don't get any ideas (Bruce Raymond, United States Navy, credited photographer, Public domain, via Wikimedia Commons)" 
library(knitr)
include_graphics("figs/19610701_Penguin_bowling_pins_at_inauguration_of_McMurdo_Station,_Antarctica,_bowling_alley.jpg")
```


## Residuals

Often we assume that the response data needs to be normally distributed to use a linear model. However, the real assumption of the model is not that the response data is normally distributed, but that the residuals are normally distributed.

Let's look at the residuals from this model
```{r}
library(broom)
myResiduals <- augment(model1) |> select(body_mass, .fitted, .resid)

hist(myResiduals$.resid, main="", xlab = "residuals", border="white", col=palette()[2])
```

It helps me to imagine that the values themselves follow a normal distribution centered around a mean that follows the model predictions.

```{r, echo=F}

par(mfrow=c(1,2))

plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])
abline(model1, col=palette()[3], lwd=2)
abline(v=40, col=palette()[2], lty=2, lwd=2)
abline(v=47, col=palette()[4], lty=2, lwd=2)
abline(v=54, col=palette()[5], lty=2, lwd=2)



#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*40, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[2], freq=F, xlim=c(2000, 7000))

#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*47, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[4], freq=F, xlim=c(2000, 7000))

#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*54, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[5], freq=F, xlim=c(2000, 7000))


### OR

plot(-5,-5, xlim = c(2000,7000), ylim = c(0,0.0006), xlab = "body mass", ylab = "", bty="n")
lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*40,
									sd=sigma(model1)),
                  col = palette()[2], lwd=2, lty=2)

lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*47,
									sd=sigma(model1)),
                  col = palette()[4], lwd=2, lty=2)

lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*54,
									sd=sigma(model1)),
                  col = palette()[5], lwd=2, lty=2)

```


## A note on history: regression to the mean

Regression was initially developed by Francis Galton. Galton was interested in understanding heritability of traits: how much was an individual's trait determined by the genetic material they inherited from their parents. Galton focused on height because it was easy to measure. However, Galton's main motivation was understanding the heritability of intelligence. He was a strong proponent of Eugenics and had a lot of gross ideas. See [@kennedy2024teaching] for more on this.

![A plot from Galton's "Essays in eugenics" (1909) showing his conception of regression toward the mean](figs/regression-toward-the-mean1.png){fig-alt="A plot showing the distribution of heights of 1000 couples with plots below showing the distributions of offpsring heights for couples at different parts of the distribution. The title is 'Standard scheme of descent'" fig-align="center" width=50%}

Galton estimated heritability by calculating the slope of the relationship between the average height of two parents and their offspring. He was surprised to see that the very tallest parents tended to have offspring that were shorter than expected and the shortest parents had offspring that were taller than expected[@stigler1997regression]. He called this "regression towards the mean" and it is this phrase that lends it's name to "linear regression" since it occurs in many contexts, not just the inheritance of traits. Galton thought that he had discovered a fundamental property of evolution that preserved a consistent amount of genetic variation across generations. Regression toward the mean has continued to be a confusing topic for many (including me!) so I want to walk through it in the hopes that it will help us better understand regression in general.

However, regression to the mean is just the result of measurement error. We can imagine that every value in a data set is the sum of some true value and measurement error. In quantitative genetics, we think about this a lot -- traits are the sum of a genetic component and an environmental component (or error). If we take the extremes of a distribution we will get individuals who have both extreme true values and extreme values of error. So in Galton's first example, the tallest parents both had the tallest genetic component and the tallest environmental component (along with any other types of error). They transmit on their genetics but not their environment and other errors, so their offspring will generally be shorter. 

Another example that may be helpful is thinking about exam grades. Large undergrad classes often give multiple midterm exams with the goal of measuring how well students understand the course material. We could imagine that in the first midterm, the students with the very best grades are not only the ones with the best understanding of the material, but also the ones that happened to get the best night's sleep, weren't sick, aren't distracted by a personal issue, etc. In the second midterm, these students may still understand the material but may not have the same environmental benefits and so, on average, the best students will seem to do slightly less well on the second midterm (and the worst students will appear to do slightly better). [For more on regression to the mean, [check out this podcast](https://quantitudepod.org/s5e14-regression-to-the-mean/)]{.aside}

Any time you do a regression with two quantities that have measurement error, you will likely find regression to the mean. This process can have real consequences for studies of the effect of interventions. For example, if a researcher identifies individuals with extremely poor values of some health-related measurement, and then gives these individuals an intervention, regression towards the mean will cause these individuals to appear to improve in the next round of measurement [@barnett2005regression]. This is why experimental controls are really important!

## Assessing significance

We have different tools for quantifying how confident we can be in the predictions made by a regression.
They vary based on what type of prediction they are making.
We won't work through the math here but it is useful to know what is actually being predicted by these tools.

### Confidence intervals

Confidence intervals relate to predicting the mean value of the response variable $y$ for individuals with a specific value of the explanatory variable $x$. When we calculate **95% confidence intervals** we are bracketing the true regression line 95% of the time. Specifically, this means that if we repeated our sampling, analysis, and confidence interval generation many many times, we would find the true slope 95% of the time. [For more on the history of confidence intervals, see this [nice blog post](https://kucharski.substack.com/p/to-understand-mathematical-concepts), which includes the very relatable quote from Jerzy Neyman: "I simply cannot work, the crisis and the struggle for existence takes all my time and energy."]{.aside}

### Prediction intervals

Prediction intervals give us an estimate of our confidence in predicting a value for the response variable $y$ for *one* individual with a specific value of the explanatory variable $x$.
When we generate **95% prediction intervals** we are bracketing 95% of potential individuals.

Prediction intervals will be much wider than confidence intervals because they include more variation.

### Extrapolation

*Extrapolation* is the prediction of $y$ outside the measured interval of $x$. It's generally a bad idea because we have no way of knowing the relationship between $x$ and $y$ outside of what we've observed.

 ![](figs/extrapolating.png){fig-alt="A carton from xkcd showing a plot with time on the x axis and number of husbands on the y axis with values for 0 and 1 and a line extrapolating. The title reads 'my hobby: extrapolating' and one person tells another person dressed as a bride 'As you can see, by late next month you'll have over four dozen husbands. Better get a bulk rate on a wedding cake'"}

## Linear regression as an ANOVA

We can partition variation in our regression the same way we did in ANOVA. The concepts are similar but the math is slightly different.

```{r varpartregresion, fig.cap = "An ANOVA framework for a linear regression. **A** Shows the difference between each observation, $Y_i$, and the  mean, $\\overline{Y}$. This is the basis for calculating $MS_{total}$.  **B** Shows the difference between each predicted value $\\widehat{Y_i}$ and the mean, $\\overline{Y}$. This is the basis for calculating $MS_{model}$. **C** Shows the difference between each observation, $Y_i$, and its predicted value  $\\widehat{Y_i}$. This is the basis for calculating $MS_{error}$.", fig.height=2.3, fig.width=7.6, echo=FALSE, warning=F, message=F}
library(cowplot)
a <- ggplot(mutate(penguins2, mean_mass = mean(body_mass)) , 
            aes(x = bill_len, y = body_mass))+
  geom_point(alpha = .5)+
  geom_hline(aes(yintercept = mean_mass))+
  geom_segment(aes(xend = bill_len, yend = mean_mass), 
               color = "black", alpha = .5)+
  theme_classic()+ 
  labs(title = "Total deviation               =")

b<-ggplot(mutate(augment(model1), mean_mass = mean(body_mass)) ,
          aes(x = bill_len, y = body_mass))+
  geom_point(alpha = .2)+
  geom_hline(aes(yintercept = mean_mass))+
  geom_segment(aes( xend = bill_len,y = .fitted,  yend = mean_mass ), 
               color = "black", alpha = .5)+
  geom_line(aes(y = .fitted), alpha  = 2, color = palette()[2])+
 theme_classic()+ 
  labs(title = "Model deviation             +")

c<-ggplot(augment(model1) ,
          aes(x = bill_len, y = body_mass))+
  geom_point(alpha = .5)+
  geom_segment(aes( xend = bill_len,  yend = .fitted), 
               color = "black", alpha = .5)+
    geom_line(aes(y = .fitted), alpha  = 2, color = palette()[2])+ 
  theme_classic()+ 
  labs(title = "Error (residual) deviation")

plot_grid(plot_grid(a + theme(legend.position = "none"),
          b + theme(legend.position = "none"),
          c + theme(legend.position = "none"), ncol = 3, labels = c("a","b","c"), label_y = .9),
           get_legend(a+theme(legend.position = "bottom")), 
                      ncol = 1, rel_heights = c(1,.1))
```

As before, we can calculate the total sum of squares ($SS_{total}$, panel A) as the sum of the squared differences between the model and the mean ($SS_{model}$, panel B) and the squared differences between each individual and the model ($SS_{residual}$). Better fitting models will have higher $SS_{model}$ relative to $SS_{residual}$.

We calculate all of these values as follows:
$$SS_{total} = \sum{(y_i - \bar{y})^2}$$
$$SS_{model} = \sum{(\hat{y_i} - \bar{y})^2}$$
$$SS_{residual} = \sum{(y_i - \bar{y})^2} $$
$$MS_{model} = SS_{model}/df_{model}$$
$$MS_{residual} =  SS_{residual}/df_{residual}$$
$$df_{model} = 1 $$
$$df_{residual} = n-2$$

where $\bar{y}$ is the mean response variable for the whole sample and $\hat{y_i}$ is the model predicted value for individual $i$ and there are $n$ individuals in the sample.

We can calculate these values by hand or get them from the anova function

```{r, message=F, warning=F}
library(car)

anova(model1)
```

## Review questions

1. Imagine you are a penguin researcher and you measure additional data on lifespan. You want to construct a linear model that models lifespan as a function of body mass. Write out the equation describing this model, and describe which parts of the equation correspond to the stochastic function vs the deterministic function.

2. What are the key differences between covariance, correlation, and linear regression?

3. What is the key assumption about the normal distribution made in a linear regression?

4. Your friend shows you an analysis of some new data: they have conducted a regression and calculated a 95% confidence interval. They tell you "95% of my data points should fall in this confidence interval." Do you agree with them? Why or why not?



