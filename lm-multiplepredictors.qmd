---
title: "Linear models with multiple predictors"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library(gridExtra)
library('DT')
library(LaCroixColoR)
library(car)
palette(lacroix_palette("Mango", type = "discrete"))


```

## What are we doing here?

Now that we've learned the basics of how to build models with categorical and continuous variables, we can start to combine them into a **general linear model**. 

But, before we start to build more complicated models, we should go over the assumptions of a linear model.

## Linear model assumptions

### Linearity
We assume that we can model our response variable by adding up the values in the model equation. The alternative could be multiplying variables together, dividing them, or putting a variable in the exponent (or other things).

The plots below show data that violates this assumption because it was simulated using an exponential equation of the form $y_i = b^{x_i + e_i}$. On the left you can see that the linear regression line does not fit the curve very well and on the right it is clear that the residuals are not evenly distributed across the values of Y.

```{r, echo=F}
x = runif(200,min=0, max=100)
mydat = data.frame(explanatory = x, response = 1.05^(x+rnorm(200,0,1)))

mymodel <- lm(response~explanatory, data=mydat)
mydat$residuals <- mymodel$residuals

p1 <- ggplot(mydat, aes(x = explanatory, y =response)) + 
  geom_point() +
  stat_smooth(formula = y ~ x,method = "lm") +
  theme_classic()

p2 <- ggplot(mydat, aes(x=explanatory, y = residuals))+
  geom_point()+
  geom_hline(yintercept=0) +
  theme_classic()

grid.arrange(p1, p2, nrow = 1)
```


### Homoschedasticity
We assume that the variance of our residuals is the same across all of our data. If this is not true and the variance changes across residuals, then our data would be heteroschedastic. The two plots below show data that is heteroschedastic. On left you can see an explanatory and response variable plotted with a regression line. On right is a plot of the residuals of the regression and it is clear that the variance of the residual increases as the fitted value increases. In fact, this data was simulated such that the variance of the residual for each point is a function of the explanatory variable.

```{r, echo=F}
set.seed(830)


err <- sapply(1:200, function(x){rnorm(1,0,5*x)})
mydat = data.frame(explanatory = 1:200, response = 3.8*x+25+err)

mymodel <- lm(response~explanatory, data=mydat)
mydat$residuals <- mymodel$residuals

p1 <- ggplot(mydat, aes(x = explanatory, y =response)) + 
  geom_point() +
  stat_smooth(formula = y ~ x,method = "lm") +
  theme_classic()

p2 <- ggplot(mydat, aes(x=explanatory, y = residuals))+
  geom_point()+
  geom_hline(yintercept=0) +
  theme_classic()

grid.arrange(p1, p2, nrow = 1)
```

### Independence of units
We assume that the observations of data point are independent from each other. Another way of saying this is that we assume that the residuals are independent from each other.

### Distribution 
For general (not generalized) linear models, we assume that our residuals are drawn from a single normal distribution. This somewhat overlaps with some of the previous assumptions -- for example, in heteroschedastic data, the variance of the distribution of the residuals differes across the range of the data.

### Looking for deviations from assumptions
As shown above for some of these cases, you can often diagnose problems with a model fit caused by violations of assumptions by looking at a plot of the residuals against the explanatory variable. You can make these and other diagnostic plots by using the plot() function on the output of lm(). 

### What if your data doesn't meet these assumptions?
You have options.

1. You can transform your data
2. You can pick a new model
3. You can continue onward. Linear models (particularly ANOVAs) tend to be robust to assumption breaking, but only to a point

## Motivating example

In the previous sections, we found that both species and bill length affect body mass. Can we build a model that incorporates both?

To make things a bit simpler we will only look at Adelie and Gentoo penguins


```{r}
penguins2 <- penguins |> filter(!is.na(body_mass) & !is.na(bill_len) & !species=="Adelie") |> droplevels() ##filter out missing data and chinstrap


plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = penguins2$species)
legend('topleft',levels(penguins2$species),bty="n", pch=1, pt.lwd=2, col=palette())

```

Examining the data, we can generate some hypotheses about how bill length and species relate to body mass.


## Building the model

Let's think about a model of some response variable ($y$) based on two explanatory variables, $x_1$ and $y_1$.

Our model looks like it did before, except now it has multiple predictors, each with its own coefficient.

$$ y_i = b_0 + b_1x_{1i} + b_2x_{2i} + e_i$$

in our penguin example

$$ \text{Body mass} = b_0 + b_1 \text{Gentoo} + b_2 \text{bill length} + e_i $$

We will revisit what these terms mean after fitting the model.


## Fitting with lm()

```{r}
model1 <- lm(body_mass~species+bill_len,data=penguins2)
model1
```

We can now rewrite our equation with these estimated values

$$ \text{Body mass} = -648.533 + 1462.168 \times \text{Gentoo} + 89.725 \times \text{bill length} + e_i $$

Interpretting what $b_0$, $b_1$, and $b_2$ are is slightly more complicated now. First, we can see that $b_1$ is not exactly the difference in means between Gentoo and Chinstrap penguins. 

```{r}
penguins2 |> dplyr::group_by(species) |> summarise(mean_body_mass = mean(body_mass))
5076-3733
```

Similarly, $b_2$ is actually about twice as large as the coefficient you would get if you ran a linear model without species.

```{r}
mymodel2 <- lm(body_mass ~ bill_len, data=penguins2)
mymodel2
```

Linear models are about making predictions, so what these coefficients tell us is the predicted value of body mass for a specific bill length and species value. So, within a specific species, we expect that increasing bill length by one unit will increase body mass by approx. 90 units. It may be helpful to look at the data again with the lines from the model fits for linear models conducted within each species, along with the fits for a model without species and the total model. We can see that the slope of the line for the total model matches the slope of the lines for the within-species models.

```{r, warning=F, message=F}
mymodel3 <- lm(body_mass ~ bill_len, data=dplyr::filter(penguins2,species=="Gentoo"))
mymodel4 <- lm(body_mass ~ bill_len, data=dplyr::filter(penguins2,species=="Chinstrap"))

plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = penguins2$species, xlim = c(40,65))
#legend('topleft',levels(penguins2$species),bty="n", pch=1, pt.lwd=2, col=palette())
abline(mymodel3, col=palette()[2], lwd=2)
abline(mymodel4, col=palette()[1], lwd=2)
abline(mymodel2, col="gray", lwd=2, lty=2)
abline(a=-267.623, b=102.298, col="black", lwd=2, lty=2)

legend("bottomright", c('Gentoo','Chinstrap','no species',"full model"), bty="n", col=c(palette()[2],palette()[1], "gray","black"), lwd=2, lty=c(1,1,2,2))
```


Now that we are working with more complicated models, this is also a good time to get in the habit of using Anova() to calculate p values instead of summary(). In this case they provide the same values, but in some cases they will not.

```{r}
library(car)
Anova(model1)
```

By the way, general models with a continuous variable and a categorical variable are sometimes called **Analysis of covariance** or **ANCOVA** for short.

## Multiple regression

We can also build a general linear model with multiple continuous variables. This is sometimes called **multiple regression**.

Any general linear model with multiple continuous variables also assumes **limited multicollinearity**: Explanatory variables are not highly correlated with each other. If this assumption is violated, it can be hard to interpret the results of the model.

```{r}

model5 <- lm(body_mass ~  bill_dep + flipper_len + bill_len, data=penguins )
model5
Anova(model5)
```

We can check for multicollinearity using the ggpairs() function from the GGally package.

```{r, warning=FALSE, message=F}
#install.packages('GGally')
library(GGally)

multregData <- dplyr::select(penguins, body_mass, bill_dep, flipper_len, bill_len)
ggpairs(multregData)+theme_classic()
```

These variables are all pretty correlated with each other, which could create issues in the future.

Since we have multiple different explanatory variables, we will instead compare the residuals to the fitted values and look at the different values.
```{r}
par(mfrow=c(1,2))

plot(model5$fitted.values,model5$residuals, xlab = "yhat", ylab = "residuals", bty="n")
abline(h=0, lty=2, col="gray60", lwd=2)

hist(model5$residuals, main="", xlab = "residuals")
```

It does look like our residuals are normally distributed and the points are for the most part evenly spaced around 0 across the distribution of yhat. There are some clumps, which I think is the result of not including species and/or sex in the model, since we know that different species and different sexes have very different body masses.

## Interactions

Statistical interactions are everywhere in biology. In my lab, we're often interested in genotype-by-environment interactions, where genotypes differ in how they respond to the environment. For example, the image below shows a genotype-by-environment interaction for two *Arabidopsis thaliana* genotypes, where one shows a strong response to environment and the other does not.

![](figs/gxe-plants.png){fig-alt="A plot of leaf area for two genotypes, where one (547) has no response to environment, and another (947) has lower leaf area in hot,dry than cool, wet."}



I noted before that our model assumed that the slope of the relationship between bill length and body mass is the same for both species. This may not always be the case.

We can extend our model to incorporate differences in slopes between categories (or between individuals with different values for a continuous variable). This pattern is called a **statistical interaction**.

$$ y_i = b_0 + b_1x_{1i} + b_2x_{2i} + b_3 x_{1i} \times x_{2i} + e_i$$

in our penguin example

$$ \text{Body mass} = b_0 + b_1 \text{Gentoo} + b_2 \text{bill length} + b_3 \text{Gentoo} \times \text{bill length} + e_i $$

We can fit the model with lm() using the following code:

```{r}
model2 <- lm(body_mass~bill_len+species+bill_len*species,data=penguins2)
summary(model2)

```

In this case the statistical interaction isn't significant, suggesting that the slope of the relationship between bill length and mass does not differ between species.


## Matrix notation

It might be helpful for some of you to see a linear model written out in matrix notation. 
The equation of three matrices below describes a linear model.

\begin{equation} 
\begin{pmatrix}
    1 & x_{1,1} & x_{2,1} & \dots  & x_{k,1} \\
    1 & x_{1,2} & x_{2,2} & \dots  & x_{k,2} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{1,n} & x_{2,n} & \dots  & x_{k,n}
\end{pmatrix}
\cdot 
\begin{pmatrix}
    b_0 \\
    b_{1}\\
    b_{2}\\
    \vdots  \\
     b_{k}
\end{pmatrix}
=
\begin{pmatrix}
    \hat{Y_1} \\
    \hat{Y_2}\\
    \vdots  \\
    \hat{Y_n}
\end{pmatrix}
\end{equation}

The leftmost matrix is a *design matrix* where each row corresponds to an individual $i$ and each column corresponds to an explanatory variable. 
A cell in the $i^{th}$ row and $j^{th}$ column corresponds to individual $i$'s value for the $j^{th}$ explanatory variable. 
The first row corresponds to the intercept so all individuals have a value of 1.

The middle matrix is a list of the estimated effect sizes for each corresponding explanatory variable.

The dot product of these two matrices gives us the predicted value for the response variable for each individual.




:::fyi
**General vs Generalized linear models**

These are often confused!
:::
