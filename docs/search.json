[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EEB 830/831",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Chapter 1 – Introduction",
    "section": "",
    "text": "1.1 Hi!\nHi! Welcome to the ebook for IBIO/PLB/ENT 830, the introductory statistics course in Michigan State University’s Ecology and Evolutionary Biology Graduate program.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-goals",
    "href": "01-intro.html#learning-goals",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.2 Learning Goals",
    "text": "1.2 Learning Goals\nCourse learning goals go here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#themes",
    "href": "01-intro.html#themes",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.3 Themes",
    "text": "1.3 Themes\nThroughout the course we’ll be talking about the following themes…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#who-we-are",
    "href": "01-intro.html#who-we-are",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.4 Who we are",
    "text": "1.4 Who we are\nStuff about the professors goes here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#how-this-book-works",
    "href": "01-intro.html#how-this-book-works",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.5 How this book works",
    "text": "1.5 How this book works\nStuff about logistics goes here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html",
    "href": "02-likelihood.html",
    "title": "2  Chapter 2 – Likelihood",
    "section": "",
    "text": "2.1 What are we doing here?\nIn this chapter, we’ll talk about the concept of likelihood and how we can use it to make statistical inferences from our data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#motivating-question",
    "href": "02-likelihood.html#motivating-question",
    "title": "2  Chapter 2 – Likelihood",
    "section": "2.2 Motivating question",
    "text": "2.2 Motivating question\nSomeone hands us a coin and we flip it 100 times and get 46 heads.\nIs the coin fair? (probability of heads = 50%)\nWhat is the most likely probability of flipping heads for this coin?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#likelihood",
    "href": "02-likelihood.html#likelihood",
    "title": "2  Chapter 2 – Likelihood",
    "section": "2.3 Likelihood",
    "text": "2.3 Likelihood\nThe likelihood of the data is the probability of the data as a function of some unknown parameters.\nLikelihood is probability…in reverse!\nIn probability, we think about some stochastic process, and figure out ways to calculate the probability of possible outcomes\nFor example: given that the coin is fair, what’s the probability of getting 46 heads out of 100 flips?\nIn calculating probabilities, we consider a single parameter value and describe the probabilities of all possible outcomes of a process parameterized by that value.\nIn statistics, we start with some observed outcomes, and try to figure out the underlying process.\nFor example: given some flip data, can we figure out the fairness of the coin?\nIn calculating likelihoods, we consider a single outcome (or set of outcomes) and many possible parameter values that could best explain it.\nIn formulating the problem this way, we are treating the observed data (\\(k=46\\)) as a known, and treating \\(p\\) as an unknown parameter of the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#parametric-statistics",
    "href": "02-likelihood.html#parametric-statistics",
    "title": "2  Chapter 2 – Likelihood",
    "section": "2.4 Parametric statistics",
    "text": "2.4 Parametric statistics\nWe can use likelihood in a framework of parametric statistics.\nIn parametric inference, we treat observed data as draws from an underlying process or population, and we try to learn about that population from our sample.\nA statistical parameter is a value that tells you something about a population, like the true mean height of students in our class, or the true frequency of a genetic variant in a species.\nBUT, we rarely get to know the truth!\n(we would know the truth if we censused everyone in a population, or repeated a random process an infinite number of times)\nInstead, we can take samples to try to learn about –\nor estimate – parameters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "02.5-pvalues.html",
    "href": "02.5-pvalues.html",
    "title": "3  P values",
    "section": "",
    "text": "3.1 What are we doing here?\nIn this chapter, we’ll talk about the concept of likelihood and how we can use it to make statistical inferences from our data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "02.5-pvalues.html#motivating-question",
    "href": "02.5-pvalues.html#motivating-question",
    "title": "3  P values",
    "section": "3.2 Motivating question",
    "text": "3.2 Motivating question\nSomeone hands us a coin and we flip it 100 times and get 46 heads.\nIs the coin fair? (probability of heads = 50%)\nWhat is the most likely probability of flipping heads for this coin?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "02.5-pvalues.html#degrees-of-freedom",
    "href": "02.5-pvalues.html#degrees-of-freedom",
    "title": "3  P values",
    "section": "3.3 Degrees of freedom",
    "text": "3.3 Degrees of freedom",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html",
    "href": "03-linearModels1.html",
    "title": "4  Intro to Linear Models",
    "section": "",
    "text": "4.1 What are we doing here?\nIn this chapter, we will meet the linear model and discuss what it is exactly that we are doing when we build linear models.\nWe’ll build the most basic type of linear model by fitting the mean of a distribution.\nWe’ll also learn to interpret residuals and visualize them with R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#linear-models",
    "href": "03-linearModels1.html#linear-models",
    "title": "4  Intro to Linear Models",
    "section": "4.2 Linear models",
    "text": "4.2 Linear models\nAs biologists, we often want to understand if one thing causes another thing.\n\nDo traits determine an organism’s fitness? In other words, are traits under selection?\nDoes genotype at this locus determine phenotype?\nDo the amount of resources shape community diversity?\nget Lauren et al to fill in some other questions\n\nTo answer these types questions, we need a way to carefully relate two variables together. We will refer throughout this course to two types of variables:\n\nExplanatory variables, also called independent variables or predictor variables.\nResponse variables, also called dependent variables.\n\nLinear models estimate the conditional mean of the \\(i^{th}\\) observation of a continuous response variable, \\(\\hat{Y}_i\\) from a (combination) of value(s) of the explanatory variables (\\(\\text{explanatory variables}_i\\)):\n\\[\\begin{equation}\n\\hat{Y}_i = f(\\text{explanatory variables}_i)\n\\end{equation}\\]\nThese models are “linear” because we estimate of the conditional mean (\\(\\hat{Y}_i\\)) by adding up all components of the model. So, each explanatory variable \\(y_{j,i}\\) is multiplied by its effect size \\(b_j\\). It might help to look at an example model below:\n\\[\\begin{equation}\n\\hat{Y}_i = a + b_1  x_{1,i} + b_2 x_{2,i} + \\dots{}\n\\end{equation}\\]\nIn this example, \\(\\hat{Y}_i\\) is estimated as the sum of the “intercept” (\\(a\\)), its value for the first explanatory variable (\\(y_{1,i}\\)) times the effect of this variable (\\(b_1\\)), its value for the second explanatory variable (\\(y_{2,i}\\)) times the effect of this variable, \\(b_2\\), and so on for all included explanatory variables.\nIn practice, fitting a linear model requires picking explanatory variables and then estimating the values of \\(a\\), \\(b_1\\), \\(b_2\\), and so on that best predict the response variables. These estimates then tell us how the explanatory variables relate to the response variable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#the-mean",
    "href": "03-linearModels1.html#the-mean",
    "title": "4  Intro to Linear Models",
    "section": "4.3 The mean",
    "text": "4.3 The mean\nWe are going to start with the simplest linear model possible. You likely already know how to calculate the mean (\\(\\overline{y}\\)) of a set of data: \\(\\overline{y} = \\frac{\\sum y_i}{n}\\) where \\(y_i\\) is each data point and \\(n\\) is the number of samples.\nIn the simplest linear model we can also think of the mean as the intercept (\\(b_0\\)) so we can predict each data point \\(y_i\\) as simply the mean plus an error term or residual (\\(e_i\\)).\n\\[\\hat{y}_i = b_0 + e_i\\]\n\n4.3.1 Wait, what’s a residual?\nObserved values often differ from the predictions made by a linear model.\nWe define a residual (\\(e_i\\)) as the difference between an observed value(\\(Y_i\\)) and its predicted value from a linear model (\\(\\hat{y}_i\\)).\n\\[e_i = y_i - \\hat{y}_i\\]\nYou can also rearrange this to think about it the other way around so that the observed variable (\\(Y_i\\)) is the sum of the value predicted by the model (\\(\\hat{y}_i\\)) and the residual \\(e_i\\).\n\\[y_i = \\hat{y}_i + e_i\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#fitting-a-linear-model-with-maximum-likelihood",
    "href": "03-linearModels1.html#fitting-a-linear-model-with-maximum-likelihood",
    "title": "4  Intro to Linear Models",
    "section": "4.4 Fitting a linear model with maximum likelihood",
    "text": "4.4 Fitting a linear model with maximum likelihood\nWe will use the principals of likelihood to pick the parameters that best fit the linear model.\nThroughout this section we’ll be using a dataset of penguin traits. This data should be already available in your version of R, but if it isn’t, use the following code to install it.\n\ninstall.packages(\"palmerpenguins\")\nlibrary(\"palmerpenguins\")\n\nYou can read more about all the variables available in the package\n\n?penguins\n\nWe’ll start by thinking about body size. We want to model each penguin’s body mass as the sum of a mean body mass and a residual.\n\\[\\text{penguin body mass} = \\text{mean body mass} + e_i\\]\n\n4.4.1 The distribution of residuals\nA key piece of our linear model is that the residuals follow a specific distribution. In this part of the course we will focus on the normal distribution since it is broadly useful, but you could use any distribution you want. Models with residuals that follow non-normal distributions are called Generalized Linear Models\nIn math terms, we would write\n\\[ e_i \\sim N(0, \\sigma^2)\\] where \\(\\sigma^2\\) is the variance of the residuals. The mean of the residuals is 0.\n\nRemember that the standard error is the standard deviation divided by the square root of the sample size: \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\)\n\nWe can combine the equation for a linear model with the equation describing the distribution of the variables and get\n\\[y_i = \\hat{y}_i + N(0, \\sigma^2)\\]\nand this simplifies to\n\\[ y_i \\sim N(\\hat{y}_i, \\sigma^2)\\]\nFor our penguin example\n\\[ \\text{penguin body mass} \\sim N(\\text{mean body mass}, \\sigma^2)\\]\n\n\n4.4.2 The likelihood of one data point given specific parameters\nWe can use the dnorm() function in R to calculate the probability of observing a specific penguin body mass given known parameters.\nLet’s pick a random penguin\n\nset.seed(14)\nmyPenguin = penguins[sample(1:nrow(penguins), 1),]\n\nmyPenguin\n\n    species island bill_len bill_dep flipper_len body_mass    sex year\n265  Gentoo Biscoe     43.5     15.2         213      4650 female 2009\n\n\nWe can calculate the likelihood of our penguin’s body mass if \\(\\hat{y}_i = 4500\\) and \\(\\sigma^2\\) = 40 with the following code:\n\ndnorm(myPenguin$body_mass, mean=4500, sd=40)\n\n[1] 8.814892e-06\n\n\nWhat happens if we change the parameters?\n\ndnorm(myPenguin$body_mass, mean=4600, sd=40)\n\n[1] 0.004566227\n\n\nGiven that our penguin weighs 4650 grams, it is more likely to observe this data if the underlying model has a mean closer to 4650.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#calculating-the-likelihood-of-many-datapoints",
    "href": "03-linearModels1.html#calculating-the-likelihood-of-many-datapoints",
    "title": "4  Intro to Linear Models",
    "section": "4.5 Calculating the likelihood of many datapoints",
    "text": "4.5 Calculating the likelihood of many datapoints\nWe will get better parameter estimates if we have more data – this is a basic fact of statistics. But, how do we calculate a likelihood of many data points?\nRemember from our probability rules that if you want to know the probability of observing two things, you can multiply the probability of observing the first thing times the probability of observing the second thing. And remember that likelihoods are just probabilities\nSo, if we wanted to calculate the likelihood of observing two penguin body masses for specific parameters, we calculate the likelihood of observing each penguin body mass and multiply those likelihoods together.\n\nmyPenguins = penguins[sample(1:nrow(penguins), 2),]\n\nmyPenguins\n\n      species island bill_len bill_dep flipper_len body_mass    sex year\n329 Chinstrap  Dream     45.7     17.3         193      3600 female 2009\n267    Gentoo Biscoe     46.2     14.1         217      4375 female 2009\n\ndnorm(myPenguins$body_mass[1], mean=4600, sd=40)*dnorm(myPenguins$body_mass[2], mean=4600, sd=40)\n\n[1] 2.570397e-147\n\n\nSince the dnorm() function is vectorized, we can also write this as one line of code.\n\nprod(dnorm(myPenguins$body_mass, mean=4600, sd=40))\n\n[1] 2.570397e-147\n\n\nThis new code formulation lets us calculate the likelihood for any length vector of observations. We can even calculate the likelihood of observing the entire penguin data set.\n\npenguins2 &lt;- penguins |&gt; filter(!is.na(body_mass)) ##filter out ones with NAs\nprod(dnorm(penguins2$body_mass, mean=4600, sd=40))\n\n[1] 0\n\n\nWait, what’s going on? How are we getting a probability of 0?\n\n4.5.1 Log likelihood solves underflow problems\n\nThe product of small numbers are smaller numbers. Very very very small numbers cannot be represented in your computer’s memory. This problem is called underflow.\nWe often deal with underflow by using logs. You may remember from your high school or undergrad math classes that\n\\[\\text{log}(A \\times B \\times C) = \\text{log}(A) + \\text{log}(B) + \\text{log}(C)\\]\nand more generally:\n\\[\\text{log}\\left(\\prod\\limits_{i=1}^n X_i \\right) = \\sum\\limits_{i=1}^n \\text{log}(X_i)\\] Additionally, the log is monotonic which means that\nIf \\(X &gt; Y\\) then \\(\\text{log}(X) &gt; \\text{log}(Y)\\)\nIn practice, instead of multiplying likelihoods together to calculate the likelihood of observing a large dataset, we can sum log likelihoods together.\n\nsum(dnorm(myPenguins$body_mass, mean=4600, sd=40, log=T))\n\n[1] -337.5359\n\n\n\n\n4.5.2 Estimating paramaters with maximum likelihood using grid search.\nWe aren’t just interested in estimating the likelihood for one parameter – instead we want to find the parameters that give us the highest likelihood of observing our data. One way to do this is with a grid search where we calculate the log likelihood for a grid of parameters and identify the parameter associated with the highest likelihood. We’ll do this here for a grid of possible values of \\(\\hat{y}_i\\) (we won’t mess with \\(\\sigma^2\\) here).\n\nmyGrid &lt;- seq(2000,6000,length.out=100) #make the grid\n\nmyLogLikes &lt;- sapply(myGrid,function(m)\n  {sum(dnorm(penguins2$body_mass,mean=m,sd=40,log=TRUE))}) ## function to calculate the log likeliood for each value in the grid\n\nmyGrid[which.max(myLogLikes)] ## figure out the grid value that corresponds to the maximum likelihood\n\n[1] 4181.818\n\nplot(myGrid, myLogLikes, bty=\"n\", xlab = \"mean body weight\", ylab = \"log likelihood\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#using-rs-lm-function",
    "href": "03-linearModels1.html#using-rs-lm-function",
    "title": "4  Intro to Linear Models",
    "section": "4.6 Using R’s lm() function",
    "text": "4.6 Using R’s lm() function\nIn practice, grid searchers are very inefficient and it is often easier to use premade R functions to fit linear models.\nBelow is code for linear model with the penguins data using R’s lm() function.\n\nmodel1 = lm(body_mass ~ 1, data = penguins)\nmodel1\n\n\nCall:\nlm(formula = body_mass ~ 1, data = penguins)\n\nCoefficients:\n(Intercept)  \n       4202  \n\n\nThe output gives us the estimated intercept — which, in this case with no predictors, is simply the mean\nWe can also use the summarize() function to look more carefully at the model\n\nsummary(model1)\n\n\nCall:\nlm(formula = body_mass ~ 1, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1501.8  -651.8  -151.8   548.2  2098.2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4201.75      43.36   96.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 802 on 341 degrees of freedom\n  (2 observations deleted due to missingness)\n\n\nWe can update our model from above with our new parameter estimate of the mean.\n\\[\\text{penguin body mass} = 4201.75 + e_i\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#more-on-residuals",
    "href": "03-linearModels1.html#more-on-residuals",
    "title": "4  Intro to Linear Models",
    "section": "4.7 More on residuals",
    "text": "4.7 More on residuals\nThe residual (\\(e_i\\)) for each individual penguin tells us how much that penguin’s mass differs from the population mean.\nFor example, we can look at one specific penguin\n\npenguins[1,]\n\n  species    island bill_len bill_dep flipper_len body_mass  sex year\n1  Adelie Torgersen     39.1     18.7         181      3750 male 2007\n\n\nThis is a male Adelie penguin from Torgersen Island. Its body mass is 3750. We can describe this penguin’s mass as\n\\[3750 = 4201.75 + e_i\\] \\(e_i = 4201.75-3750 = 451.75\\)\nBelow I plot all the body mass data. Each point is a penguin. You can hover over the points to see the body mass of each penguin and the residual.\n\n\nCode\nlibrary(plotly)\nprop_hybrid_plot &lt;- penguins                          |&gt;\n  filter(!is.na(body_mass))                         |&gt;\n  mutate(i = 1:n(),\n         e_i = body_mass - mean(body_mass),\n         e_i = round(e_i, digits = 3),\n         y_hat_i = round(mean(body_mass),digits=3),\n         y_i = round(body_mass, digits = 3))                         |&gt;\n  ggplot(aes(x = i, y = y_i, y_hat_i = y_hat_i, e_i = e_i))+\n  geom_point(size = 4, alpha = .6)+\n  scale_color_manual(values = c(\"black\",\"darkgreen\"))+\n  geom_hline(yintercept = 4201.76,\n             linetype = \"dashed\", color = \"red\", size = 2)+\n  labs(y = \"Body Mass\", title =\"This plot is interactive!! Hover over a point to see its residual\")+\n  theme(legend.position = \"none\")\n\nggplotly(prop_hybrid_plot)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#calculating-residuals",
    "href": "03-linearModels1.html#calculating-residuals",
    "title": "4  Intro to Linear Models",
    "section": "4.8 Calculating residuals",
    "text": "4.8 Calculating residuals\nYou can look at residuals and model predictions using the augment() function in the broom package. The code below uses augment() to make a table where each row has a penguin’s body mass, the expectation of body mass from the fitted model, and the residual.\n\nlibrary(broom)\naugment(model1) |&gt; select(body_mass, .fitted, .resid)\n\n\n\n\n\n\n\nYou could also generate residuals without any additional packages by using the following code\n\npenguins2 &lt;- penguins |&gt; filter(!is.na(body_mass)) ##filter out ones with NAs\nmodel1Residuals &lt;-  penguins2$body_mass - model1$fitted.values\n\nor even more simply, the lm() output includes the residuals.\n\nmodel1Residuals &lt;- model1$residuals",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#calculating-the-ss_residual",
    "href": "03-linearModels1.html#calculating-the-ss_residual",
    "title": "4  Intro to Linear Models",
    "section": "4.9 Calculating the \\(SS_{residual}\\)",
    "text": "4.9 Calculating the \\(SS_{residual}\\)\nTODO explain why if we’re doing this\n\nlibrary(broom)\nmodel1       |&gt;\n augment()                |&gt;\n mutate(sq_resid=.resid^2)|&gt;\n summarise(SS=sum(sq_resid))\n\n# A tibble: 1 × 1\n          SS\n       &lt;dbl&gt;\n1 219307697.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#the-mean-minimizes-ss_residual",
    "href": "03-linearModels1.html#the-mean-minimizes-ss_residual",
    "title": "4  Intro to Linear Models",
    "section": "4.10 The mean minimizes \\(SS_{residual}\\)",
    "text": "4.10 The mean minimizes \\(SS_{residual}\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#practice-problems",
    "href": "03-linearModels1.html#practice-problems",
    "title": "4  Intro to Linear Models",
    "section": "4.11 Practice problems",
    "text": "4.11 Practice problems\n\nContinuing with the penguin data, we will look at a different variable: bill length. a) write out a linear model of bill length with one paramater for the mean.\n\n\ncalculate mean bill length\n\n\ncalculate the likelihood of your data given the mean from part b and a \\(\\sigma^2\\) of 5.\n\n\ncalculate the log likelihood of your data given the mean from part b\n\n\ncalculate the log likelihood for a grid of means of 100 values ranging from 20 to 60. Make a plot showing these values.\n\n\ndescribe in words what the plot below is showing you about your data on bill length",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "04-lm-categories.html",
    "href": "04-lm-categories.html",
    "title": "5  Linear models with categories and ANOVA",
    "section": "",
    "text": "5.1 What are we doing here?\nWe previously discussed linear models and learned to build a very simple linear model to estimate the mean. We also learned how to calculate residuals, which in this case were the differences between each datapoint and the mean.\nIn this section we’ll expand our linear model to incorporate differences between categories.\nWe’ll also cover ANOVA, which lets us determine how much variation is explained by a category.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "04-lm-categories.html#motivating-example",
    "href": "04-lm-categories.html#motivating-example",
    "title": "5  Linear models with categories and ANOVA",
    "section": "5.2 Motivating example",
    "text": "5.2 Motivating example\nYou may have noticed in the plot of penguin body mass in the last chapter that there appeared to be differences between different groups. Let’s look more closely by making a plot of the body masses by color.\n\nplot(penguins$body_mass, bty=\"n\", ylab = \"body mass\", \n     col=penguins$species, lwd=2)\nlegend('topleft',levels(penguins$species),bty=\"n\", pch=1, pt.lwd=2, col=palette())\n\n\n\n\n\n\n\n\nIt definitely looks like something is different between these different species. But can we use linear models to confidently say so?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "04-lm-categories.html#linear-models-with-categories",
    "href": "04-lm-categories.html#linear-models-with-categories",
    "title": "5  Linear models with categories and ANOVA",
    "section": "5.3 Linear models with categories",
    "text": "5.3 Linear models with categories\nLet’s walk through how to use a linear model to estimate the differences between categories\n\n5.3.1 Estimating conditional means\nWe want to start by estimating the mean body mass of each species of penguin. The code below does this\n\npenguins                                             |&gt;\n  filter(!is.na(body_mass) , !is.na(species)) |&gt;  #remove NAs\n  group_by(species)                             |&gt;\n  summarise(mean_body_mass = mean(body_mass))\n\n# A tibble: 3 × 2\n  species   mean_body_mass\n  &lt;fct&gt;              &lt;dbl&gt;\n1 Adelie             3701.\n2 Chinstrap          3733.\n3 Gentoo             5076.\n\n\nYou may be wondering why I am referring to this as a conditional mean. To explain this I’ll walk through the math of the model, focussing only on Adelie and Gentoo penguins.\nWe will model body mass of an individual penguin, \\(i\\), conditional on its species.\n\\[\\text{Mass}_i = b_0 + b_1 \\times \\text{Adelie}_i + e_i \\]\n\n\\(b_0\\) is the intercept and, in this case, the mean body mass of Gentoo penguins\n\\(b_1\\) is the difference in mass between Adelie and Gentoo penguins.\n\\(\\text{Adelie}_i\\) is an indicator variable that is \\(1\\) if individual \\(i\\) is an Adelie penguin and \\(0\\) if individual \\(i\\) is a Gentoo penguin.\n\\(e_i\\) is the residual\n\nWe could rewrite the above equation as two equations that describe the mass of individuals conditional on whether they are Adelie or Gentoo. \\[ \\text{Mass}_{i|\\text{Adelie}} = b_0 + e_i\\] \\[ \\text{Mass}_{i|\\text{Gentoo}} = b_0 + b_1 + e_i\\]\nWe can work through these equations to estimate the parameters of the model. The mean mass of an Adelie penguin (\\(b_0\\)) is 3701 and since the mean mass of a Gentoo penguin is 5076, \\(b_1 =\\) 1375$.\n\n\n5.3.2 Fitting a model with lm()\n\n## first filter out chinstraps and anything with missing data\npenguins2 &lt;- penguins                                             |&gt;\n  filter(!is.na(body_mass) , !is.na(species), species != \"Chinstrap\") |&gt; \n  droplevels()\n\nmodel1 &lt;- lm(body_mass ~ species, data=penguins2)\nsummary(model1)\n\n\nCall:\nlm(formula = body_mass ~ species, data = penguins2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1126.02  -369.50   -26.02   343.00  1223.98 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3700.66      39.02   94.83   &lt;2e-16 ***\nspeciesGentoo  1375.35      58.24   23.61   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 479.5 on 272 degrees of freedom\nMultiple R-squared:  0.6721,    Adjusted R-squared:  0.6709 \nF-statistic: 557.6 on 1 and 272 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n5.3.3 Residuals\n\nLinear models vs t-tests\nHow is what we’ve just done different from a t-test? Or is it?\n\nadelies = dplyr::filter(penguins, species==\"Adelie\")\ngentoos = dplyr::filter(penguins, species==\"Gentoo\")\n\nt.test(adelies$body_mass, gentoos$body_mass)\n\n\n    Welch Two Sample t-test\n\ndata:  adelies$body_mass and gentoos$body_mass\nt = -23.386, df = 249.64, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1491.183 -1259.525\nsample estimates:\nmean of x mean of y \n 3700.662  5076.016 \n\n\nBoth a t-test and the linear models we’ve discussed so far are based on a normal distribution. However, the t-test uses a t distribution. The t-distribution incorporates uncertainty in our estimate of the standard deviation of a normal distribution. The more data we have, the closer the t-distribution approximates a normal distribution.\nNote that we can compare the output of our t.test above and the summary of the corresponding linear model and the t-value for species is the same!\n\n\n\n5.3.4 More than two categories\nWhat if we want to look at all three species? We can use the same modelling approach as before.\n\\[\\text{Mass}_i = b_0 + b_1 \\times \\text{Adelie}_i +b_2 \\times \\text{Chinstrap} + e_i \\]\n\n\\(b_0\\) is the intercept and, in this case, the mean body mass of Gentoo penguins\n\\(b_1\\) is the difference in mass between Adelie and Gentoo penguins.\n\\(b_2\\) is the difference in mass between Chinstrap and Gentoo penguins.\n\\(\\text{Adelie}_i\\) is an indicator variable that is \\(1\\) if individual \\(i\\) is an Adelie penguin and \\(0\\) if not.\n\\(\\text{Chinstrap}_i\\) is an indicator variable that is \\(1\\) if individual \\(i\\) is a Chinstrap penguin and \\(0\\) if not.\n\\(e_i\\) is the residual\n\n\n\n5.3.5 Building a model with more than two categories with lm()\n\nmodel2 &lt;- lm(body_mass ~ species, data = penguins)\nmodel2\n\n\nCall:\nlm(formula = body_mass ~ species, data = penguins)\n\nCoefficients:\n     (Intercept)  speciesChinstrap     speciesGentoo  \n         3700.66             32.43           1375.35  \n\nsummary(model2)\n\n\nCall:\nlm(formula = body_mass ~ species, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1126.02  -333.09   -33.09   316.91  1223.98 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3700.66      37.62   98.37   &lt;2e-16 ***\nspeciesChinstrap    32.43      67.51    0.48    0.631    \nspeciesGentoo     1375.35      56.15   24.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 462.3 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.6697,    Adjusted R-squared:  0.6677 \nF-statistic: 343.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n5.3.6 A note on the reference categories.\nOur model will look slightly different depending on which reference category we use. In the previous model, our reference category was Gentoo since it is the mean body mass of Gentoo penguins that is equal to the intercept. We can change the reference category.\nThe code below runs a new model that uses the relevel() function to set the reference as Chinstrap.\n\nmodel3 &lt;- lm(body_mass ~relevel(species, ref=\"Chinstrap\"), data=penguins)\nmodel3\n\n\nCall:\nlm(formula = body_mass ~ relevel(species, ref = \"Chinstrap\"), \n    data = penguins)\n\nCoefficients:\n                              (Intercept)  \n                                  3733.09  \nrelevel(species, ref = \"Chinstrap\")Adelie  \n                                   -32.43  \nrelevel(species, ref = \"Chinstrap\")Gentoo  \n                                  1342.93",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "04-lm-categories.html#anova",
    "href": "04-lm-categories.html#anova",
    "title": "5  Linear models with categories and ANOVA",
    "section": "5.4 ANOVA",
    "text": "5.4 ANOVA\nThere may be times where we have multiple category values but instead of knowing the effect of each category value on the outcome, we just want to know if individuals from different species have different body masses in general. For example, if we had 100s of penguin species, we might care less about the effect of each species than how much species matter in general. This latter value would be easier to interpret.\nAnswering this question requires an ANOVA, which stands for “Analysis of Variance” Formally, we are asking if individuals from different groups are drawn from the same distribution (our null hypothesis) or different distributions (our alternative hypothesis).\nThe ANOVA also solves some other problems with multiple category linear models. The model above compares Gentoo and Chinstrap penguins with Adelie penguins but it doesn’t compare Gentoo and Chinstrap with each other. Making that comparison requires rearranging the order of the model.\n\n5.4.1 \nANOVAs work by estimating the amount of variation within groups and among groups. The null hypothesis suggests that individuals from different groups will be no more different from each other than individuals from the same group. If this is true, then the variance among groups should equal the variance within groups.\nWe need ways to measure this variance. We’ll start by describing the sum of squares groups. \\(\\text{SS}_{\\text{group}} =  \\Sigma_{i} n_{i}(\\hat{Y_i} - \\bar{Y})^{2}\\) where \\(\\hat{Y_i}\\) is the mean of the \\(i^{th}\\) group, \\(n_i\\) is the number of individuals in the group, and \\(\\bar{Y}\\) is the mean of the entire sample. The differences between the model mean and the mean of the whole sample are shown in panel b below.\nThe corresponding value within groups is\\(\\text{SS}_{\\text{error}}\\)m the sum of squares error. It is calculated as \\(\\text{SS}_{\\text{error}} = \\Sigma_{i}\\Sigma{j}(Y_{ij} - \\bar{Y_i})^2\\) where \\(Y_{ij}\\) is the value for each individual \\(j\\) in group \\(i\\). The differences between each individual and the group means are shown in panel c below.\nTogether, the error mean square and the group mean square sum to the total mean square which tells us the deviations between each individual and the group mean (panel c below).\n\n\n\n\n\nPartitioning deviations in an ANOVA. A Shows the difference between each observation, \\(Y_i\\), and the grand mean, \\({\\overline{Y}}\\). This is the basis for calculating \\(SS_{total}\\). B Shows the difference between each predicted value \\(\\widehat{Y_i}\\) and the grand mean, \\({\\overline{Y}}\\). This is the basis for calculating \\(SS_{group}\\). C Shows the difference between each observation, \\(Y_i\\), and its predicted value \\(\\widehat{Y_i}\\). This is the basis for calculating \\(SS_{error}\\).\n\n\n\n\n\n\n5.4.2 Significance with an F statistic\nSo remember that if there is no difference in means between groups, the variance within groups will be the same as the variance between groups.\nANOVAs test this claim using a F statistic which calculates the ratio of these two values.\n\\[ F = \\frac{\\text{Variance among groups}}{\\text{Variance within groups}}\\]\nWhile different terms are used across texts, here we will call the variance among groups is called the group mean square or \\(\\text{MS}_{\\text{groups}}\\) and it is calculated as \\(\\text{MS}_{\\text{groups}} =  \\frac{\\text{SS}_{\\text{groups}}}{df_{\\text{groups}}}\\). In this case \\(df_{\\text{groups} = n_{\\text{groups}} - 1\\).\nThe variance within groups will be called the error mean square or \\(\\text{MS}_{\\text{error}}\\) and is calculated as \\(\\text{MS}_{\\text{error}}  = \\frac{\\text{SS}_{\\text{error}}}{df_{\\text{error}}}\\). Here, \\(df_{\\text{error}}= n- n_{\\text{groups}}\\).\n\\[ F = \\frac{\\text{MS}_{\\text{groups}}}{\\text{MS}_{\\text{error}}}\\]\nUsing our penguin data we can calculate the sum of squares values\n\npenguins2 &lt;- filter(penguins, !is.na(body_mass)) #remove nas for body mass\n\nsstable &lt;- augment(model2) |&gt; ## give us the means of each group in the .fitted row\n     mutate(sample_mean  = mean(body_mass)) |&gt; # calculate the total mean\n            summarise(n_groups = n_distinct(species),\n                      n=n(),\n        SS_total = sum((body_mass-sample_mean)^2), #total ss from body mass minus sample mean\n        SS_groups=sum((.fitted-sample_mean)^2), #model ss from group means minus total mean\n        SS_error = sum((body_mass-.fitted)^2), # error ss from body mass minus the group means\n)\n \nsstable\n\n# A tibble: 1 × 5\n  n_groups     n   SS_total  SS_groups  SS_error\n     &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1        3   342 219307697. 146864214. 72443483.\n\n\nNext we can calculate the mean square values\n\nftable &lt;- sstable |&gt;\n  mutate(\n    df_groups = n_groups-1,\n    df_error = n - n_groups,\n    ms_groups = SS_groups/df_groups,\n    ms_error = SS_error/df_error,\n    F_value = ms_groups/ms_error\n  )\n\nftable\n\n# A tibble: 1 × 10\n  n_groups     n   SS_total  SS_groups  SS_error df_groups df_error ms_groups\n     &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1        3   342 219307697. 146864214. 72443483.         2      339 73432107.\n# ℹ 2 more variables: ms_error &lt;dbl&gt;, F_value &lt;dbl&gt;\n\n\nWe can then use an F-test to test the hypothesis that F is different from 1.\n\npf(q = ftable$F_value, df1 = ftable$df_groups, df2 = ftable$df_error, lower.tail = FALSE)\n\n[1] 2.892368e-82\n\n\n\n\n5.4.3 Fitting an ANOVA with in R\nInstead of calculating everything by hand, we can use R to fit an ANOVA with the following code:\n\n#install.packages('car')\nlibrary(car)\n\nAnova(model2)\n\nAnova Table (Type II tests)\n\nResponse: body_mass\n             Sum Sq  Df F value    Pr(&gt;F)    \nspecies   146864214   2  343.63 &lt; 2.2e-16 ***\nResiduals  72443483 339                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that our calculations match the Anova function!\n\n\n5.4.4 Pairwise comparisons\nNow, to determine if there is a difference between any pair of categories, like Chinstrap and Gentoo, we can do a post-hoc pairwise comparison. Post-hoc tells us that we are doing these tests after we reject the null hypothesis that all of these categories have the same distribution.\nIf we want to look at all possible comparisons, this is called doing unplanned comparisons\n\n#install.packages(\"emmeans\")\nlibrary(emmeans)\n\nout.emmeans &lt;- emmeans(model2, specs = \"species\")\npairs(out.emmeans)\n\n contrast           estimate   SE  df t.ratio p.value\n Adelie - Chinstrap    -32.4 67.5 339  -0.480  0.8807\n Adelie - Gentoo     -1375.4 56.1 339 -24.495  &lt;.0001\n Chinstrap - Gentoo  -1342.9 69.9 339 -19.224  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\n\n5.4.5 Assumptions of an ANOVA\n\nEqual variance within groups The ANOVA null hypothesis is that variance within groups is the same as variance between groups, so having different amounts of variance within different groups will break the null hypothesis.\n\nA general rule of thumb is that as long as the variances of each group are within a factor of five of each other, you can use an ANOVA.\n\nNormally distributed residuals\n\nThis assumption is similar to what we have in a linear model. Fortunately, ANOVAs are pretty robust to this assumption",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "04-lm-categories.html#review-questions",
    "href": "04-lm-categories.html#review-questions",
    "title": "5  Linear models with categories and ANOVA",
    "section": "5.5 Review questions",
    "text": "5.5 Review questions\nMaize pollen can provide food for Anopheles arabiensis, a vector of malaria. Imagine that you are interested in whether maize cultivation levels affect malaria levels. You have data from Kebede et al. 2005 on maize cultivation level and rates of malaria from multiple locations in Ethiopia.\nHere is a plot of the data below\n\n\n\n\n\n\n\n\n\n\nWrite out an equation for a linear model testing for the effects of each yield category on malaria rates. Define each variable in the model (all the \\(b\\) values)\nWhat if you were interested in whether yield in general affected malaria rate? Would you use a linear model or an ANOVA? Why?\nDoes it look like this data matches the assumptions of an ANOVA? If not, why not?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "05-lm-continuousvars.html",
    "href": "05-lm-continuousvars.html",
    "title": "6  Linear models with continuous variables",
    "section": "",
    "text": "6.1 What are we doing here?\nMany of the variables you are interested are continuous, not categorical. In this section we’ll extend our linear models to incorporate continuous predictor variables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "05-lm-continuousvars.html#motivating-example",
    "href": "05-lm-continuousvars.html#motivating-example",
    "title": "6  Linear models with continuous variables",
    "section": "6.2 Motivating example",
    "text": "6.2 Motivating example\nContinuing with the penguin example data, imagine that we are interested in understanding how bill length determines body mass – do penguins with longer bills also have larger bodies?\n\npenguins2 &lt;- penguins |&gt; filter(!is.na(body_mass) & !is.na(bill_len)) ##filter out ones with NAs\nplot(penguins2$bill_len, penguins2$body_mass, bty=\"n\", ylab = \"body mass\", xlab = \"bill length\", lwd=2, col = palette()[1])\n\n\n\n\n\n\n\n\nWe can plot out the data and see that there is potentially a relationship between bill length and body mass. But, imagine we have a biological reason for wanting to model body mass as a function of bill length. How would we do that?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "05-lm-continuousvars.html#starting-with-categories",
    "href": "05-lm-continuousvars.html#starting-with-categories",
    "title": "6  Linear models with continuous variables",
    "section": "6.3 Starting with categories",
    "text": "6.3 Starting with categories\nIn the last section we learned about testing for differences between categorical predictor variables. We could start there to think about how to approach this challenge. Specifically, imagine we divide our penguins in half and compare body mass between the half with longer bills and the half with shorter bills.\nWe’ll start by making a categorical variable, “long”, that is true if a penguin’s bill length is greater than the median bill length and otherwise false.\n\npenguins2 &lt;- mutate(penguins2, long = bill_len&gt;median(bill_len))\n\nNext, we will see if long bill length predicts mass, using a categorical model:\n\\[\\text{Mass}_i = b_0 + b_1 \\times \\text{Long}_i + e_i \\] Now, let’s fit this model with lm()\n\nmodel1 &lt;- lm(body_mass ~ long, data = penguins2)\nsummary(model1)\n\n\nCall:\nlm(formula = body_mass ~ long, data = penguins2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1937.87  -487.87   -14.25   512.13  1662.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3765.64      51.51   73.11   &lt;2e-16 ***\nlongTRUE      872.22      72.85   11.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 673.6 on 340 degrees of freedom\nMultiple R-squared:  0.2966,    Adjusted R-squared:  0.2945 \nF-statistic: 143.4 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\n\nWe get a pretty strong signal that the long-billed penguins are larger than small-billed penguins since \\(b_1\\) is estimated to be 872.22, which is the difference in means between long-billed and short-billed penguins.\nYou are likely shaking your head at this model, since we are losing a lot of information about our data by collapsing what we have into two means. In fact, doing this type of collapsing of continuous data into categorical data is often a really bad idea for multiple reasons.\nNOTE IS THIS A GOOD APPROACH I DON’T KNOW",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "05-lm-continuousvars.html#linear-models-with-continuous-predictors",
    "href": "05-lm-continuousvars.html#linear-models-with-continuous-predictors",
    "title": "6  Linear models with continuous variables",
    "section": "6.4 Linear models with continuous predictors",
    "text": "6.4 Linear models with continuous predictors\nInstead of letting the mean of the model vary between categories, we now build our model with the mean as a linear function of the predictor variable. This model is called linear regression and it has two components: the determenistic function and the stochastic function\n\n6.4.1 The deterministic function\nThe deterministic function describes how the explanatory variables relate to the conditional mean of the response variable. In this case, since this is a linear model, we are modelling the response variable as a line:\n\\[\\hat{y}_i = b_0 + b_1 \\times x_i\\] As before, \\(y_i\\) is the conditional mean of the response variable for an individual with value \\(x_i\\), \\(b_0\\) is the intercept or the conditional mean of an individual with a value of \\(x_i = 0\\) (although this value will not be meaningful if it lies far outside the range of the data).\nHowever, we can think of \\(b_1\\) now as the slope of the relationship between the predictor and response variable. This means that for every increase of 1 unit in \\(x\\), \\(y\\) increases by \\(b_1\\).\n\n\n6.4.2 The stochastic function\nThe stochastic function explains how the residuals are distributed around the conditional means predicted by the deterministic function. Since we are working with regular linear models, we will assume that the residuals are normally distributed.\n\\[ e_i \\sim N(0, \\sigma^2)\\]\nWe can combine the deterministic and stochastic functions into one model:\n\\[y_i = \\hat{y}_i + N(b_0 + b_1 \\times x_i, \\sigma^2)\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "05-lm-continuousvars.html#estimating-parameters",
    "href": "05-lm-continuousvars.html#estimating-parameters",
    "title": "6  Linear models with continuous variables",
    "section": "6.5 Estimating parameters",
    "text": "6.5 Estimating parameters\n\\[b_1 = \\frac{cov_{x,y}}{s^2_x}  =  \\frac{\\frac{1}{(n-1)}\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\frac{1}{(n-1)}\\sum(x_i-\\bar{x})^2}=\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}\\]\n(only include this if we do variance and covariance somewhere)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "05-lm-continuousvars.html#likelihood-and-linear-models",
    "href": "05-lm-continuousvars.html#likelihood-and-linear-models",
    "title": "6  Linear models with continuous variables",
    "section": "6.6 Likelihood and linear models",
    "text": "6.6 Likelihood and linear models",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "05-lm-continuousvars.html#fitting-with-lm",
    "href": "05-lm-continuousvars.html#fitting-with-lm",
    "title": "6  Linear models with continuous variables",
    "section": "6.7 Fitting with lm()",
    "text": "6.7 Fitting with lm()\nLet’s try applying a linear model to our penguin data. We’ll use the lm() function in R.\n\\[\\text{Body mass}_i = b_0 + b_1 \\times \\text{Bill length}_i \\]\n\nmodel1 &lt;- lm(body_mass ~ bill_len, data=penguins2)\nmodel1\n\n\nCall:\nlm(formula = body_mass ~ bill_len, data = penguins2)\n\nCoefficients:\n(Intercept)     bill_len  \n     362.31        87.42  \n\n\nBased on these estimates, we can now write out our model as\n\\[\\text{Body mass}_i = 362.31 + 87.42 \\times \\text{Bill length}_i \\]\nWe can interpret this as for every mm of bill length increase, body mass increases by 87 grams.\nWe can also estimate \\(\\sigma^2\\), the variance of the residuals.\n\nsigma(model1)\n\n[1] 645.4333\n\n\nThis allows us to write our model including the stochastic function as:\n\\[ \\text{Body mass}_i \\sim N(362.31 + 87.42 \\times \\text{Bill length}_i, 645.43) \\]\nWe can also plot the predictions from this model over the raw data.\n\nplot(penguins2$bill_len, penguins2$body_mass, bty=\"n\", ylab = \"body mass\", xlab = \"bill length\", lwd=2, col = palette()[1])\nabline(model1, col=palette()[3], lwd=2)\n\n\n\n\n\n\n\n\n\n6.7.1 Fitting with glmmTMB()\n\n\n6.7.2 Residuals\nIt can be a bit confusing to think about the distribution of residuals in these types of models. Often we assume that the response data needs to be normally distributed to use a linear model. However, the real assumption of the model is not that the response data is normally distributed, but that the residuals are normally distributed.\nLet’s look at the residuals from this model\n\nlibrary(broom)\nmyResiduals &lt;- augment(model1) |&gt; select(body_mass, .fitted, .resid)\n\nhist(myResiduals$.resid, main=\"\", xlab = \"residuals\", border=\"white\", col=palette()[2])\n\n\n\n\n\n\n\n\nIt helps me to imagine that the values themselves follow a normal distribution centered around a mean that follows the model predictions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "06-lm-multiplepredictors.html",
    "href": "06-lm-multiplepredictors.html",
    "title": "7  Linear models with multiple predictors",
    "section": "",
    "text": "7.1 What are we doing here?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "06-lm-multiplepredictors.html#motivating-example",
    "href": "06-lm-multiplepredictors.html#motivating-example",
    "title": "7  Linear models with multiple predictors",
    "section": "7.2 Motivating example",
    "text": "7.2 Motivating example\nIn the previous sections, we found that both species and bill length affect body mass. Can we build a model that incorporates both?\nTo make things a bit simpler we will only look at Adelie and Gentoo penguins\n\npenguins2 &lt;- penguins |&gt; filter(!is.na(body_mass) & !is.na(bill_len) & !species==\"Chinstrap\") |&gt; droplevels() ##filter out missing data and chinstrap\n\n\nplot(penguins2$bill_len, penguins2$body_mass, bty=\"n\", ylab = \"body mass\", xlab = \"bill length\", lwd=2, col = penguins2$species)\nlegend('topleft',levels(penguins2$species),bty=\"n\", pch=1, pt.lwd=2, col=palette())\n\n\n\n\n\n\n\n\nExamining the data, we can generate some hypotheses about how bill length and species relate to body mass.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "06-lm-multiplepredictors.html#building-the-model",
    "href": "06-lm-multiplepredictors.html#building-the-model",
    "title": "7  Linear models with multiple predictors",
    "section": "7.3 Building the model",
    "text": "7.3 Building the model\nLet’s think about a model of some response variable (\\(y\\)) based on two explanatory variables, \\(x_1\\) and \\(y_1\\).\nOur model looks like it did before, except now it has multiple predictors, each with its own coefficient.\n\\[ \\hat{y}_i = b_0 + b_1x_1i + b_2x_2i e_i\\]\nin our penguin example\n\\[ \\text{Body mass} = b_0 + b_1 \\text{Gentoo} + b_2 \\text{bill length} + e_i \\]\nwhere \\(b_1\\) is the difference in means between Gentoo and Adelie and \\(b_2\\) is the effect of increasing bill length one unit on body mass. \\(b_0\\) is the intercept, or the predicted body mass for an Adelie penguin with a bill length of 0 (we may question how useful this value is for predictions but it is certainly useful for the model). Note that this assumes that Adelie and Gentoo penguins have the same slope, which may not always be the case.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "06-lm-multiplepredictors.html#fitting-with-lm",
    "href": "06-lm-multiplepredictors.html#fitting-with-lm",
    "title": "7  Linear models with multiple predictors",
    "section": "7.4 Fitting with lm()",
    "text": "7.4 Fitting with lm()\n\nmodel1 &lt;- lm(body_mass~bill_len+species,data=penguins2)\nmodel1\n\n\nCall:\nlm(formula = body_mass ~ bill_len + species, data = penguins2)\n\nCoefficients:\n  (Intercept)       bill_len  speciesGentoo  \n       -267.6          102.3          484.0  \n\n\nWe can now rewrite our equation with these estimated values\n\\[ \\text{Body mass} = -267.6 + 484.0 \\times \\text{Gentoo} + 102.3 \\times \\text{bill length} + e_i \\]\nNote that the intercept does not provide a particularly useful prediction.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "06-lm-multiplepredictors.html#interactions",
    "href": "06-lm-multiplepredictors.html#interactions",
    "title": "7  Linear models with multiple predictors",
    "section": "7.5 Interactions",
    "text": "7.5 Interactions\nI noted before that our model assumed that the slope of the relationship between bill length and body mass is the same for both species. This may not always be the case.\nWe can extend our model to incorporate differences in slopes between categories (or between individuals with different values for a continuous variable). This pattern is called a statistical interaction.\n\\[ \\hat{y}_i = b_0 + b_1x_1i + b_2x_2i + b_3 x_1 \\times x_2 e_i\\]\nin our penguin example\n\\[ \\text{Body mass} = b_0 + b_1 \\text{Gentoo} + b_2 \\text{bill length} + b_3 \\text{Gentoo} \\times \\text{bill length} + e_i \\]\nWe can fit the model with lm() using the following code:\n\nmodel2 &lt;- lm(body_mass~bill_len+species+bill_len*species,data=penguins2)\nsummary(model2)\n\n\nCall:\nlm(formula = body_mass ~ bill_len + species + bill_len * species, \n    data = penguins2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-782.6 -261.8  -12.9  249.2 1126.3 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               34.88     453.96   0.077    0.939    \nbill_len                  94.50      11.68   8.094 1.98e-14 ***\nspeciesGentoo           -158.71     699.81  -0.227    0.821    \nbill_len:speciesGentoo    14.96      16.17   0.925    0.356    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 380.8 on 270 degrees of freedom\nMultiple R-squared:  0.7947,    Adjusted R-squared:  0.7924 \nF-statistic: 348.4 on 3 and 270 DF,  p-value: &lt; 2.2e-16\n\n\nIn this case the statistical interaction isn’t significant, suggesting that the slope of the relationship between bill length and mass does not differ between species.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "06-lm-multiplepredictors.html#the-general-linear-model",
    "href": "06-lm-multiplepredictors.html#the-general-linear-model",
    "title": "7  Linear models with multiple predictors",
    "section": "7.6 The General Linear Model",
    "text": "7.6 The General Linear Model\nFor the last few classes, we have been building up to a General Linear Model (not to be confused with a Generalized Linear Model!).\n\n7.6.1 Assumptions of the general linear model\nThere are a few major assumptions of general linear models.\n\nThe predictor \\(x_1\\) is linearly related to the response variable \\(y_1\\).\nThe errors (residuals), \\(e_i\\), are independent and identically distributed\nThe errors, \\(e_i\\), have a constant variance \\(\\sigma^2\\)\nIf performing inference, the errors, \\(e_i\\), are normally distributed\n\nHowever, if your data doesn’t seem to meet these assumptions, you have options.\n\nYou can transform your data\nYou can pick a new model\nYou can continue onward. Linear models (particularly ANOVAs) tend to be robust to assumption breaking, but only to a point\n\nWhatever you decide to do, be transparent in your paper!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "06-lm-multiplepredictors.html#matrix-notation",
    "href": "06-lm-multiplepredictors.html#matrix-notation",
    "title": "7  Linear models with multiple predictors",
    "section": "7.7 Matrix notation",
    "text": "7.7 Matrix notation\nIt might be helpful for some of you to see a linear model written out in matrix notation. The equation of three matrices below describes a linear model.\n\\[\\begin{equation}\n\\begin{pmatrix}\n    1 & x_{1,1} & x_{2,1} & \\dots  & x_{k,1} \\\\\n    1 & x_{1,2} & x_{2,2} & \\dots  & x_{k,2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{1,n} & x_{2,n} & \\dots  & x_{k,n}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n    b_0 \\\\\n    b_{1}\\\\\n    b_{2}\\\\\n    \\vdots  \\\\\n     b_{k}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    \\hat{Y_1} \\\\\n    \\hat{Y_2}\\\\\n    \\vdots  \\\\\n    \\hat{Y_n}\n\\end{pmatrix}\n\\end{equation}\\]\nThe leftmost matrix is a design matrix where each row corresponds to an individual \\(i\\) and each column corresponds to an explanatory variable. A cell in the \\(i^{th}\\) row and \\(j^{th}\\) column corresponds to individual \\(i\\)’s value for the \\(j^{th}\\) explanatory variable. The first row corresponds to the intercept so all individuals have a value of 1.\nThe middle matrix is a list of the estimated effect sizes for each corresponding explanatory variable.\nThe dot product of these two matrices gives us the predicted value for the response variable for each individual.\n\nGeneral vs Generalized linear models\nThese are often confused!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "07-lm-interactions.html",
    "href": "07-lm-interactions.html",
    "title": "8  Linear models and interactions",
    "section": "",
    "text": "8.1 What are we doing here?\nWe previously discussed linear models and learned to build a very simple linear model to estimate the mean. We also learned how to calculate residuals, which in this case were the differences between each datapoint and the mean.\nIn this section we’ll expand our linear model to incorporate differences between categories.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linear models and interactions</span>"
    ]
  },
  {
    "objectID": "07-lm-interactions.html#motivating-example",
    "href": "07-lm-interactions.html#motivating-example",
    "title": "8  Linear models and interactions",
    "section": "8.2 Motivating example",
    "text": "8.2 Motivating example",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linear models and interactions</span>"
    ]
  },
  {
    "objectID": "08-multipletesting.html",
    "href": "08-multipletesting.html",
    "title": "9  Multiple testing issues (and opportunities)",
    "section": "",
    "text": "9.1 What are we doing here?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multiple testing issues (and opportunities)</span>"
    ]
  },
  {
    "objectID": "08-multipletesting.html#motivating-example",
    "href": "08-multipletesting.html#motivating-example",
    "title": "9  Multiple testing issues (and opportunities)",
    "section": "9.2 Motivating example",
    "text": "9.2 Motivating example\nYou may have noticed in the plot of penguin body mass in the last chapter that there appeared to be differences between different groups. Let’s look more closely by making a plot of the body masses by color.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multiple testing issues (and opportunities)</span>"
    ]
  },
  {
    "objectID": "09-GWAS.html",
    "href": "09-GWAS.html",
    "title": "10  Applying linear models with GWAS",
    "section": "",
    "text": "10.1 What are we doing here?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Applying linear models with GWAS</span>"
    ]
  },
  {
    "objectID": "09-GWAS.html#motivating-example",
    "href": "09-GWAS.html#motivating-example",
    "title": "10  Applying linear models with GWAS",
    "section": "10.2 Motivating example",
    "text": "10.2 Motivating example",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Applying linear models with GWAS</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#calculating-the-ss_textresidual",
    "href": "03-linearModels1.html#calculating-the-ss_textresidual",
    "title": "4  Intro to Linear Models",
    "section": "4.9 Calculating the \\(SS_{\\text{residual}}\\)",
    "text": "4.9 Calculating the \\(SS_{\\text{residual}}\\)\nWe can quantify how well our model fits the data using the residuals, in particular by calculating the Sum of Squares of the residuals (\\(SS_{\\text{residual}}\\))\n\nlibrary(broom)\nmodel1       |&gt;\n augment()                |&gt;\n mutate(sq_resid=.resid^2)|&gt;\n summarise(SS=sum(sq_resid))\n\n# A tibble: 1 × 1\n          SS\n       &lt;dbl&gt;\n1 219307697.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#calculating-the-textss_textresidual",
    "href": "03-linearModels1.html#calculating-the-textss_textresidual",
    "title": "4  Intro to Linear Models",
    "section": "4.9 Calculating the \\(\\text{SS}_{\\text{residual}}\\)",
    "text": "4.9 Calculating the \\(\\text{SS}_{\\text{residual}}\\)\nWe can quantify how well our model fits the data using the residuals, in particular by calculating the sum of squared residuals (\\(\\text{SS}_{\\text{residual}}\\))\n\nlibrary(broom)\nss_resid &lt;-model1       |&gt; \n augment()                |&gt; #use augment to calculate residuals\n mutate(sq_resid=.resid^2)|&gt; # use mutate to calculate squared residuals\n summarise(SS=sum(sq_resid)) #use summarise to sum the residuals\n\nss_resid\n\n# A tibble: 1 × 1\n          SS\n       &lt;dbl&gt;\n1 219307697.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#the-mean-minimizes-textss_textresidual",
    "href": "03-linearModels1.html#the-mean-minimizes-textss_textresidual",
    "title": "4  Intro to Linear Models",
    "section": "4.10 The mean minimizes \\(\\text{SS}_{\\text{residual}}\\)",
    "text": "4.10 The mean minimizes \\(\\text{SS}_{\\text{residual}}\\)\nWe know how to calculate the mean of our data. But, imagine that we didn’t and we wanted to figure out a way to ‘best’ summarize our data. One way to do this is to find a measure that minimizes the distance between our observed data and the predictions made by the model, which means minimizing the residuals. In practice, this means minimizing \\(\\text{SS}_{\\text{residual}}\\)\nAbove, we calculated \\(\\text{SS}_{\\text{residual}}\\) for a mean of 4202. The following code calculates \\(\\text{SS}_{\\text{residual}}\\) for a model with a mean of 5000.\n\nss_resid2 &lt;- model1 |&gt;\n  augment() |&gt;\nmutate(sq_resid2 = (5000-body_mass)^2) |&gt; #calculate new residuals based on a \"mean \" of 5000\nsummarise(SS2 = sum(sq_resid2))\n\nss_resid2\n\n# A tibble: 1 × 1\n        SS2\n      &lt;dbl&gt;\n1 437228750\n\nss_resid2 &gt; ss_resid\n\n      SS2\n[1,] TRUE\n\n\nSo the sum of squares for residuals from 5000 is much larger than the sum of squares for residuals from 4202, the mean.\nThe plot below shows potential means on the x axis and \\(\\text{SS}_{\\text{residual}}\\) on the Y axis",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  }
]