[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EEB 830/831",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Chapter 1 – Introduction",
    "section": "",
    "text": "1.1 Hi!\nHi! Welcome to the ebook for IBIO/PLB/ENT 830, the introductory statistics course in Michigan State University’s Ecology and Evolutionary Biology Graduate program.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-goals",
    "href": "01-intro.html#learning-goals",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.2 Learning Goals",
    "text": "1.2 Learning Goals\nCourse learning goals go here"
  },
  {
    "objectID": "01-intro.html#themes",
    "href": "01-intro.html#themes",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.3 Themes",
    "text": "1.3 Themes\nThroughout the course we’ll be talking about the following themes…"
  },
  {
    "objectID": "01-intro.html#who-we-are",
    "href": "01-intro.html#who-we-are",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.4 Who we are",
    "text": "1.4 Who we are\nStuff about the professors goes here"
  },
  {
    "objectID": "01-intro.html#how-this-book-works",
    "href": "01-intro.html#how-this-book-works",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.5 How this book works",
    "text": "1.5 How this book works\nStuff about logistics goes here."
  },
  {
    "objectID": "02-probability.html",
    "href": "02-probability.html",
    "title": "2  Chapter 2 – Probability",
    "section": "",
    "text": "2.1 What are we doing here?\nIn this chapter, we’ll talk about the basics of probability and learn how to simulate data to investigate the probability in different scenarios.\nThere are lots of reasons we might care about probability.\nWe can use probability in lots of ways. For example, if I’m playing Monopoly and I’m on Park Place, what is the probability that my next roll will take me to Boardwalk?\nOr, imagine that you’re pregnant with a due date of Oct 22. What is the probability that you’ll give birth on a specific day?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#what-are-we-doing-here",
    "href": "02-probability.html#what-are-we-doing-here",
    "title": "2  Chapter 2 – Probability",
    "section": "",
    "text": "Many important biological processes are influenced by chance so we need probability to describe these processes.\nWe don’t want to tell science stories about coincidences. We can use probability to describe the potential for our data to be shaped by coincidence vs the process we are interested in.\nUnderstanding probability helps us understand statistics!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#definitions",
    "href": "02-probability.html#definitions",
    "title": "2  Chapter 2 – Probability",
    "section": "2.3 Definitions",
    "text": "2.3 Definitions\n\n2.3.1 Sample Space\nThe sample space is the set of all possible outcomes. For example, when you flip a coin the possible outcomes are heads and tails. The sample space for our class example was the set of all the students in the class.\nIt also might help to think about sample space using the following example of balls falling through different bins.\n\n\n\n\n\n\nFigure 2.1: An example of probability. Here outcomes A, B, and C are mutatually exclusive and make up all of state space\n\n\n\nThe sample space for this example is that (A), The ball can fall through the orange bin, (B) the ball can fall through the green bin, and (C) the ball can fall through the blue bin\n\n\n2.3.2 Probability\nWe can think about the probability of some outcome as the frequency of that outcome within the sample space.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#motivating-examples",
    "href": "02-probability.html#motivating-examples",
    "title": "2  Chapter 2 – Probability",
    "section": "2.2 Motivating examples",
    "text": "2.2 Motivating examples\nWe can use probability in lots of ways. For example, if I’m playing Monopoly and I’m on Park Place, what is the probability that my next roll will take me to Boardwalk?\n\n\n\n\n\n\n\n\n\nOr, imagine that you’re pregnant. What is the probability that you’ll give birth on a specific day?\nWhen will I give birth?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Probability</span>"
    ]
  },
  {
    "objectID": "02-probability.html#simulating-data",
    "href": "02-probability.html#simulating-data",
    "title": "2  Chapter 2 – Probability",
    "section": "2.2 Simulating data",
    "text": "2.2 Simulating data\nYou may have learned some mathematical rules to describe probability in a previous class. If so, that’s great! For now though, pretend that you don’t know these rules because this chapter will focus on using simulations to understand probability.\nOne of the most powerful tools that we’ll have in our statistics learning toolkit are simulations. Simulations let you generate data that you know should look a certain way, so you can test your intuitions. Simulations also let you do the same thing over and over and over.\nLet’s start by simulating our class. We’ll create a vector with a number corresponding to each person in the class.\n\nthisClass = 1:25\nthisClass\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n\n\nNow let’s use the R function sample() to sample a random person from the class.\n\nsample(thisClass, size=1)\n\n[1] 2\n\n\nWe can do this again to conduct a second sample.\n\nsample(thisClass, size=1)\n\n[1] 5\n\n\nWe can use sample() to answer the following questions:\nIf we randomly pick a student, how likely are we to select student #19?\n\nmySamples &lt;- replicate(1000, sample(thisClass, size=1))\nsum(mySamples==19)/10000\n\n[1] 0.0033\n\n\nWhat we’ve done here is write code that samples one student from the class and then we’ve used replicate() to repeat that sampling 1000 times. After that, we checked how many times we sampled student #19.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Probability</span>"
    ]
  },
  {
    "objectID": "01-intro.html#hi",
    "href": "01-intro.html#hi",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.1 Hi!",
    "text": "1.1 Hi!\nHi! Welcome to the ebook for IBIO/PLB/ENT 830, the introductory statistics course in Michigan State University’s Ecology and Evolutionary Biology Graduate program."
  },
  {
    "objectID": "02-likelihood.html",
    "href": "02-likelihood.html",
    "title": "2  Chapter 2 – Likelihood",
    "section": "",
    "text": "2.1 What are we doing here?\nIn this chapter, we’ll talk about the concept of likelihood and how we can use it to make statistical inferences from our data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#motivating-question",
    "href": "02-likelihood.html#motivating-question",
    "title": "2  Chapter 2 – Likelihood",
    "section": "2.2 Motivating question",
    "text": "2.2 Motivating question\nSomeone hands us a coin and we flip it 100 times and get 46 heads.\nIs the coin fair? (probability of heads = 50%)\nWhat is the most likely probability of flipping heads for this coin?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#likelihood",
    "href": "02-likelihood.html#likelihood",
    "title": "2  Chapter 2 – Likelihood",
    "section": "2.3 Likelihood",
    "text": "2.3 Likelihood\nThe likelihood of the data is the probability of the data as a function of some unknown parameters.\nLikelihood is probability…in reverse!\nIn probability, we think about some stochastic process, and figure out ways to calculate the probability of possible outcomes\nFor example: given that the coin is fair, what’s the probability of getting 46 heads out of 100 flips?\nIn calculating probabilities, we consider a single parameter value and describe the probabilities of all possible outcomes of a process parameterized by that value.\nIn statistics, we start with some observed outcomes, and try to figure out the underlying process.\nFor example: given some flip data, can we figure out the fairness of the coin?\nIn calculating likelihoods, we consider a single outcome (or set of outcomes) and many possible parameter values that could best explain it.\nIn formulating the problem this way, we are treating the observed data (\\(k=46\\)) as a known, and treating \\(p\\) as an unknown parameter of the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "02-likelihood.html#parametric-statistics",
    "href": "02-likelihood.html#parametric-statistics",
    "title": "2  Chapter 2 – Likelihood",
    "section": "2.4 Parametric statistics",
    "text": "2.4 Parametric statistics\nWe can use likelihood in a framework of parametric statistics.\nIn parametric inference, we treat observed data as draws from an underlying process or population, and we try to learn about that population from our sample.\nA statistical parameter is a value that tells you something about a population, like the true mean height of students in our class, or the true frequency of a genetic variant in a species.\nBUT, we rarely get to know the truth!\n(we would know the truth if we censused everyone in a population, or repeated a random process an infinite number of times)\nInstead, we can take samples to try to learn about –\nor estimate – parameters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html",
    "href": "03-linearModels1.html",
    "title": "3  Linear Models 1: The Mean",
    "section": "",
    "text": "3.1 What are we doing here?\nIn this chapter, we will meet the linear model and discuss what it is exactly that we are doing when we build linear models.\nWe’ll build the most basic type of linear model by fitting the mean of a distribution.\nWe’ll also learn to interpret residuals and visualize them with R.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Models 1: The Mean</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#motivating-question",
    "href": "03-linearModels1.html#motivating-question",
    "title": "3  Chapter 3 – Linear Models 1: The Mean",
    "section": "3.2 Motivating question",
    "text": "3.2 Motivating question",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3 -- Linear Models 1: The Mean</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#the-mean",
    "href": "03-linearModels1.html#the-mean",
    "title": "3  Linear Models 1: The Mean",
    "section": "3.3 The mean",
    "text": "3.3 The mean\nWe are going to start with the simplest linear model possible. You likely already know how to calculate the mean (\\(\\overline{y}\\)) of a set of data: \\(\\overline{y} = \\frac{\\sum y_i}{n}\\) where \\(y_i\\) is each data point and \\(n\\) is the number of samples.\nIn the simplest linear model we can also think of the mean as the intercept (\\(b_0\\)) so we can predict each data point \\(y_i\\) as simply the mean plus an error term or residual (\\(e_i\\)).\n\\[\\hat{y}_i = b_0 + e_i\\]\n\n3.3.1 Wait, what’s a residual?\nObserved values often differ from the predictions made by a linear model.\nWe define a residual (\\(e_i\\)) as the difference between an observed value(\\(Y_i\\)) and its predicted value from a linear model (\\(\\hat{y}_i\\)).\n\\[e_i = y_i - \\hat{y}_i\\]\nYou can also rearrange this to think about it the other way around so that the observed variable (\\(Y_i\\)) is the sum of the value predicted by the model (\\(\\hat{y}_i\\)) and the residual \\(e_i\\).\n\\[y_i = \\hat{y}_i + e_i\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Models 1: The Mean</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#linear-models",
    "href": "03-linearModels1.html#linear-models",
    "title": "3  Linear Models 1: The Mean",
    "section": "3.2 Linear models",
    "text": "3.2 Linear models\nAs biologists, we often want to understand if one thing causes another thing.\n\nDo traits determine an organism’s fitness? In other words, are traits under selection?\nDoes genotype at this locus determine phenotype?\nDo the amount of resources shape community diversity?\nget Lauren et al to fill in some other questions\n\nTo answer these types questions, we need a way to carefully relate two variables together. We will refer throughout this course to two types of variables:\n\nExplanatory variables, also called independent variables or predictor variables.\nResponse variables, also called dependent variables.\n\nLinear models estimate the conditional mean of the \\(i^{th}\\) observation of a continuous response variable, \\(\\hat{Y}_i\\) from a (combination) of value(s) of the explanatory variables (\\(\\text{explanatory variables}_i\\)):\n\\[\\begin{equation}\n\\hat{Y}_i = f(\\text{explanatory variables}_i)\n\\end{equation}\\]\nThese models are “linear” because we estimate of the conditional mean (\\(\\hat{Y}_i\\)) by adding up all components of the model. So, each explanatory variable \\(y_{j,i}\\) is multiplied by its effect size \\(b_j\\). It might help to look at an example model below:\n\\[\\begin{equation}\n\\hat{Y}_i = a + b_1  y_{1,i} + b_2 y_{2,i} + \\dots{}\n\\end{equation}\\]\nIn this example, \\(\\hat{Y}_i\\) is estimated as the sum of the “intercept” (\\(a\\)), its value for the first explanatory variable (\\(y_{1,i}\\)) times the effect of this variable (\\(b_1\\)), its value for the second explanatory variable (\\(y_{2,i}\\)) times the effect of this variable, \\(b_2\\), and so on for all included explanatory variables.\nIn practice, fitting a linear model requires picking explanatory variables and then estimating the values of \\(a\\), \\(b_1\\), \\(b_2\\), and so on that best predict the response variables. These estimates then tell us how the explanatory variables relate to the response variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Models 1: The Mean</span>"
    ]
  },
  {
    "objectID": "03-linearModels1.html#using-rs-lm-function",
    "href": "03-linearModels1.html#using-rs-lm-function",
    "title": "3  Linear Models 1: The Mean",
    "section": "3.4 Using R’s lm() function",
    "text": "3.4 Using R’s lm() function",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Models 1: The Mean</span>"
    ]
  }
]