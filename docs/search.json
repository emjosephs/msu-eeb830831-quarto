[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EEB 830 pt 2",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Chapter 1 – Introduction",
    "section": "",
    "text": "1.1 Hi!\nHi! Welcome to the ebook for the second half of IBIO/PLB/ENT 830, the introductory statistics course in Michigan State University’s Ecology and Evolutionary Biology Graduate program.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#learning-goals",
    "href": "01-intro.html#learning-goals",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.2 Learning Goals",
    "text": "1.2 Learning Goals\nCourse learning goals go here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#themes",
    "href": "01-intro.html#themes",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.3 Themes",
    "text": "1.3 Themes\nThroughout the course we’ll be talking about the following themes…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#who-i-am",
    "href": "01-intro.html#who-i-am",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.4 Who I am",
    "text": "1.4 Who I am\nI’m an evolutionary geneticist. My lab mostly works on understanding how evolution has shaped quantitative trait variation, especially in the context of environmental variation and genome evolution. We use statistics to infer things about trait variation and genomic sequence variation, and the links between the two.\nI’m actually a second-generation statistics teacher – my dad was a high school math teacher and taught statistics for years.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#how-this-book-works",
    "href": "01-intro.html#how-this-book-works",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.5 How this book works",
    "text": "1.5 How this book works\nStuff about logistics goes here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#acknowledgements",
    "href": "01-intro.html#acknowledgements",
    "title": "1  Chapter 1 – Introduction",
    "section": "1.6 Acknowledgements",
    "text": "1.6 Acknowledgements\nYaniv Brandvain’s online statistics textbook inspired this project and he has answered so many of my statistical and logistical questions.\nThis course section was originally developed by Gideon Bradburd and I learned a ton from teaching it.\nDoc Edge and his cool statistics textbook have been big sources of statistical knowledge.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Chapter 1 -- Introduction</span>"
    ]
  },
  {
    "objectID": "likelihood.html",
    "href": "likelihood.html",
    "title": "2  Hypothesis testing and likelihood",
    "section": "",
    "text": "2.1 What are we doing here?\nWelcome to statistics! You’ve already learned the basics of R, along with probability. You’ll now be using your knowledge to make inferences about the world using data. Today we’ll be covering likelihood, a flexible approach for linking data to parameters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#motivating-question",
    "href": "likelihood.html#motivating-question",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.2 Motivating question",
    "text": "2.2 Motivating question\nSomeone hands us a coin and we flip it 100 times and get 46 heads.\nIs the coin fair? (probability of heads = 50%)\nWhat is the most likely probability of flipping heads for this coin?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#likelihood",
    "href": "likelihood.html#likelihood",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.3 Likelihood",
    "text": "2.3 Likelihood\nThe likelihood of the data is the probability of the data as a function of some unknown parameters.\nHow is this different from the probability we’ve been talking about? In probability, we think about some stochastic process, and figure out ways to calculate the probability of possible outcomes. For example: given that the coin is fair, what’s the probability of getting 46 heads out of 100 flips? When we calculate probabilities, we consider a single parameter value and describe the probabilities of all possible outcomes of a process that is parameterized by that value.\nIn contrast, in statistics, we start with some observed outcomes, and try to figure out the underlying process. For example: given some flip data, can we figure out the fairness of the coin? When we calculate likelihoods, we consider a single outcome (or set of outcomes) and many possible parameter values that could best explain it.\nIn formulating the problem this way, we are treating the observed data (\\(k=46\\)) as a known, and treating \\(p\\) as an unknown parameter of the model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#parametric-statistics",
    "href": "likelihood.html#parametric-statistics",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.4 Parametric statistics",
    "text": "2.4 Parametric statistics\nWe can use likelihood in a framework of parametric statistics. In parametric statistics, we treat observed data as draws from an underlying process or population, and we try to learn about that population from our sample.\nA statistical parameter is a value that tells you something about a , like the true mean height of students in our class, or the true frequency of a genetic variant in a species.\nBUT, we rarely get to know the truth! (we would know the truth if we censused everyone in a population, or repeated a random process an infinite number of times). Instead, we can take samples to try to learn about – or estimate – parameters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#a-coin-flip-example",
    "href": "likelihood.html#a-coin-flip-example",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.5 A coin flip example",
    "text": "2.5 A coin flip example\nImagine that we flip a coin 100 times and get 46 heads.\nWe know that we can model coin flipping using a Binomial distribution where \\(x\\) is the number of heads, \\(n\\) is the number of flips, and \\(p\\) is the probability of heads:\n\\[ x \\sim B(n,p)\\]\nFor this particular case:\n\\[ 46 \\sim B(100,p=??) \\]\nWe can flip this around and write the probability of our data as the conditional probablity of observing 46 heads out of 100 flips given a specific \\(p\\).\n\\[ P(\\text{observation}) = P(x=46 \\mid p, n=100)\\]\n\n2.5.1 Calculating likelihood\nWe can use R to calculate these probabilities. In particular, the dbinom() function lets us calculate the probability of observing a particular number of successes given a specific \\(n\\) and \\(p\\). Below is code to calculate the probability of getting 46 heads if the coin is fair (\\(p=0.5\\)). Note the syntax of ‘size’ for \\(n\\).\n\ndbinom(x=46, p=0.5, size=100)\n\n[1] 0.0579584\n\n\nThis code gives us the likelihood of the data for one value of \\(p\\). But what if we want to calculate it for a range of potential \\(p\\)s? Turns out we can give dbinom() a vector of \\(p\\) and it will return a vector of probabilities.\n\npvector = seq(0,1,length.out=101)\n  probabilities &lt;- dbinom(x=46,size=100,p=pvector)\n\nInstead of printing the list of values, let’s make a plot.\n\nplot(pvector,probabilities, xlim=c(0,1),ylim=c(0,0.1), xlab=\"probability of heads (p)\",ylab=\"p(data)\", pch=20,col=2)\n\n\n\n\n\n\n\n\nWhat we’ve just is called a grid search. It is one strategy to identify the parameter that gives you the highest probability of observing your data (aka likelihood). Looking at the plot above, what \\(p\\) seems to give the highest likelihood?\nWe can calculate this directly with the following code:\n\npvector[which.max(probabilities)]\n\n[1] 0.46",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#maximum-likelihood",
    "href": "likelihood.html#maximum-likelihood",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.6 Maximum likelihood",
    "text": "2.6 Maximum likelihood\nMaximum likelihood inference is a method for estimating the values of the parameters of a statistical model that maximize the likelihood of the observed data.\nThe maximum likelihood estimate (MLE) is the parameter value (or, if there are multiple parameters, the vector of parameter values) that maximize the likelihood of the data. The MLE is our best guess at the true value of the unknown population (or process) parameter.\nAbove we used a grid search to identify the MLE. However, grid searches can get unwieldy. Luckily, R has built-in functions to identify the MLE more efficiently. See the code below\n\n# function to estimate the p(k=46|n=100,p) for a given p\n    l.binom &lt;- function(p){\n        binom.prob &lt;- dbinom(x=46,size=100,prob=p)\n        return(binom.prob)\n    }\n# use optimize() to get the mle\n    mle &lt;- optimize(f = l.binom,lower=0,upper=1,maximum=TRUE)\n    mle$maximum\n\n[1] 0.4600159",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#now-hang-on",
    "href": "likelihood.html#now-hang-on",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.7 Now hang on…",
    "text": "2.7 Now hang on…\nCalculating the probability of some possible outcome makes sense… But calculating the probability of something that has already happened seems bananas!\nBut, that’s not really what we’re doing - We’re calculating the probability of the observed data as a draw from some distribution, given the values of the parameters of that distribution. Our goal is to use these probabilities to estimate the distribution parameters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#log-likelihood-for-lots-of-data-points",
    "href": "likelihood.html#log-likelihood-for-lots-of-data-points",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.8 Log likelihood for lots of data points",
    "text": "2.8 Log likelihood for lots of data points\nThroughout this part of the course we’ll be focussing on data that is normally distributed so let’s look at a log likelihood in this context. We’ll start by simulating a bunch of data under the normal distribution with a mean of 10 and standard distribution of 3.\n\nset.seed(100)\nmyData &lt;- rnorm(n=500, mean=10, sd=3)\nhist(myData, border=\"white\", main=\"\", breaks=20)\n\n\n\n\n\n\n\n\nWe can use what we just learned to calculate the likelihood of a specific datapoint given a set of parameters. The code before shows how to do that with the first datapoint and the parameters that we used to simulate the data.\n\nmyData[1]\n\n[1] 8.493423\n\ndnorm(myData[1], mean=10, sd=3)\n\n[1] 0.1172263\n\n\nWe can also calculate the MLE for this datapoint (we’ll focus on the mean and not the standard deviation here)\n\n# function to estimate the likelihood for a given mean\n    l.norm &lt;- function(u){\n        norm.prob &lt;- dnorm(myData[1],mean=u, sd=3)\n        return(norm.prob)\n    }\n# use optimize() to get the mle using the range of the data\n    mle &lt;- optimize(f = l.norm,lower=0,upper=20,maximum=TRUE)\n    mle$maximum\n\n[1] 8.493409\n\n\nAre you surprised that the MLE does not match the mean you used to calculate the data? You may already expect that using a single data point may not be very helpful in estimating the parameter of a distribution. We will get a better estimate using multiple data points. Let’s start with the first 2 data points.\nRemember that if you want to know the probability of observing two things, you can multiply the probability of observing the first thing times the probability of observing the second thing. And remember that likelihoods are just probabilities, so we get the likelihood of two data points by multiplying the likelihood of each data point together.\n\\[ P(\\text{2 observations}) = P(\\text{observation 1}) \\times P(\\text{observation 2})\\]\nThe code below does just this (note the prod() function if you haven’t seen it before).\n\nmyData[1:2]\n\n[1]  8.493423 10.394593\n\nprod(dnorm(myData[1:2], mean=10, sd=3))\n\n[1] 0.01545457\n\n\nWe can again use optimize to find the MLE although we will need to update the l.norm() function.\n\n# function to estimate the likelihood for a given mean\n    l.norm &lt;- function(u){\n        norm.prob &lt;- prod(dnorm(myData[1:2],mean=u, sd=3))\n        return(norm.prob)\n    }\n    mle &lt;- optimize(f = l.norm,lower=0,upper=20,maximum=TRUE)\n    mle$maximum\n\n[1] 9.443993\n\n\nNow, what about the whole dataset? We should be able to to calculate the likelihood of each data point and multiply these likelihoods together to get the likelihood of seeing all this data.\n\n#multiple likelihoods of each datapoint together\nprod(dnorm(myData, mean=10, sd=3))\n\n[1] 0\n\n\nWait, what’s going on? How are we getting a probability of 0?\n\n2.8.1 Log likelihood solves underflow problems\n\nThe product of small numbers are smaller numbers. Very very very small numbers cannot be represented in your computer’s memory. This problem is called underflow. We often deal with underflow by using logs. You may remember from your high school or undergrad math classes that\n\\[\\text{log}(A \\times B \\times C) = \\text{log}(A) + \\text{log}(B) + \\text{log}(C)\\]\nand more generally:\n\\[\\text{log}\\left(\\prod\\limits_{i=1}^n X_i \\right) = \\sum\\limits_{i=1}^n \\text{log}(X_i)\\] Additionally, the log is monotonic which means that\nIf \\(X &gt; Y\\) then \\(\\text{log}(X) &gt; \\text{log}(Y)\\)\nIn practice, instead of multiplying likelihoods together to calculate the likelihood of observing a large data set, we can sum log likelihoods together.\nThe code below calculates the log likelihood of all of the data we simulated. Note that we replace the prod() function with the sum() function and use the log=T option in the dnorm() function.\n\nsum(dnorm(myData, mean=10, sd=3, log=T))\n\n[1] -1261.091\n\n\nWe can now optimize this function\n\n# function to estimate the likelihood for a given mean\n    l.norm &lt;- function(u){\n        norm.prob &lt;- sum(dnorm(myData,mean=u, sd=3, log=T))\n        return(norm.prob)\n    }\n    mle &lt;- optimize(f = l.norm,lower=0,upper=20,maximum=TRUE)\n    mle$maximum\n\n[1] 9.887178\n\n\nThis returns something pretty close to the value we used to simulate the data. Is it close enough that we trust that our simulation worked? That’s a question we’ll address in a future class.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#law-of-large-numbers",
    "href": "likelihood.html#law-of-large-numbers",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.9 Law of large numbers",
    "text": "2.9 Law of large numbers\nThe law of large numbers simply states that as the number of samples increases, the sample average converges on the population average. (Note that the law of large numbers is often written regarding the expectation, which is a mathematical way of specifying the population average)\nIntuitively, the law of large numbers explains why when we tried to calculate the sample mean above, we got a much better estimate of the mean used to simulate the data when we used more data than when we used just one or two data points.\nThe following visualization might be helpful. Imagine that we sequentially sample from a normal distribution with a mean of 10 (as we did above) and with each sample, we calculate the mean and plot it below.\n\n\n\n\n\n\n\n\n\nWe can see that for the first few samples we got a lower running mean but over time we end up close to the mean used to simulate the data. If we repeat this exercise a bunch of times we get the following plot:\n\nset.seed(100)\n    plot(0,xlim=c(0,n),ylim=c(0,20),type='n',\n            xlab=\"number of draws from a normal\",\n            ylab=\"running mean\")\n        draw.running.mean(r = 100,n = 100)\n        abline(h=10,col=2)\n\n\n\n\n\n\n\n\nWhile this may seem obvious, it is a fundamental principal across the sciences. For example, the law of large numbers explains why small populations have stronger genetic drift than larger populations – each generation, the gametes that survive are more likely to sample an allele frequency that matches the parental generation if there are many gametes. If there are few gametes surviving each generation, there will be more variation in the mean allele frequencies.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#practice-problems",
    "href": "likelihood.html#practice-problems",
    "title": "2  Hypothesis testing and likelihood",
    "section": "2.10 Practice problems",
    "text": "2.10 Practice problems\n\nSomeone tells you that the average height of a 4-year-old is 1 meter with a standard deviation of 5 centimeters. You want to know if this is accurate so you go to a daycare and measure the heights of 10 4-year-olds.Write, in math, the probability distribution you expect for 4-year-old height.\nUse R to calculate the likelihood of observing a 4-year-old who is 110 cm tall if your information is accurate.\nWhat if the actual mean height of a 4-year-old is 115 cm? Now, what is the likelihood of observing a 110 cm tall 4-year-old?\nYou measure a second kid and find that they are 105 cm tall. Use R to calculate the likelihood of observing both kids if the underlying mean is 1 meter. What about if the underlying mean height is 110 cm?\nYou measure all the kids and get the following vector of heights: 112 89 112 102 126 114 111 112 112 114. Use a grid search to estimate the MLE for mean height for this data. You can use likelihood or log-likelihood (or both and compare).\nDoes it make sense to travel to another daycare to measure the height of another 10 kids to get a better estimate of height? How many kids would you want to measure to feel confident?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing and likelihood</span>"
    ]
  },
  {
    "objectID": "linearModels1.html",
    "href": "linearModels1.html",
    "title": "3  Intro to Linear Models",
    "section": "",
    "text": "3.1 What are we doing here?\nIn this chapter, we will meet the linear model and discuss what it is exactly that we are doing when we build linear models.\nWe’ll build the most basic type of linear model by fitting the mean of a distribution.\nWe’ll also learn to interpret residuals and visualize them with R.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "linearModels1.html#linear-models",
    "href": "linearModels1.html#linear-models",
    "title": "3  Intro to Linear Models",
    "section": "3.2 Linear models",
    "text": "3.2 Linear models\nAs biologists, we often want to understand if one thing causes another thing.\n\nDo traits determine an organism’s fitness? In other words, are traits under selection?\nDoes genotype at this locus determine phenotype?\nDo the amount of resources shape community diversity?\n\nTo answer these types questions, we need a way to carefully relate two variables together. We will refer throughout this course to two types of variables:\n\nExplanatory variables, also called independent variables or predictor variables.\nResponse variables, also called dependent variables.\n\nLinear models estimate the conditional mean of the \\(i^{th}\\) observation of a continuous response variable, \\(\\hat{Y}_i\\) from a (combination) of value(s) of the explanatory variables (\\(\\text{explanatory variables}_i\\)):\n\\[\\begin{equation}\n\\hat{Y}_i = f(\\text{explanatory variables}_i)\n\\end{equation}\\]\nThese models are “linear” because we estimate of the conditional mean (\\(\\hat{Y}_i\\)) by adding up all components of the model. So, each explanatory variable \\(x_{j,i}\\) is multiplied by its effect size \\(b_j\\). It might help to look at an example model below:\n\\[\\begin{equation}\n\\hat{Y}_i = b_0 + b_1  x_{1,i} + b_2 x_{2,i} + \\dots{}\n\\end{equation}\\]\nIn this example, \\(\\hat{Y}_i\\) is estimated as the sum of the “intercept” (\\(b_0\\)), its value for the first explanatory variable (\\(x_{1,i}\\)) times the effect of this variable (\\(b_1\\)), its value for the second explanatory variable (\\(x_{2,i}\\)) times the effect of this variable, \\(b_2\\), and so on for all included explanatory variables.\nIn practice, fitting a linear model requires picking explanatory variables and then estimating the values of \\(a\\), \\(b_1\\), \\(b_2\\), and so on that best predict the response variables. These estimates then tell us how the explanatory variables relate to the response variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "linearModels1.html#the-mean",
    "href": "linearModels1.html#the-mean",
    "title": "3  Intro to Linear Models",
    "section": "3.3 The mean",
    "text": "3.3 The mean\nWe are going to start with the simplest linear model possible. You likely already know how to calculate the mean (\\(\\overline{y}\\)) of a set of data: \\(\\overline{y} = \\frac{\\sum y_i}{n}\\) where \\(y_i\\) is each data point and \\(n\\) is the number of samples.\nIn the simplest linear model we can also think of the mean as the intercept (\\(b_0\\)) so we can predict each data point \\(y_i\\) as simply the mean plus an error term or residual (\\(e_i\\)).\n\\[\\hat{y}_i = b_0 + e_i\\]\n\n3.3.1 Wait, what’s a residual?\nObserved values often differ from the predictions made by a linear model.\nWe define a residual (\\(e_i\\)) as the difference between an observed value(\\(Y_i\\)) and its predicted value from a linear model (\\(\\hat{y}_i\\)).\n\\[e_i = y_i - \\hat{y}_i\\]\nYou can also rearrange this to think about it the other way around so that the observed variable (\\(Y_i\\)) is the sum of the value predicted by the model (\\(\\hat{y}_i\\)) and the residual \\(e_i\\).\n\\[y_i = \\hat{y}_i + e_i\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "linearModels1.html#fitting-a-linear-model-with-maximum-likelihood",
    "href": "linearModels1.html#fitting-a-linear-model-with-maximum-likelihood",
    "title": "3  Intro to Linear Models",
    "section": "3.4 Fitting a linear model with maximum likelihood",
    "text": "3.4 Fitting a linear model with maximum likelihood\nWe will use the principals of likelihood to pick the parameters that best fit the linear model.\nThroughout this section we’ll be using a dataset of penguin traits. This data should be already available in your version of R, but if it isn’t, use the following code to install it.\n\n#install.packages(\"palmerpenguins\")\nlibrary(\"palmerpenguins\")\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\n\nYou can read more about all the variables available in the package\n\n?penguins\n\n\n\n\n\n\n\nA cartoon by Sam Gross\n\n\n\nWe’ll start by thinking about body size. We want to model each penguin’s body mass as the sum of a mean body mass and a residual.\n\\[\\text{penguin body mass} = \\text{mean body mass} + e_i\\]\n\n3.4.1 The distribution of residuals\nA key piece of our linear model is that the residuals follow a specific distribution. In this part of the course we will focus on the normal distribution since it is broadly useful, but you could use any distribution you want. Models with residuals that follow non-normal distributions are called Generalized Linear Models\nIn math terms, we would write\n\\[ e_i \\sim N(0, \\sigma^2)\\] where \\(\\sigma^2\\) is the variance of the residuals. The mean of the residuals is 0.\n\nRemember that the standard deviation is the squsre root of the variance and the standard error is the standard deviation divided by the square root of the sample size: \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\)\n\nWe can combine the equation for a linear model with the equation describing the distribution of the variables and get\n\\[y_i = b_0 + N(0, \\sigma^2)\\]\nand this simplifies to\n\\[ y_i \\sim N(b_0, \\sigma^2)\\]\nFor our penguin example\n\\[ \\text{penguin body mass} \\sim N(\\text{mean body mass}, \\sigma^2)\\]\nNote that a linear model has two components: a deterministic component and a stochastic component. The deterministic component tells us about how the explanatory variable relates to the model prediction for the response variable. The stochastic component tells us about how the residuals are distributed.\nIn this case, the deterministic equation tells us that the predicted value of body mass is mean body mass (\\(\\hat{y} = b_0\\)) and the stochastic equation tells us that the residuals are normally distributed (\\(e_i \\sim N(0,\\sigma^2\\))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "linearModels1.html#using-maximum-likelihood-to-estimate-parameters-in-a-linear-model",
    "href": "linearModels1.html#using-maximum-likelihood-to-estimate-parameters-in-a-linear-model",
    "title": "3  Intro to Linear Models",
    "section": "3.5 Using maximum likelihood to estimate parameters in a linear model",
    "text": "3.5 Using maximum likelihood to estimate parameters in a linear model\nRemember from our earlier lectures that we can use likelihood to estimate the parameters of a distribution. The code below calculates the log likelihood of observing our data on penguin body mass for specific parameters of the normal distribution.\n\npenguins2 &lt;- penguins |&gt; filter(!is.na(body_mass_g)) ##filter out ones with NAs\nsum(dnorm(penguins2$body_mass_g, mean=4600, sd=40, log=T))\n\n[1] -87059.86\n\n\n\n3.5.1 Estimating paramaters with maximum likelihood using grid search.\nAs we learned in our lecture on likelihoods, we can use a grid search to find the MLE. We’ll do this here for a grid of possible values of \\(\\hat{y}_i\\) (we won’t mess with \\(\\sigma^2\\) here).\n\nmyGrid &lt;- seq(2000,6000,length.out=100) #make the grid\n\nmyLogLikes &lt;- sapply(myGrid,function(m)\n  {sum(dnorm(penguins2$body_mass_g,mean=m,sd=40,log=TRUE))}) ## function to calculate the log likeliood for each value in the grid\n\nmyGrid[which.max(myLogLikes)] ## figure out the grid value that corresponds to the maximum likelihood\n\n[1] 4181.818\n\nplot(myGrid, myLogLikes, bty=\"n\", xlab = \"mean body weight\", ylab = \"log likelihood\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "linearModels1.html#using-rs-lm-function",
    "href": "linearModels1.html#using-rs-lm-function",
    "title": "3  Intro to Linear Models",
    "section": "3.6 Using R’s lm() function",
    "text": "3.6 Using R’s lm() function\nIn practice, grid searchers are very inefficient and it is often easier to use premade R functions to fit linear models.\nBelow is code for linear model with the penguins data using R’s lm() function.\n\nmodel1 = lm(body_mass_g ~ 1, data = penguins)\nmodel1\n\n\nCall:\nlm(formula = body_mass_g ~ 1, data = penguins)\n\nCoefficients:\n(Intercept)  \n       4202  \n\n\nThe output gives us the estimated intercept — which, in this case with no predictors, is simply the mean\nWe can also use the summarize() function to look more carefully at the model\n\nsummary(model1)\n\n\nCall:\nlm(formula = body_mass_g ~ 1, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1501.8  -651.8  -151.8   548.2  2098.2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4201.75      43.36   96.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 802 on 341 degrees of freedom\n  (2 observations deleted due to missingness)\n\n\nWe can update our model from above with our new parameter estimate of the mean.\n\\[\\text{penguin body mass} = 4201.75 + e_i\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "linearModels1.html#residuals",
    "href": "linearModels1.html#residuals",
    "title": "3  Intro to Linear Models",
    "section": "3.7 Residuals",
    "text": "3.7 Residuals\nThe residual (\\(e_i\\)) for each individual penguin tells us how much that penguin’s mass differs from the population mean.\nFor example, we can look at one specific penguin\n\npenguins[1,]\n\n# A tibble: 1 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThis is a male Adelie penguin from Torgersen Island. Its body mass is 3750. We can describe this penguin’s mass as\n\\[3750 = 4201.75 + e_i\\] \\(e_i = 4201.75-3750 = 451.75\\)\nBelow I plot all the body mass data. Each point is a penguin. You can hover over the points to see the body mass of each penguin and the residual.\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n#| code-fold: true\n#| message: false\n#| warning: false\n#| label: fig-plotly\n#| fig-cap: \"An interactive plot showing the mean body size of a penguin. Each point represents an individual penguin, and the dashed red line shows the sample mean across all penguins. Hovering over a point reveals its residual — the difference between the observed value and the mean.\"\n#| fig-alt: \"Interactive scatterplot of observed mean body mass of penguins. Points are plotted by index along the x-axis, with body mass on the y-axis. A horizontal dashed red line marks the sample mean. When hovering over a point, the residual (difference between the point’s value and the mean) is displayed.\"\n#| cap-location: margin\nresid_plot &lt;- penguins                          |&gt;\n  filter(!is.na(body_mass_g))                         |&gt;\n  mutate(i = 1:n(),\n         e_i = body_mass_g - mean(body_mass_g),\n         e_i = round(e_i, digits = 3),\n         y_hat_i = round(mean(body_mass_g),digits=3),\n         y_i = round(body_mass_g, digits = 3))                         |&gt;\n  ggplot(aes(x = i, y = y_i, y_hat_i = y_hat_i, e_i = e_i))+\n  geom_point(size = 4, alpha = .6)+\n  scale_color_manual(values = c(\"black\",\"darkgreen\"))+\n  geom_hline(yintercept = 4201.76,\n             linetype = \"dashed\", color = \"red\", size = 2)+\n  labs(y = \"Body Mass\", title =\"This plot is interactive!! Hover over a point to see its residual\")+\n  theme(legend.position = \"none\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nggplotly(resid_plot)\n\n\n\n\n\n\n3.7.1 Calculating residuals\nYou can look at residuals and model predictions using the augment() function in the broom package. The code below uses augment() to make a table where each row has a penguin’s body mass, the expectation of body mass from the fitted model, and the residual.\n\nlibrary(broom)\naugment(model1) |&gt; select(body_mass_g, .fitted, .resid)\n\n\n\n\n\n\n\nYou could also generate residuals without any additional packages by using the following code\n\nmodel1Residuals &lt;-  penguins2$body_mass_g - model1$fitted.values\n\nor even more simply, the lm() output includes the residuals.\n\nmodel1Residuals &lt;- model1$residuals \n\n\n\n3.7.2 Calculating the \\(\\text{SS}_{\\text{residual}}\\)\nWe can quantify how well our model fits the data using the residuals, in particular by calculating the sum of squared residuals (\\(\\text{SS}_{\\text{residual}}\\))\n\nlibrary(broom)\nss_resid &lt;-model1       |&gt; \n augment()                |&gt; #use augment to calculate residuals\n mutate(sq_resid=.resid^2)|&gt; # use mutate to calculate squared residuals\n summarise(SS=sum(sq_resid)) #use summarise to sum the residuals\n\nss_resid\n\n# A tibble: 1 × 1\n          SS\n       &lt;dbl&gt;\n1 219307697.\n\n\n\n\n3.7.3 The mean minimizes \\(\\text{SS}_{\\text{residual}}\\)\nWe know how to calculate the mean of our data. But, imagine that we didn’t and we wanted to figure out a way to ‘best’ summarize our data. One way to do this is to find a measure that minimizes the distance between our observed data and the predictions made by the model, which means minimizing the residuals. In practice, this means minimizing \\(\\text{SS}_{\\text{residual}}\\)\nAbove, we calculated \\(\\text{SS}_{\\text{residual}}\\) for a mean of 4202. The following code calculates \\(\\text{SS}_{\\text{residual}}\\) for a model with a mean of 5000.\n\nss_resid2 &lt;- model1 |&gt;\n  augment() |&gt;\nmutate(sq_resid2 = (5000-body_mass_g)^2) |&gt; #calculate new residuals based on a \"mean \" of 5000\nsummarise(SS2 = sum(sq_resid2))\n\nss_resid2\n\n# A tibble: 1 × 1\n        SS2\n      &lt;dbl&gt;\n1 437228750\n\nss_resid2 &gt; ss_resid\n\n      SS2\n[1,] TRUE\n\n\nSo the sum of squares for residuals from 5000 is much larger than the sum of squares for residuals from 4202, the mean.\nThe plot below shows potential means on the x axis and \\(\\text{SS}_{\\text{residual}}\\) on the Y axis",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "linearModels1.html#practice-problems",
    "href": "linearModels1.html#practice-problems",
    "title": "3  Intro to Linear Models",
    "section": "3.8 Practice problems",
    "text": "3.8 Practice problems\nContinuing with the penguin data, we will look at a different variable: bill length.\nHere is an equation for a linear model:\n\\[ \\text{Bill length}_i = b_0 + e_i\\ \\]\n\nDescribe what \\(b_0\\) and \\(e_i\\) represent.\n\nHere is a different way of writing the linear model\n\\[ \\text{Bill length}_i \\sim N(b_0, \\sigma^2) \\]\n\nWhich part of the above equation is the deterministic part of the model? Which part is the stochastic part of the model?\n\n\n\n\n\n\n\n\n\n\n3.The plot above shows the data from our model. Each point represents a penguin and three penguins are highlighted in dark blue by either a square, triangle, or star. Which penguin has the largest residual?\n\nWhich of the following pieces of code calculates the log likelihood of the model above?\n\n\n\n\n\nsum(dnorm(penguins$bill_length_mm, mean=44, sd=5, log=T))\n\n\n\n\n\nprod(dnorm(penguins$bill_length_mm, mean=44, sd=5))\n\n\n\n\n\nprod(dnorm(penguins$body_mass_g, mean=44, sd=5))\n\n\n\n\n\nsum(dnorm(penguins$bill_length_mm, mean=500, sd=5, log=T))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to Linear Models</span>"
    ]
  },
  {
    "objectID": "lm-categories.html",
    "href": "lm-categories.html",
    "title": "4  Linear models with categories and ANOVA",
    "section": "",
    "text": "4.1 What are we doing here?\nWe previously discussed linear models and learned to build a very simple linear model to estimate the mean. We also learned how to calculate residuals, which in this case were the differences between each datapoint and the mean.\nIn this section we’ll expand our linear model to incorporate differences between categories.\nWe’ll also cover ANOVA, which lets us determine how much variation is explained by a category.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "lm-categories.html#motivating-example",
    "href": "lm-categories.html#motivating-example",
    "title": "4  Linear models with categories and ANOVA",
    "section": "4.2 Motivating example",
    "text": "4.2 Motivating example\nYou may have noticed in the plot of penguin body mass in the last chapter that there appeared to be differences between different groups. Let’s look more closely by making a plot of the body masses by color.\n\nplot(penguins$body_mass_g, bty=\"n\", ylab = \"body mass\", \n     col=penguins$species, lwd=2)\nlegend('topleft',levels(penguins$species),bty=\"n\", pch=1, pt.lwd=2, col=palette())\n\n\n\n\n\n\n\n\nIt definitely looks like something is different between these different species. But can we use linear models to confidently say so?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "lm-categories.html#linear-models-with-categories",
    "href": "lm-categories.html#linear-models-with-categories",
    "title": "4  Linear models with categories and ANOVA",
    "section": "4.3 Linear models with categories",
    "text": "4.3 Linear models with categories\nLet’s walk through how to use a linear model to estimate the differences between categories\n\n4.3.1 Estimating conditional means\n\n\n\n\n\n\nAdelie Penguins – By cyfer13 - IMG_1815, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=4048994\n\n\n\nWe want to start by estimating the mean body mass of each species of penguin. The code below does this\n\npenguins                                             |&gt;\n  filter(!is.na(body_mass_g) , !is.na(species)) |&gt;  #remove NAs\n  group_by(species)                             |&gt;\n  summarise(mean_body_mass_g = mean(body_mass_g))\n\n# A tibble: 3 × 2\n  species   mean_body_mass_g\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie               3701.\n2 Chinstrap            3733.\n3 Gentoo               5076.\n\n\nYou may be wondering why I am referring to this as a conditional mean. To explain this I’ll walk through the math of the model, focussing only on Adelie and Gentoo penguins.\nWe will model body mass of an individual penguin, \\(i\\), conditional on its species.\n\\[\\text{Mass}_i = b_0 + b_1 \\times \\text{Adelie}_i + e_i \\]\n\n\\(b_0\\) is the intercept and, in this case, the mean body mass of Gentoo penguins\n\\(b_1\\) is the difference in mass between Adelie and Gentoo penguins.\n\\(\\text{Adelie}_i\\) is an indicator variable that is \\(1\\) if individual \\(i\\) is an Adelie penguin and \\(0\\) if individual \\(i\\) is a Gentoo penguin.\n\\(e_i\\) is the residual\n\nWe could rewrite the above equation as two equations that describe the mass of individuals conditional on whether they are Adelie or Gentoo. \\[ \\text{Mass}_{i|\\text{Gentoo}} = b_0 + e_i\\] \\[ \\text{Mass}_{i|\\text{Adelie}} = b_0 + b_1 + e_i\\]\nWe can work through these equations to estimate the parameters of the model. The mean mass of an Adelie penguin (\\(b_0\\)) is 3701 and since the mean mass of a Gentoo penguin is 5076, \\(b_1 = 1375\\).\n\n\n4.3.2 Fitting a model with lm()\n\n## first filter out chinstraps and anything with missing data\npenguins2 &lt;- penguins                                             |&gt;\n  filter(!is.na(body_mass_g) , species != \"Chinstrap\") |&gt; \n  droplevels()\n\nmodel1 &lt;- lm(body_mass_g ~ species, data=penguins2)\nsummary(model1)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1126.02  -369.50   -26.02   343.00  1223.98 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3700.66      39.02   94.83   &lt;2e-16 ***\nspeciesGentoo  1375.35      58.24   23.61   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 479.5 on 272 degrees of freedom\nMultiple R-squared:  0.6721,    Adjusted R-squared:  0.6709 \nF-statistic: 557.6 on 1 and 272 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n4.3.3 Residuals\nThe residuals of this model will be the difference between each data point and the conditional means estimated by the model.\n\n\n\n\n\n\n\n\nFigure 4.1: An interactive plot showing the mean body size of a penguin. Each point represents an individual penguin, and the dashed red line shows model predicted means. Hovering over a point reveals its residual — the difference between the observed value and the mean.\n\n\n\n\n\nLinear models vs t-tests\nHow is what we’ve just done different from a t-test? Or is it?\n\nadelies = dplyr::filter(penguins, species==\"Adelie\")\ngentoos = dplyr::filter(penguins, species==\"Gentoo\")\n\nt.test(adelies$body_mass_g, gentoos$body_mass_g)\n\n\n    Welch Two Sample t-test\n\ndata:  adelies$body_mass_g and gentoos$body_mass_g\nt = -23.386, df = 249.64, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1491.183 -1259.525\nsample estimates:\nmean of x mean of y \n 3700.662  5076.016 \n\n\nBoth a t-test and the linear models we’ve discussed so far are based on a normal distribution. However, the t-test uses a t distribution. The t-distribution incorporates uncertainty in our estimate of the standard deviation of a normal distribution. The more data we have, the closer the t-distribution approximates a normal distribution.\nNote that we can compare the output of our t.test above and the summary of the corresponding linear model and the t-value for species is the same!\n\n\n\n4.3.4 More than two categories\nWhat if we want to look at all three species? We can use the same modelling approach as before.\n\\[\\text{Mass}_i = b_0 + b_1 \\times \\text{Adelie}_i +b_2 \\times \\text{Chinstrap} + e_i \\]\n\n\\(b_0\\) is the intercept and, in this case, the mean body mass of Gentoo penguins\n\\(b_1\\) is the difference in mass between Adelie and Gentoo penguins.\n\\(b_2\\) is the difference in mass between Chinstrap and Gentoo penguins.\n\\(\\text{Adelie}_i\\) is an indicator variable that is \\(1\\) if individual \\(i\\) is an Adelie penguin and \\(0\\) if not.\n\\(\\text{Chinstrap}_i\\) is an indicator variable that is \\(1\\) if individual \\(i\\) is a Chinstrap penguin and \\(0\\) if not.\n\\(e_i\\) is the residual\n\n\n\n4.3.5 Building a model with more than two categories with lm()\n\nmodel2 &lt;- lm(body_mass_g ~ species, data = penguins)\nmodel2\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins)\n\nCoefficients:\n     (Intercept)  speciesChinstrap     speciesGentoo  \n         3700.66             32.43           1375.35  \n\nsummary(model2)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1126.02  -333.09   -33.09   316.91  1223.98 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3700.66      37.62   98.37   &lt;2e-16 ***\nspeciesChinstrap    32.43      67.51    0.48    0.631    \nspeciesGentoo     1375.35      56.15   24.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 462.3 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.6697,    Adjusted R-squared:  0.6677 \nF-statistic: 343.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n4.3.6 A note on the reference categories.\nOur model will look slightly different depending on which reference category we use. In the previous model, our reference category was Gentoo since it is the mean body mass of Gentoo penguins that is equal to the intercept. We can change the reference category.\nThe code below runs a new model that uses the relevel() function to set the reference as Chinstrap.\n\nmodel3 &lt;- lm(body_mass_g ~relevel(species, ref=\"Chinstrap\"), data=penguins)\nmodel3\n\n\nCall:\nlm(formula = body_mass_g ~ relevel(species, ref = \"Chinstrap\"), \n    data = penguins)\n\nCoefficients:\n                              (Intercept)  \n                                  3733.09  \nrelevel(species, ref = \"Chinstrap\")Adelie  \n                                   -32.43  \nrelevel(species, ref = \"Chinstrap\")Gentoo  \n                                  1342.93",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "lm-categories.html#anova",
    "href": "lm-categories.html#anova",
    "title": "4  Linear models with categories and ANOVA",
    "section": "4.4 ANOVA",
    "text": "4.4 ANOVA\nThere may be times where we have multiple category values but instead of knowing the effect of each category value on the outcome, we just want to know if individuals from different species have different body masses in general. For example, if we had 100s of penguin species, we might care less about the effect of each species than how much species matter in general. This latter value would be easier to interpret.\nAnswering this question requires an ANOVA, which stands for “Analysis of Variance” Formally, we are asking if individuals from different groups are drawn from the same distribution (our null hypothesis) or different distributions (our alternative hypothesis).\nThe ANOVA also solves some other problems with multiple category linear models. The model above compares Gentoo and Chinstrap penguins with Adelie penguins but it doesn’t compare Gentoo and Chinstrap with each other. Making that comparison requires rearranging the order of the model.\n\n4.4.1 \nANOVAs work by estimating the amount of variation within groups and among groups. The null hypothesis suggests that individuals from different groups will be no more different from each other than individuals from the same group. If this is true, then the variance among groups should equal the variance within groups.\nWe need ways to measure this variance. We’ll start by describing the sum of squares groups. \\(\\text{SS}_{\\text{group}} =  \\Sigma_{i} n_{i}(\\hat{Y_i} - \\bar{Y})^{2}\\) where \\(\\hat{Y_i}\\) is the mean of the \\(i^{th}\\) group, \\(n_i\\) is the number of individuals in the group, and \\(\\bar{Y}\\) is the mean of the entire sample. The differences between the model mean and the mean of the whole sample are shown in panel b below.\nThe corresponding value within groups is \\(\\text{SS}_{\\text{error}}\\) the sum of squares error. It is calculated as \\(\\text{SS}_{\\text{error}} = \\Sigma_{i}\\Sigma_{j}(Y_{ij} - \\bar{Y_i})^2\\) where \\(Y_{ij}\\) is the value for each individual \\(j\\) in group \\(i\\). The differences between each individual and the group means are shown in panel c below.\nTogether, the error mean square and the group mean square sum to the total mean square which tells us the deviations between each individual and the group mean (panel c below).\n\n\n\n\n\nPartitioning deviations in an ANOVA. A Shows the difference between each observation, \\(Y_i\\), and the grand mean, \\({\\overline{Y}}\\). This is the basis for calculating \\(SS_{total}\\). B Shows the difference between each predicted value \\(\\widehat{Y_i}\\) and the grand mean, \\({\\overline{Y}}\\). This is the basis for calculating \\(SS_{group}\\). C Shows the difference between each observation, \\(Y_i\\), and its predicted value \\(\\widehat{Y_i}\\). This is the basis for calculating \\(SS_{error}\\).\n\n\n\n\n\n\n4.4.2 Significance with an F statistic\nSo remember that if there is no difference in means between groups, the variance within groups will be the same as the variance between groups.\nANOVAs test this claim using a F statistic which calculates the ratio of these two values.\n\\[ F = \\frac{\\text{Variance among groups}}{\\text{Variance within groups}}\\]\nWhile different terms are used across texts, here we will call the variance among groups is called the group mean square or \\(\\text{MS}_{\\text{groups}}\\) and it is calculated as \\(\\text{MS}_{\\text{groups}} =  \\frac{\\text{SS}_{\\text{groups}}}{df_{\\text{groups}}}\\). In this case \\(df_{\\text{groups}} = n_{\\text{groups}} - 1\\).\nThe variance within groups will be called the error mean square or \\(\\text{MS}_{\\text{error}}\\) and is calculated as \\(\\text{MS}_{\\text{error}}  = \\frac{\\text{SS}_{\\text{error}}}{df_{\\text{error}}}\\). Here, \\(df_{\\text{error}}= n- n_{\\text{groups}}\\).\n\\[ F = \\frac{\\text{MS}_{\\text{groups}}}{\\text{MS}_{\\text{error}}}\\]\nUsing our penguin data we can calculate the sum of squares values\n\npenguins2 &lt;- filter(penguins, !is.na(body_mass_g)) #remove nas for body mass\n\nsstable &lt;- augment(model2) |&gt; ## give us the means of each group in the .fitted row\n     mutate(sample_mean  = mean(body_mass_g)) |&gt; # calculate the total mean\n            summarise(n_groups = n_distinct(species),\n                      n=n(),\n        SS_total = sum((body_mass_g-sample_mean)^2), #total ss from body mass minus sample mean\n        SS_groups=sum((.fitted-sample_mean)^2), #model ss from group means minus total mean\n        SS_error = sum((body_mass_g-.fitted)^2), # error ss from body mass minus the group means\n)\n \nsstable\n\n# A tibble: 1 × 5\n  n_groups     n   SS_total  SS_groups  SS_error\n     &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1        3   342 219307697. 146864214. 72443483.\n\n\nNext we can calculate the mean square values\n\nftable &lt;- sstable |&gt;\n  mutate(\n    df_groups = n_groups-1,\n    df_error = n - n_groups,\n    ms_groups = SS_groups/df_groups,\n    ms_error = SS_error/df_error,\n    F_value = ms_groups/ms_error\n  )\n\nftable\n\n# A tibble: 1 × 10\n  n_groups     n   SS_total  SS_groups  SS_error df_groups df_error ms_groups\n     &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1        3   342 219307697. 146864214. 72443483.         2      339 73432107.\n# ℹ 2 more variables: ms_error &lt;dbl&gt;, F_value &lt;dbl&gt;\n\n\nWe can then use an F-test to test the hypothesis that F is different from 1.\n\npf(q = ftable$F_value, df1 = ftable$df_groups, df2 = ftable$df_error, lower.tail = FALSE)\n\n[1] 2.892368e-82\n\n\n\n\n4.4.3 Fitting an ANOVA with in R\nInstead of calculating everything by hand, we can use R to fit an ANOVA with the following code:\n\n#install.packages('car')\nlibrary(car)\n\nAnova(model2)\n\nAnova Table (Type II tests)\n\nResponse: body_mass_g\n             Sum Sq  Df F value    Pr(&gt;F)    \nspecies   146864214   2  343.63 &lt; 2.2e-16 ***\nResiduals  72443483 339                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that our calculations match the Anova function!\n\n\n4.4.4 Pairwise comparisons\nNow, to determine if there is a difference between any pair of categories, like Chinstrap and Gentoo, we can do a post-hoc pairwise comparison. Post-hoc tells us that we are doing these tests after we reject the null hypothesis that all of these categories have the same distribution.\nIf we want to look at all possible comparisons, this is called doing unplanned comparisons\n\n#install.packages(\"emmeans\")\nlibrary(emmeans)\n\nout.emmeans &lt;- emmeans(model2, specs = \"species\")\npairs(out.emmeans)\n\n contrast           estimate   SE  df t.ratio p.value\n Adelie - Chinstrap    -32.4 67.5 339  -0.480  0.8807\n Adelie - Gentoo     -1375.4 56.1 339 -24.495  &lt;.0001\n Chinstrap - Gentoo  -1342.9 69.9 339 -19.224  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\n\n4.4.5 Assumptions of an ANOVA\n\nEqual variance within groups The ANOVA null hypothesis is that variance within groups is the same as variance between groups, so having different amounts of variance within different groups will break the null hypothesis.\n\nA general rule of thumb is that as long as the variances of each group are within a factor of five of each other, you can use an ANOVA.\n\nNormally distributed residuals\n\nThis assumption is similar to what we have in a linear model. Fortunately, ANOVAs are pretty robust to this assumption",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "lm-categories.html#review-questions",
    "href": "lm-categories.html#review-questions",
    "title": "4  Linear models with categories and ANOVA",
    "section": "4.5 Review questions",
    "text": "4.5 Review questions\nMaize pollen can provide food for Anopheles arabiensis, a vector of malaria. Imagine that you are interested in whether maize cultivation levels affect malaria levels. You have data from Kebede et al. 2005 on maize cultivation level and rates of malaria from multiple locations in Ethiopia.\nHere is a plot of the data below\n\n\n\n\n\n\n\n\n\n\nWrite out an equation for a linear model testing for the effects of each yield category on malaria rates. Define each variable in the model (all the \\(b\\) values)\nWhat if you were interested in whether yield in general affected malaria rate? Would you use a linear model or an ANOVA? Why?\nDoes it look like this data matches the assumptions of an ANOVA? If not, why not?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear models with categories and ANOVA</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html",
    "href": "lm-continousvars.html",
    "title": "5  Linear models with continuous variables",
    "section": "",
    "text": "5.1 What are we doing here?\nMany of the variables you are interested are continuous, not categorical. In this section we’ll extend our linear models to incorporate continuous predictor variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html#motivating-example",
    "href": "lm-continousvars.html#motivating-example",
    "title": "5  Linear models with continuous variables",
    "section": "5.2 Motivating example",
    "text": "5.2 Motivating example\nContinuing with the penguin example data, imagine that we are interested in understanding how bill length determines body mass – do penguins with longer bills also have larger bodies?\n\npenguins2 &lt;- penguins |&gt; filter(!is.na(body_mass) & !is.na(bill_len)) ##filter out ones with NAs\nplot(penguins2$bill_len, penguins2$body_mass, bty=\"n\", ylab = \"body mass\", xlab = \"bill length\", lwd=2, col = palette()[1])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut do the penguins have hats? (Jeanette Warmuth, Public domain, via Wikimedia Commons)\n\n\n\nWe can plot out the data and see that there is potentially a relationship between bill length and body mass. But, imagine we have a biological reason for wanting to model body mass as a function of bill length. How would we do that?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html#linear-models-with-continuous-predictors",
    "href": "lm-continousvars.html#linear-models-with-continuous-predictors",
    "title": "5  Linear models with continuous variables",
    "section": "5.3 Linear models with continuous predictors",
    "text": "5.3 Linear models with continuous predictors\nInstead of letting the mean of the model vary between categories, we now build our model with the mean as a linear function of the predictor variable. This model is called linear regression and it has two components: the deterministic function and the stochastic function\n\n5.3.1 The deterministic function\nThe deterministic function describes how the explanatory variables relate to the conditional mean of the response variable. In this case, since this is a linear model, we are modelling the response variable as a line:\n\\[\\hat{y}_i = b_0 + b_1 \\times x_i\\] As before, \\(\\hat{y_i}\\) is the conditional mean of the response variable for an individual with value \\(x_i\\), \\(b_0\\) is the intercept or the conditional mean of an individual with a value of \\(x_i = 0\\) (although this value will not be meaningful if it lies far outside the range of the data).\nHowever, we can think of \\(b_1\\) now as the slope of the relationship between the predictor and response variable. This means that for every increase of 1 unit in \\(x\\), \\(y\\) increases by \\(b_1\\).\n\n\n5.3.2 The stochastic function\nThe stochastic function explains how the residuals are distributed around the conditional means predicted by the deterministic function. Since we are working with regular linear models, we will assume that the residuals are normally distributed.\n\\[ e_i \\sim N(0, \\sigma^2)\\]\nWe can combine the deterministic and stochastic functions into one model:\n\\[y_i = b_0 + b_1 \\times x_i + N(0,\\sigma^2)\\]\nor\n\\[y_i \\sim N(b_0 + b_1 \\times x_i, \\sigma^2)\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html#estimating-parameters-of-the-determinstic-function",
    "href": "lm-continousvars.html#estimating-parameters-of-the-determinstic-function",
    "title": "5  Linear models with continuous variables",
    "section": "5.4 Estimating parameters of the determinstic function",
    "text": "5.4 Estimating parameters of the determinstic function\nHere is some math for estimating these parameters using the method of least squares (this is different from maximum likelihood inference). The idea behind this method is that we are finding the line is closest to all the points. We measure closeness by finding the distance, in vertical space, between each point and the line. These distances are squared and the line with the smallest values of these squares is considered to be the best fitting regression line.\nFirst, we start with the covariance between our response variable \\(y\\) and our explanatory variable \\(x\\). The covariance tells us about how much \\(x\\) and \\(y\\) jointly deviate from their means – a high covariance means that an individual with a high value of \\(x\\) likely also has a high value of \\(y\\) while a low covariance tells us that an individual with a high value of \\(x\\) could have any value of \\(y\\).\nWe write the covariance of \\(x\\) and \\(y\\) as \\(\\text{cov}_{x,y}\\) and calculate it using the following equation:\n\\[ \\text{cov}_{x,y} = \\frac{1}{n-1} \\sum(x_i - \\bar{x})\\times(y_i-\\bar{y})\\]\nwhere \\(n\\) is the number of individuals, \\(x_i\\) and \\(y_i\\) are the values for the \\(i^{th}\\) individual, and \\(\\bar{x}\\) and \\(\\bar{y}\\) are the mean values of \\(x\\) and \\(y\\).\nNotice that for a covariance, which variable is the explanatory variable and which is the response variable doesn’t matter – you could switch \\(x\\) and \\(y\\) and the equation would remain the same.\nCovariance is related to a correlation (\\(r_{x,y}\\)), which tells us how reliably \\(x\\) and \\(y\\) covary by standardizing the covariance by how much \\(x\\) and \\(y\\) themselves vary, quantified using the standard deviations of \\(x\\) (\\(s_x\\)) and \\(y\\) (\\(s_y\\).\n\\[ r_{x,y} = \\frac{cov_{x,y}}{s_{x} \\times s_{y}}\\]\nAgain, the order of variables doesn’t matter. \\(r_{x,y} = r_{y,x}\\)\nIn contrast, linear regression distinguishes between the response and explanatory variables because, here, the goal is to build a model that reduces residuals.\nHowever, the equation for calculating the slope of the linear regression line (\\(b_1\\)) looks somewhat similar to the correlation but this time we only standardize by \\(s_x\\).\n\\[b_1 = \\frac{cov_{x,y}}{s^2_x}  =  \\frac{\\frac{1}{(n-1)}\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\frac{1}{(n-1)}\\sum(x_i-\\bar{x})^2}=\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}\\]\nNote that \\(x\\) and \\(y\\) are no longer interchangeable.\n\n5.4.1 Some notes on intuition\nWhile the equations for correlation and for the regression coefficient are similar, it’s possible to have a large correlation and a small regression slope (and vice versa).\nIn general, lots of variation within \\(x\\) will increase \\(s_x\\) and reduce the regression coefficient.\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Fitting with lm()\nLet’s try applying a linear model to our penguin data. We’ll use the lm() function in R.\n\\[\\text{Body mass}_i = b_0 + b_1 \\times \\text{Bill length}_i \\]\n\nmodel1 &lt;- lm(body_mass ~ bill_len, data=penguins2)\nsummary(model1)\n\n\nCall:\nlm(formula = body_mass ~ bill_len, data = penguins2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1762.08  -446.98    32.59   462.31  1636.86 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  362.307    283.345   1.279    0.202    \nbill_len      87.415      6.402  13.654   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 645.4 on 340 degrees of freedom\nMultiple R-squared:  0.3542,    Adjusted R-squared:  0.3523 \nF-statistic: 186.4 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\n\nBased on these estimates, we can now write out our model as\n\\[\\text{Body mass}_i = 362.31 + 87.42 \\times \\text{Bill length}_i \\]\nWe can interpret this as for every mm of bill length increase, body mass increases by 87 grams.\nWe can also estimate \\(\\sigma^2\\), the variance of the residuals.\n\nsigma(model1)\n\n[1] 645.4333\n\n\nThis allows us to write our model including the stochastic function as:\n\\[ \\text{Body mass}_i \\sim N(362.31 + 87.42 \\times \\text{Bill length}_i, 645.43) \\]\nWe can also plot the predictions from this model over the raw data.\n\nplot(penguins2$bill_len, penguins2$body_mass, bty=\"n\", ylab = \"body mass\", xlab = \"bill length\", lwd=2, col = palette()[1])\nabline(model1, col=palette()[3], lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t get any ideas (Bruce Raymond, United States Navy, credited photographer, Public domain, via Wikimedia Commons)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html#residuals",
    "href": "lm-continousvars.html#residuals",
    "title": "5  Linear models with continuous variables",
    "section": "5.5 Residuals",
    "text": "5.5 Residuals\nOften we assume that the response data needs to be normally distributed to use a linear model. However, the real assumption of the model is not that the response data is normally distributed, but that the residuals are normally distributed.\nLet’s look at the residuals from this model\n\nlibrary(broom)\nmyResiduals &lt;- augment(model1) |&gt; select(body_mass, .fitted, .resid)\n\nhist(myResiduals$.resid, main=\"\", xlab = \"residuals\", border=\"white\", col=palette()[2])\n\n\n\n\n\n\n\n\nIt helps me to imagine that the values themselves follow a normal distribution centered around a mean that follows the model predictions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html#a-note-on-history-regression-to-the-mean",
    "href": "lm-continousvars.html#a-note-on-history-regression-to-the-mean",
    "title": "5  Linear models with continuous variables",
    "section": "5.6 A note on history: regression to the mean",
    "text": "5.6 A note on history: regression to the mean\nRegression was initially developed by Francis Galton. Galton was interested in understanding heritability of traits: how much was an individual’s trait determined by the genetic material they inherited from their parents. Galton focused on height because it was easy to measure. However, Galton’s main motivation was understanding the heritability of intelligence. He was a strong proponent of Eugenics and had a lot of gross ideas. See (Kennedy-Shaffer 2024) for more on this.\n\nKennedy-Shaffer, Lee. 2024. “Teaching the Difficult Past of Statistics to Improve the Future.” Journal of Statistics and Data Science Education 32 (1): 108–19.\n\n\n\nA plot from Galton’s “Essays in eugenics” (1909) showing his conception of regression toward the mean\n\n\nGalton estimated heritability by calculating the slope of the relationship between the average height of two parents and their offspring. He was surprised to see that the very tallest parents tended to have offspring that were shorter than expected and the shortest parents had offspring that were taller than expected(Stigler 1997). He called this “regression towards the mean” and it is this phrase that lends it’s name to “linear regression” since it occurs in many contexts, not just the inheritance of traits. Galton thought that he had discovered a fundamental property of evolution that preserved a consistent amount of genetic variation across generations. Regression toward the mean has continued to be a confusing topic for many (including me!) so I want to walk through it in the hopes that it will help us better understand regression in general.\n\nStigler, Stephen M. 1997. “Regression Towards the Mean, Historically Considered.” Statistical Methods in Medical Research 6 (2): 103–14.\nHowever, regression to the mean is just the result of measurement error. We can imagine that every value in a data set is the sum of some true value and measurement error. In quantitative genetics, we think about this a lot – traits are the sum of a genetic component and an environmental component (or error). If we take the extremes of a distribution we will get individuals who have both extreme true values and extreme values of error. So in Galton’s first example, the tallest parents both had the tallest genetic component and the tallest environmental component (along with any other types of error). They transmit on their genetics but not their environment and other errors, so their offspring will generally be shorter.\nAnother example that may be helpful is thinking about exam grades. Large undergrad classes often give multiple midterm exams with the goal of measuring how well students understand the course material. We could imagine that in the first midterm, the students with the very best grades are not only the ones with the best understanding of the material, but also the ones that happened to get the best night’s sleep, weren’t sick, aren’t distracted by a personal issue, etc. In the second midterm, these students may still understand the material but may not have the same environmental benefits and so, on average, the best students will seem to do slightly less well on the second midterm (and the worst students will appear to do slightly better). For more on regression to the mean, check out this podcast\nAny time you do a regression with two quantities that have measurement error, you will likely find regression to the mean. This process can have real consequences for studies of the effect of interventions. For example, if a researcher identifies individuals with extremely poor values of some health-related measurement, and then gives these individuals an intervention, regression towards the mean will cause these individuals to appear to improve in the next round of measurement (Barnett, Van Der Pols, and Dobson 2005). This is why experimental controls are really important!\n\nBarnett, Adrian G, Jolieke C Van Der Pols, and Annette J Dobson. 2005. “Regression to the Mean: What It Is and How to Deal with It.” International Journal of Epidemiology 34 (1): 215–20.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html#assessing-significance",
    "href": "lm-continousvars.html#assessing-significance",
    "title": "5  Linear models with continuous variables",
    "section": "5.7 Assessing significance",
    "text": "5.7 Assessing significance\nWe have different tools for quantifying how confident we can be in the predictions made by a regression. They vary based on what type of prediction they are making. We won’t work through the math here but it is useful to know what is actually being predicted by these tools.\n\n5.7.1 Confidence intervals\nConfidence intervals relate to predicting the mean value of the response variable \\(y\\) for individuals with a specific value of the explanatory variable \\(x\\). When we calculate 95% confidence intervals we are bracketing the true regression line 95% of the time. Specifically, this means that if we repeated our sampling, analysis, and confidence interval generation many many times, we would find the true slope 95% of the time. For more on the history of confidence intervals, see this nice blog post, which includes the very relatable quote from Jerzy Neyman: “I simply cannot work, the crisis and the struggle for existence takes all my time and energy.”\n\n\n5.7.2 Prediction intervals\nPrediction intervals give us an estimate of our confidence in predicting a value for the response variable \\(y\\) for one individual with a specific value of the explanatory variable \\(x\\). When we generate 95% prediction intervals we are bracketing 95% of potential individuals.\nPrediction intervals will be much wider than confidence intervals because they include more variation.\n\n\n5.7.3 Extrapolation\nExtrapolation is the prediction of \\(y\\) outside the measured interval of \\(x\\). It’s generally a bad idea because we have no way of knowing the relationship between \\(x\\) and \\(y\\) outside of what we’ve observed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html#linear-regression-as-an-anova",
    "href": "lm-continousvars.html#linear-regression-as-an-anova",
    "title": "5  Linear models with continuous variables",
    "section": "5.8 Linear regression as an ANOVA",
    "text": "5.8 Linear regression as an ANOVA\nWe can partition variation in our regression the same way we did in ANOVA. The concepts are similar but the math is slightly different.\n\n\n\n\n\nAn ANOVA framework for a linear regression. A Shows the difference between each observation, \\(Y_i\\), and the mean, \\(\\overline{Y}\\). This is the basis for calculating \\(MS_{total}\\). B Shows the difference between each predicted value \\(\\widehat{Y_i}\\) and the mean, \\(\\overline{Y}\\). This is the basis for calculating \\(MS_{model}\\). C Shows the difference between each observation, \\(Y_i\\), and its predicted value \\(\\widehat{Y_i}\\). This is the basis for calculating \\(MS_{error}\\).\n\n\n\n\nAs before, we can calculate the total sum of squares (\\(SS_{total}\\), panel A) as the sum of the squared differences between the model and the mean (\\(SS_{model}\\), panel B) and the squared differences between each individual and the model (\\(SS_{residual}\\)). Better fitting models will have higher \\(SS_{model}\\) relative to \\(SS_{residual}\\).\nWe calculate all of these values as follows: \\[SS_{total} = \\sum{(y_i - \\bar{y})^2}\\] \\[SS_{model} = \\sum{(\\hat{y_i} - \\bar{y})^2}\\] \\[SS_{residual} = \\sum{(y_i - \\bar{y})^2} \\] \\[MS_{model} = SS_{model}/df_{model}\\] \\[MS_{residual} =  SS_{residual}/df_{residual}\\] \\[df_{model} = 1 \\] \\[df_{residual} = n-2\\]\nwhere \\(\\bar{y}\\) is the mean response variable for the whole sample and \\(\\hat{y_i}\\) is the model predicted value for individual \\(i\\) and there are \\(n\\) individuals in the sample.\nWe can calculate these values by hand or get them from the anova function\n\nlibrary(car)\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: body_mass\n           Df    Sum Sq  Mean Sq F value    Pr(&gt;F)    \nbill_len    1  77669072 77669072  186.44 &lt; 2.2e-16 ***\nResiduals 340 141638626   416584                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "lm-continousvars.html#review-questions",
    "href": "lm-continousvars.html#review-questions",
    "title": "5  Linear models with continuous variables",
    "section": "5.9 Review questions",
    "text": "5.9 Review questions\n\nImagine you are a penguin researcher and you measure additional data on lifespan. You want to construct a linear model that models lifespan as a function of body mass. Write out the equation describing this model, and describe which parts of the equation correspond to the stochastic function vs the deterministic function.\nWhat are the key differences between covariance, correlation, and linear regression?\nWhat is the key assumption about the normal distribution made in a linear regression?\nYour friend shows you an analysis of some new data: they have conducted a regression and calculated a 95% confidence interval. They tell you “95% of my data points should fall in this confidence interval.” Do you agree with them? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models with continuous variables</span>"
    ]
  },
  {
    "objectID": "pvalues.html",
    "href": "pvalues.html",
    "title": "6  P values",
    "section": "",
    "text": "6.1 What are we doing here?\nFor the last few weeks we’ve been building linear models to estimate population parameters. Now we’ll discuss how to test for whether these estimates are significant. And, if you’re wondering what we mean by significant, that’s a good thing to wonder – stay tuned.\nAs we work through this section we’ll return to our penguin data. We’ll start by thinking about how to tell if the difference in body mass between penguins of different species is significant.\nWe previously used a linear model to estimate the conditional mean body mass of each species using the following code:\n## filter out chinstraps and anything with missing data\npenguins2 &lt;- penguins |&gt;\n  filter(!is.na(body_mass) ,species != \"Chinstrap\") |&gt; \n  droplevels()\n\nmodel1 &lt;- lm(body_mass ~ species, data=penguins2)\nsummary(model1)\n\n\nCall:\nlm(formula = body_mass ~ species, data = penguins2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1126.02  -369.50   -26.02   343.00  1223.98 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    3700.66      39.02   94.83   &lt;2e-16 ***\nspeciesGentoo  1375.35      58.24   23.61   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 479.5 on 272 degrees of freedom\nMultiple R-squared:  0.6721,    Adjusted R-squared:  0.6709 \nF-statistic: 557.6 on 1 and 272 DF,  p-value: &lt; 2.2e-16\n\nplot(penguins2$body_mass, bty=\"n\", ylab = \"body mass\", \n     col=penguins2$species, lwd=2)\nlegend('topleft',levels(penguins2$species),bty=\"n\", pch=1, pt.lwd=2, col=palette())\nChinstrap Penguins – Christopher Michel, CC BY 2.0 https://creativecommons.org/licenses/by/2.0, via Wikimedia Commons",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "pvalues.html#learning-goals",
    "href": "pvalues.html#learning-goals",
    "title": "6  P values",
    "section": "6.2 Learning goals",
    "text": "6.2 Learning goals\n\nApply and interpret different ways to measure variation\nDefine a scientific hypothesis, determine appropriate null hypotheses.\nCalculate and interpret p values for a linear model based on t and F distributions.\nCalculate and interpet p values from likelihood ratio tests.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "pvalues.html#the-population-vs-the-sample-recap",
    "href": "pvalues.html#the-population-vs-the-sample-recap",
    "title": "6  P values",
    "section": "6.3 The population vs the sample recap",
    "text": "6.3 The population vs the sample recap\nThe population we are studying has true values and parameters. However, without measuring every individual in the population, we will not get to know these true parameters. Instead, we sample the population and use statistics to make inferences about the population.\nThe sampling distribution is the distribution of parameters we would get from sampling multiple times. It tells us about the amount of error generated from taking a sample. We need to know this to determine how much certainty we can get from the sample.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "pvalues.html#measuring-variation",
    "href": "pvalues.html#measuring-variation",
    "title": "6  P values",
    "section": "6.4 Measuring variation",
    "text": "6.4 Measuring variation\nThere are various ways to measure variation in a data set.\nThe variance is the expectation of the differences between individuals and the population mean. For a set of data points \\(X\\) with mean \\(\\mu\\), \\(\\text{Var}(X) = E[(X - \\mu)^2]\\). (Remember that the \\(E\\), or ‘expectation’, is a fancy math way of describing the average). Practically we calculate this as \\(\\frac{\\Sigma (X - \\mu)^2}{n-1}\\) where \\(n\\) is the number of individuals in the sample. Dividing by \\(n-1\\) and not \\(n\\) deals with bias that arises because the sample mean is calculated from the sample, so all the points in the sample mean are a bit closer to it than they are to the true mean.\nThe code below will calculate the variance and standard deviation of our Adelie penguin body masses.\n\nmyN &lt;- penguins2 |&gt; filter(species==\"Adelie\") |&gt; count() #get n\n\nAdelieVar &lt;- penguins2 |&gt; filter(species==\"Adelie\") |&gt;#filter only Adelies\n  mutate(diffs = (body_mass - mean(body_mass))^2)  |&gt; #calculate sqaured difference between each penguin's body mass and mean body mass\n  summarise(var = sum(diffs))/(myN-1) #get the expectation\n\nAdelieVar\n\n       var\n1 210282.9\n\n\nYou can also use a built in R function var().\n\npenguins2 |&gt; filter(species==\"Adelie\") |&gt; select(body_mass) |&gt; var()\n\n          body_mass\nbody_mass  210282.9\n\n\nThe standard deviation is the square root of the variance. The standard deviation can be useful because it is measured in the same units as the original data (while the variance is in squared units). This property makes it a bit easier to interpret. The standard deviation is sometimes written as \\(\\sigma\\) compared to the variance, which is written as \\(\\sigma^2\\) so we can express the standard deviation kind of funnily as: \\(\\sigma = \\sqrt{\\sigma^2}\\). When we calculate the standard deviation of a sample, it is often refered to as \\(s\\) to distinguish it from the true standard deviation of the population (\\(\\sigma\\)).\nWe can get the standard deviation by taking the square root of the variance we just calculated.\n\nsqrt(AdelieVar$var)\n\n[1] 458.5661\n\n\nor by using the built in R function sd().\n\nadelieMasses &lt;- penguins2 |&gt; filter(species==\"Adelie\") |&gt; select(body_mass) \nsd(adelieMasses$body_mass)\n\n[1] 458.5661\n\n\n\n6.4.1 Standard error\nTo quantify the variance in the sampling distribution, we need to account for the size of the sample. We expect larger samples to have less variation (due to the law of large numbers!).\nThe standard error is the standard deviation adjusted for the sample size. So for a sample size of \\(n\\) and a sample standard deviation of \\(s\\), the standard error (\\(SE\\)) equals \\(\\frac{s}{\\sqrt{n-1}}\\).\nHere is code for calculating the standard error of our sample of Adelie penguins.\n\nadelieMasses &lt;- penguins2 |&gt; filter(species==\"Adelie\") |&gt; select(body_mass) \nsdAdelies = sd(adelieMasses$body_mass)/(sqrt(nrow(adelieMasses)-1))\nsdAdelies\n\n[1] 37.44177\n\n\nAnd here is a plot of how the standard error scales with sample size (from increasingly large samples of penguins)\n\nset.seed(1000)\nsampleAdelies &lt;- function(n){\n  mySample = sample(adelieMasses$body_mass, size=n,replace=F)\n  return(sd(mySample)/(sqrt(n-1)))\n}\n\nmyStderrors &lt;- sapply(2:150, sampleAdelies)\n\nplot(2:150, myStderrors, col = palette()[2], bty=\"n\", xlab=\"n\", ylab = \"standard error\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "pvalues.html#hypothesis-testing",
    "href": "pvalues.html#hypothesis-testing",
    "title": "6  P values",
    "section": "6.5 Hypothesis testing",
    "text": "6.5 Hypothesis testing\nMuch of statistics uses a hypothesis testing framework whether we try to distinguish between two possibilities: a null hypothesis and an alternative hypothesis.\nStatistical hypotheses differ from the scientific hypotheses you may be more familiar with. Scientific hypotheses make claims about phenomena and the processes that cause them. For example, a scientific hypothesis might be “Plants from the desert are adapted to drought conditions” while a statistical hypothesis would be “The mean fitness of desert genotypes is higher than the mean fitness of grassland genotypes in dry conditions”. Part of designing a good experiment is figuring out how to link statistical hypotheses to scientific hypotheses, and then do the experiments to test the statistical hypotheses.\n\n6.5.1 Null hypothesis\nThe null hypothesis is a claim about a population parameter that tells us what we expect if the process from a scientific hypothesis is not occurring. The null hypothesis should be something that would be interesting to reject. In the previous example, a null hypothesis would be that plants from different environments do not have different fitnesses in dry conditions. We often abbreviate the null hypothesis as \\(H_0\\).\nA null hypothesis needs a corresponding alternative hypothesis (\\(H_A\\)). These alternative hypotheses are usually more biologically interesting. In our previous example, \\(H_A\\) might be that “Desert genotypes have higher fitness than grassland genotypes in dry conditions”.\nNot all science relies upon a null and alternative hypothesis testing framework – this is just one potential way to do statistics. Some scientists may be more interested in making specific parameter estimates, or in exploring which of many factors are important for a process. It’s also often good scientific strategy to set up experiments where instead of rejecting a null hypothesis, your results will let you distinguish among multiple interesting potential hypotheses (making your results exciting no matter what happens).\n\n\n6.5.2 Frameworks for rejecting the null hypothesis\nNow, how do we distinguish betwen the null hypothesis and alternative hypothesis? We often use a test statistic, which is calculated from the data, and compare it to a null distribution of our expectation for the test statistic under the hypothesis. Next we’ll lay out some of the statistics and distributions that are commonly used.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "pvalues.html#t-distribution",
    "href": "pvalues.html#t-distribution",
    "title": "6  P values",
    "section": "6.6 T distribution",
    "text": "6.6 T distribution\nSo we have a null hypothesis and we have a sample mean. How do we test for whether our sample is expected under the null hypothesis?\nThe t distribution tells us about the distributions of sample means. We can use it to determine how likely it is that our sample mean falls into the distribution of sample means expected under the null hypothesis.\nWe can use this distribution to calculate a t value describing the number of standard errors between our sample mean (\\(\\bar{x}\\)) and the mean of the null distribution (\\(\\mu_0\\)). We calculate this with the following equation:\n\\[ t = \\frac{\\bar{x}-\\mu_0}{SE_x}\\]\nReturning to our penguins, we might have a somewhat contrived null hypothesis that the mean body mass of Adelie penguins is 4000g and want to test to see if our sample mean is consistent with the null hypothesis. We can calculate \\(t\\) using the following code:\n\nxbar = mean(adelieMasses$body_mass)\nmu0 = 4000\n\nmyT = (xbar-mu0)/sdAdelies\nmyT\n\n[1] -7.994755\n\n\nThis tells us that the mean body mass of our Adelie penguin sample is 5.3 standard errors below 3500g. This kind of makes sense – we can see that if we add the mean mass of the Adelie penguins in the sample to the product of our t statistic and the standard error of the Adelie masses, we get 3500.\n\nmean(adelieMasses$body_mass)-myT*sdAdelies\n\n[1] 4000\n\n\nThis seems pretty different! But how different?\n\n6.6.1 Degrees of freedom\nFirst we need to figure out the degrees of freedom that help us interpret \\(t\\). In this case, the degrees of freedom are equal to \\(n-1\\) for a sample size of \\(n\\).\n\n\n6.6.2 Calculating a p value\np values tell us about the proportion of the null distribution that is more extreme than our observed test statistic. So in a one-tailed test, a p value of 0.01 tells us that 1% of the null distribution is more extreme than the observed data. We can translate this into telling us that we expect to see data like ours 1% of the time even if the null hypothesis is true.\nTo calculate a p value for a t statistic, we can use the pt() function. The pt() function is analagous to pnorm() and other related functions.\n\ndf = nrow(adelieMasses)-1\np_val &lt;- 2*pt(q = myT, df=df, lower.tail=T) \np_val\n\n[1] 3.218126e-13\n\n\nCongrats – you’ve just calculated a p value for a t test!\n\n\n6.6.3 P values in linear regression\nYou can use the same procedure to calculate the p values for the slope of a linear regression. Generally, our null expectation for a linear regression is that the slope of the regression line (or the effect size corresponding to our predictor variable) is zero. We can think of our null hypothesis being that there is no relationship between the predictor and the response variable so that the slope of the linear regression line is 0. Our alternative hypothesis is that there the predictor variable does predict the response variable, and the slope of the linear regression line is not equal to 0.\nInstead of testing for a difference in sample means, we are now testing for a difference between the slope of the regression line and 0. Fortunately, this value, divided by its standard error, will follow a t distribution and we can use the same procedure as above to calculate a p value.\nLet’s try an example, testing a model where the predictor variable is bill length and the response variable is body mass.\n\npenguins3 &lt;- penguins |&gt; filter(!is.na(body_mass)) #filter out NAs\n\nmodel2 &lt;- lm(body_mass ~ bill_len, data=penguins3)\nmodel2\n\n\nCall:\nlm(formula = body_mass ~ bill_len, data = penguins3)\n\nCoefficients:\n(Intercept)     bill_len  \n     362.31        87.42  \n\n\nThe effect size of bill length (\\(b\\)), which is the same as the slope of the regression line, is 87.42. But can we reject the null hypothesis that the slope is 0?\nFirst we calculate the standard error of the slope (\\(SE_b\\)).\n\\[ SE_b = \\sqrt{\\frac{\\Sigma(Y_i-\\hat{Y_i})^2}{\\Sigma(X_i - \\hat{X_i})^2}} \\times \\frac{1}{\\sqrt{(n-2)}}\\]\nThis equation is kind of gnarly. Here.s how I interpret it. \\(\\Sigma(Y_i-\\hat{Y_i})^2\\) is the variance of the response variable, and the square root of it is the standard deviation of that response variable. Similarly, \\(\\Sigma(X_i-\\hat{X_i})^2\\) is the variance of the explanatory variable the the square root of it is the standard deviation of the response variable. By dividing these two values we get the standard deviation of the slope. Finally, to get the standard error, we divide the standard deviation by the square root of the degrees of freedom, which in this case is \\(n-2\\) since have used up two degrees of freedom in the calculation of slope and intercept.\n\nvarY &lt;- penguins3 |&gt; mutate(diffs = (body_mass - mean(body_mass))^2) |&gt;\n  summarise(var=sum(diffs))\n\nvarX &lt;- penguins3 |&gt; mutate(diffs = (bill_len - mean(bill_len))^2) |&gt;\n  summarise(var=sum(diffs))\ndfb &lt;- nrow(penguins3)-2\nSEb &lt;- sqrt(varY/(varX*dfb))[[1]]\n\nNow we can calculate t as the difference between our observed slope or effect size (b) and the mean of the null (0), divided by the standard error.\n\\[ t = \\frac{b-0}{SE_b}\\]\n\nmyT = model2$coefficients[2][[1]]/SEb\nmyT\n\n[1] 10.97328\n\n\nAnd then we calculate p from the t distribution.\n\np_val &lt;- 2*pt(q=myT, df=dfb, lower.tail=F)\np_val\n\n[1] 3.466897e-24\n\n\n\n\n6.6.4 Using R built in functions\nOf course, R has a built in function for doing this.\n\nmodel1&lt;-t.test(adelieMasses, mu=4000, alternative=\"two.sided\")\nmodel1$p.value\n\n[1] 2.765774e-13\n\n\nWe can also do this for a linear model\n\nsummary(model2)\n\n\nCall:\nlm(formula = body_mass ~ bill_len, data = penguins3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1762.08  -446.98    32.59   462.31  1636.86 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  362.307    283.345   1.279    0.202    \nbill_len      87.415      6.402  13.654   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 645.4 on 340 degrees of freedom\nMultiple R-squared:  0.3542,    Adjusted R-squared:  0.3523 \nF-statistic: 186.4 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\nsummary(model2)[[4]][2,4] ##\n\n[1] 3.808283e-34",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "pvalues.html#review-questions",
    "href": "pvalues.html#review-questions",
    "title": "6  P values",
    "section": "6.7 Review questions",
    "text": "6.7 Review questions\nThe following code looks at variability in penguin bill length.\n\nmyN &lt;- nrow(penguins2)\n\nsumSquares &lt;- penguins2 |&gt; \n  mutate(diffs = (bill_len - mean(bill_len))^2)  |&gt; \n  summarise(sum(diffs))\n\nvalueA &lt;- sqrt(sumSquares[1,1]/(myN-1))\nvalueB &lt;- sumSquares[1,1]/(myN-1)\nvalueC &lt;- sqrt(sumSquares[1,1]/(myN-1))/sqrt(myN-1)\n\n\nMatch ‘valueA’,‘valueB’, and ‘valueC’ with the variance, standard deviation, and standard error.\nImagine that you are interested in understanding if flipper length affects bill length in the penguin dataset. Describe a potential null hypothesis for this test.\nUse lm() to conduct a linear model for the scenario described above. Look at the p value that is outputted from lm() and describe, in words, what it tells us about the relationship between flipper length and bill length.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>P values</span>"
    ]
  },
  {
    "objectID": "lm-multiplepredictors.html",
    "href": "lm-multiplepredictors.html",
    "title": "7  Linear models with multiple predictors",
    "section": "",
    "text": "7.1 What are we doing here?\nNow that we’ve learned the basics of how to build models with categorical and continuous variables, we can start to combine them.\nBut, before we start to build more complicated models, we should go over the assumptions of a linear model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "lm-multiplepredictors.html#linear-model-assumptions",
    "href": "lm-multiplepredictors.html#linear-model-assumptions",
    "title": "7  Linear models with multiple predictors",
    "section": "7.2 Linear model assumptions",
    "text": "7.2 Linear model assumptions\n\n7.2.1 Linearity\nWe assume that we can model our response variable by adding up the values in the model equation. The alternative could be multiplying variables together, dividing them, or putting a variable in the exponent (or other things).\nThe plots below show data that violates this assumption because it was simulated using an exponential equation of the form \\(y_i = b^{x_i + e_i}\\). On the left you can see that the linear regression line does not fit the curve very well and on the right it is clear that the residuals are not evenly distributed across the values of Y.\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Homoschedasticity\nWe assume that the variance of our residuals is the same across all of our data. If this is not true and the variance changes across residuals, then our data would be heteroschedastic. The two plots below show data that is heteroschedastic. On left you can see an explanatory and response variable plotted with a regression line. On right is a plot of the residuals of the regression and it is clear that the variance of the residual increases as the explanatory variable increases. In fact, this data was simulated such that the variance of the residual for each point is a function of the explanatory variable.\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Independence of units\nWe assume that the observations of data point are independent from each other. Another way of saying this is that we assume that the residuals are independent from each other.\n\n\n7.2.4 Distribution\nFor general (not generalized) linear models, we assume that our residuals are drawn from a single normal distribution. This somewhat overlaps with some of the previous assumptions – for example, in heteroschedastic data, the variance of the distribution of the residuals differes across the range of the data.\n\n\n7.2.5 Looking for deviations from assumptions\nAs shown above for some of these cases, you can often diagnose problems with a model fit caused by violations of assumptions by looking at a plot of the residuals against the explanatory variable. You can make these and other diagnostic plots by using the plot() function on the output of lm().",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "lm-multiplepredictors.html#motivating-example",
    "href": "lm-multiplepredictors.html#motivating-example",
    "title": "7  Linear models with multiple predictors",
    "section": "7.3 Motivating example",
    "text": "7.3 Motivating example\nIn the previous sections, we found that both species and bill length affect body mass. Can we build a model that incorporates both?\nTo make things a bit simpler we will only look at Adelie and Gentoo penguins\n\npenguins2 &lt;- penguins |&gt; filter(!is.na(body_mass) & !is.na(bill_len) & !species==\"Chinstrap\") |&gt; droplevels() ##filter out missing data and chinstrap\n\n\nplot(penguins2$bill_len, penguins2$body_mass, bty=\"n\", ylab = \"body mass\", xlab = \"bill length\", lwd=2, col = penguins2$species)\nlegend('topleft',levels(penguins2$species),bty=\"n\", pch=1, pt.lwd=2, col=palette())\n\n\n\n\n\n\n\n\nExamining the data, we can generate some hypotheses about how bill length and species relate to body mass.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "lm-multiplepredictors.html#building-the-model",
    "href": "lm-multiplepredictors.html#building-the-model",
    "title": "7  Linear models with multiple predictors",
    "section": "7.4 Building the model",
    "text": "7.4 Building the model\nLet’s think about a model of some response variable (\\(y\\)) based on two explanatory variables, \\(x_1\\) and \\(y_1\\).\nOur model looks like it did before, except now it has multiple predictors, each with its own coefficient.\n\\[ \\hat{y}_i = b_0 + b_1x_1i + b_2x_2i e_i\\]\nin our penguin example\n\\[ \\text{Body mass} = b_0 + b_1 \\text{Gentoo} + b_2 \\text{bill length} + e_i \\]\nwhere \\(b_1\\) is the difference in means between Gentoo and Adelie and \\(b_2\\) is the effect of increasing bill length one unit on body mass. \\(b_0\\) is the intercept, or the predicted body mass for an Adelie penguin with a bill length of 0 (we may question how useful this value is for predictions but it is certainly useful for the model). Note that this assumes that Adelie and Gentoo penguins have the same slope, which may not always be the case.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "lm-multiplepredictors.html#fitting-with-lm",
    "href": "lm-multiplepredictors.html#fitting-with-lm",
    "title": "7  Linear models with multiple predictors",
    "section": "7.5 Fitting with lm()",
    "text": "7.5 Fitting with lm()\n\nmodel1 &lt;- lm(body_mass~bill_len+species,data=penguins2)\nmodel1\n\n\nCall:\nlm(formula = body_mass ~ bill_len + species, data = penguins2)\n\nCoefficients:\n  (Intercept)       bill_len  speciesGentoo  \n       -267.6          102.3          484.0  \n\n\nWe can now rewrite our equation with these estimated values\n\\[ \\text{Body mass} = -267.6 + 484.0 \\times \\text{Gentoo} + 102.3 \\times \\text{bill length} + e_i \\]\nNote that the intercept does not provide a particularly useful prediction.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "lm-multiplepredictors.html#interactions",
    "href": "lm-multiplepredictors.html#interactions",
    "title": "7  Linear models with multiple predictors",
    "section": "7.6 Interactions",
    "text": "7.6 Interactions\nI noted before that our model assumed that the slope of the relationship between bill length and body mass is the same for both species. This may not always be the case.\nWe can extend our model to incorporate differences in slopes between categories (or between individuals with different values for a continuous variable). This pattern is called a statistical interaction.\n\\[ \\hat{y}_i = b_0 + b_1x_1i + b_2x_2i + b_3 x_1 \\times x_2 e_i\\]\nin our penguin example\n\\[ \\text{Body mass} = b_0 + b_1 \\text{Gentoo} + b_2 \\text{bill length} + b_3 \\text{Gentoo} \\times \\text{bill length} + e_i \\]\nWe can fit the model with lm() using the following code:\n\nmodel2 &lt;- lm(body_mass~bill_len+species+bill_len*species,data=penguins2)\nsummary(model2)\n\n\nCall:\nlm(formula = body_mass ~ bill_len + species + bill_len * species, \n    data = penguins2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-782.6 -261.8  -12.9  249.2 1126.3 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               34.88     453.96   0.077    0.939    \nbill_len                  94.50      11.68   8.094 1.98e-14 ***\nspeciesGentoo           -158.71     699.81  -0.227    0.821    \nbill_len:speciesGentoo    14.96      16.17   0.925    0.356    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 380.8 on 270 degrees of freedom\nMultiple R-squared:  0.7947,    Adjusted R-squared:  0.7924 \nF-statistic: 348.4 on 3 and 270 DF,  p-value: &lt; 2.2e-16\n\n\nIn this case the statistical interaction isn’t significant, suggesting that the slope of the relationship between bill length and mass does not differ between species.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "lm-multiplepredictors.html#the-general-linear-model",
    "href": "lm-multiplepredictors.html#the-general-linear-model",
    "title": "7  Linear models with multiple predictors",
    "section": "7.7 The General Linear Model",
    "text": "7.7 The General Linear Model\nFor the last few classes, we have been building up to a General Linear Model (not to be confused with a Generalized Linear Model!).\n\n7.7.1 Assumptions of the general linear model\nThere are a few major assumptions of general linear models.\n\nThe predictor \\(x_1\\) is linearly related to the response variable \\(y_1\\).\nThe errors (residuals), \\(e_i\\), are independent and identically distributed\nThe errors, \\(e_i\\), have a constant variance \\(\\sigma^2\\)\nIf performing inference, the errors, \\(e_i\\), are normally distributed\n\nHowever, if your data doesn’t seem to meet these assumptions, you have options.\n\nYou can transform your data\nYou can pick a new model\nYou can continue onward. Linear models (particularly ANOVAs) tend to be robust to assumption breaking, but only to a point\n\nWhatever you decide to do, be transparent in your paper!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "lm-multiplepredictors.html#matrix-notation",
    "href": "lm-multiplepredictors.html#matrix-notation",
    "title": "7  Linear models with multiple predictors",
    "section": "7.8 Matrix notation",
    "text": "7.8 Matrix notation\nIt might be helpful for some of you to see a linear model written out in matrix notation. The equation of three matrices below describes a linear model.\n\\[\\begin{equation}\n\\begin{pmatrix}\n    1 & x_{1,1} & x_{2,1} & \\dots  & x_{k,1} \\\\\n    1 & x_{1,2} & x_{2,2} & \\dots  & x_{k,2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_{1,n} & x_{2,n} & \\dots  & x_{k,n}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n    b_0 \\\\\n    b_{1}\\\\\n    b_{2}\\\\\n    \\vdots  \\\\\n     b_{k}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    \\hat{Y_1} \\\\\n    \\hat{Y_2}\\\\\n    \\vdots  \\\\\n    \\hat{Y_n}\n\\end{pmatrix}\n\\end{equation}\\]\nThe leftmost matrix is a design matrix where each row corresponds to an individual \\(i\\) and each column corresponds to an explanatory variable. A cell in the \\(i^{th}\\) row and \\(j^{th}\\) column corresponds to individual \\(i\\)’s value for the \\(j^{th}\\) explanatory variable. The first row corresponds to the intercept so all individuals have a value of 1.\nThe middle matrix is a list of the estimated effect sizes for each corresponding explanatory variable.\nThe dot product of these two matrices gives us the predicted value for the response variable for each individual.\n\nGeneral vs Generalized linear models\nThese are often confused!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear models with multiple predictors</span>"
    ]
  },
  {
    "objectID": "multipletesting.html",
    "href": "multipletesting.html",
    "title": "8  Multiple testing issues (and opportunities)",
    "section": "",
    "text": "8.1 What are we doing here?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multiple testing issues (and opportunities)</span>"
    ]
  },
  {
    "objectID": "multipletesting.html#motivating-example",
    "href": "multipletesting.html#motivating-example",
    "title": "8  Multiple testing issues (and opportunities)",
    "section": "8.2 Motivating example",
    "text": "8.2 Motivating example\nYou may have noticed in the plot of penguin body mass in the last chapter that there appeared to be differences between different groups. Let’s look more closely by making a plot of the body masses by color.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multiple testing issues (and opportunities)</span>"
    ]
  },
  {
    "objectID": "GWAS.html",
    "href": "GWAS.html",
    "title": "9  Applying linear models with GWAS",
    "section": "",
    "text": "9.1 What are we doing here?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Applying linear models with GWAS</span>"
    ]
  },
  {
    "objectID": "GWAS.html#motivating-example",
    "href": "GWAS.html#motivating-example",
    "title": "9  Applying linear models with GWAS",
    "section": "9.2 Motivating example",
    "text": "9.2 Motivating example",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Applying linear models with GWAS</span>"
    ]
  },
  {
    "objectID": "lab1.1-likelihood.html",
    "href": "lab1.1-likelihood.html",
    "title": "10  Lab 1 – Likelihood",
    "section": "",
    "text": "10.1 Koala bellows\nThis is a lab to practice likelihood and simple mean-only linear models.\nA koala (photo by David Illif)\nKoalas bellow to attract mates. Charlton et al. studied female response to male bellows by exposing females to loudspeakers playing a bellow from a large male on one side and a small male on the other. They counted the number of times that females looked at each loudspeaker and calculate the preference for large males as the times females looked at the speaker with the large male minus the times looked at the speaker with the small male. (see https://doi.org/10.1016/j.anbehav.2012.09.034 for more)\nHere is data from their paper:\nkoalaprefs = c(-2,2,6,9,13,2,5,7,2,-6,4,3,2,6,-6)\nkoalanames = c(\"Sprite\",\"Nivea\",\"Miniata\",\"Trace\",\"Donatella\",\"Snorkle\",\"Theresa\",\"Crumpet\",\"Yulara\",\"Kat\",\"Crumble\",\"Chassis\",\"Eeyore\",\"Violet\",\"Paddle\")",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lab 1 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "lab1.1-likelihood.html#part-1-likelihood",
    "href": "lab1.1-likelihood.html#part-1-likelihood",
    "title": "10  Lab 1 – Likelihood",
    "section": "10.2 Part 1: Likelihood",
    "text": "10.2 Part 1: Likelihood\n\nTake the sixth koala in your simulated dataset. Her name is Snorkle. What is the likelihood of getting a the data corresponding with Snorkle if the mean preference is 0 and sd is 5?\n\n\nThe eight Koala is named Crumpet. Calculate the likelihood of getting the data for Crumpet and Snorkle if the mean preference is 0 and sd is 5.\n\n\nCalculate the likelihood of all Koalas if the mean preference is 0 and sd is 5.\n\n\nCalculate the log likelihood of all Koalas if the mean preference is 0 and sd is 5.\n\n\nUse a grid search to calculate the maximum likelihood estimate for the mean (assume the sd is 5, use a log likelihood). Make a plot of with mean on the x axis and likelihood values on the y axis.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lab 1 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "lab1.1-likelihood.html#part-2-linear-models",
    "href": "lab1.1-likelihood.html#part-2-linear-models",
    "title": "10  Lab 1 – Likelihood",
    "section": "10.3 Part 2: Linear models",
    "text": "10.3 Part 2: Linear models\n\nLet’s model this data with a very simple linear model. Write an equation describing a model that includes a response variable (preference) as a function of a mean and residual.\nUse lm() to fit this linear model. Does the mean found my lm() match your maximum likelihood estimate from question 5?\n\n\nExamine the residuals from your linear model. What is the name of the Koala with the largest residual?",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lab 1 -- Likelihood</span>"
    ]
  },
  {
    "objectID": "lab4.1-ANOVA.html",
    "href": "lab4.1-ANOVA.html",
    "title": "11  Lab 2 – Categorical Linear Models and ANOVA",
    "section": "",
    "text": "This is a lab to practice categorical linear models and ANOVA\nYou will be simulating data to use in this model.\n\nTrout lilys (Erythronium americanum) have two distinct anther color morphs: yellow or red. The ecological consequences of anther color are unknown. Ripley and colleagues were interested in whether anther color affected pollinator preference. They went to a population of red-anthered flowers and counted the number of pollinator visits to local red flowers, and red and yellow flowers from a population 19km away. They chose these categories to disentangle the effects of color from the effects of novelty.\n(scenario is from https://doi.org/10.17912/micropub.biology.001286)\nWe will start by simulating data under this model using a normal distribution and 3 categories: local red (1), distant red (2), and distant yellow (3). There will be 30 flowers of each type. The code below shows how to simulate this data and combine it into a data frame that we can analyze.\n\nn.1 &lt;- 30\nn.2 &lt;- 30\nn.3 &lt;- 30\nmean.1 &lt;- 29\nmean.2 &lt;- 42\nmean.3 &lt;- 46\nsd.1 &lt;- 6\n\nset.seed(5)\n\n#sample fitnesses based on parameters above (the round function gives us integers)\ngroup1 &lt;- round(\n  rnorm(n=n.1, mean=mean.1, sd=sd.1))\ngroup2 &lt;- round(\n  rnorm(n=n.2, mean=mean.2, sd=sd.1))\ngroup3 &lt;- round(\n  rnorm(n=n.3, mean=mean.3, sd=sd.1))\n\n## but data together in a dataframe\nsimData &lt;- data.frame(resp = c(group1, group2, group3), \n            pop=c(rep('1', n.1), rep('2', n.2),rep('3',n.3)))\n\n#look at it\nplot(jitter(rep(c(0, .5,1), c(n.1, n.2,n.3))), simData$resp, bty=\"n\", xlim=c(-.5,1.5),\n     xlab = \"\", ylab = \"number of visits\", xaxt=\"n\")\naxis(1, at=c(0,.5,1), lab = c('red local','red distant','yellow distant'))\n\n\n\n\n\n\n\n\nQ1: Make a linear model with lm(). How do you interpret the output of the linear model? What do the results tell you about pollinator behavior?\n\n# code goes here\n\nNext, we’ll use this data to do an ANOVA to determine how much flower type in general affects pollinator visitation.\nQ2: in the chunk below, write the code to do an ANOVA using lm(). Check to see if the p value matches the results from the ANOVA done by hand above\n\n#code goes here\n\nQ2.1: (Extra challenge question) Edit the code below to conduct the ANOVA by hand. I’ve included some phrases in []s that you will need to replace with variable names or numbers\n\nlibrary(broom)\n\n## Calculate SS_groups\nSS_groups &lt;- sum((augment([model])$.fitted - mean(simData$resp))^2) ##replace [model] with whatever you called your model from the previous question\n\n## Calculate MS_groups\nMS_groups &lt;- SS_groups/([degrees of freedom])\n\n## Calculate SS_error\nSS_error &lt;- sum((augment([model])$.resid)^2)\n##SS_error &lt;- sum((augment(model1)$resp - augment(model1)$.fitted)^2) THIS ALSO WORKS\n\n## Calculate MS_error\nMS_error &lt;- SS_error/([degrees of freedom])\n\n## Calculate F\nmyF = MS_groups/MS_error\n\n## Do the F test\npf(q=myF, df1 = [degrees of freedom for MS_groups], df2 = [degrees of freedom for MS_error], lower.tail=F)\n\nQ3: Use emmeans to test for pairwise differences between each population. Interpret the ANOVA and emmeans results – what matters most for dtermining pollinator visits?\n\n#code goes here\n\nQ4: Ripley et al. discover a rare purple-anthered morph. They repeat the experiment including the purple anthered morph. Copy and edit the code above to simulate data for an additional category with a mean of 2 visits per flower.\nQ5: Rerun the linear model, ANOVA (with the anova() function), and emmeans on this new data. Interpret the outputs.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lab 2 -- Categorical Linear Models and ANOVA</span>"
    ]
  },
  {
    "objectID": "lab5.1-regression.html",
    "href": "lab5.1-regression.html",
    "title": "12  Lab 3 – Linear Regression",
    "section": "",
    "text": "This is a lab to practice linear regressions and p values\n\n\n\nA diagram showing the accumulation of mutations between two individuals who share a common ancestor. The more time there is since these individual’s most recent shared ancestor, the more time there is for mutations to accumulate between them. Figure is from Graham Coop’s population genetics notes\n\n\nPopulation genetic theory suggests that population size should predict the amount of neutral genetic variation present within species. In a large population, individuals will be more distantly related to each other than individuals in a small population will be. The more distantly related individuals are to each other, the more time there has been for neutral mutations to accumulate between them. Therefore, in large populations, there will be more neutral genetic variation present between individuals. Because of this process, we expect to see a relationship between population size and neutral genetic variation when we look across species.\nYou can download this data from Vince Buffalo’s github. Use the code below to read in the data.\n\nmyData &lt;- read.table('data/buffalo_combined_data.tsv', sep=\"\\t\", header=T) #note the use of sep=\"\\t\" to parse the tab deliminated file\n\nFirst, remove individuals that are missing data for diversity and log10_popsize\nUse lm() to fit a linear model with diversity as the response variable and log10_popsize as the explanatory variable. Look at the output of the model wiht summary(). Make a plot of the data with explanatory variable on the x axis and response variable on the y axis and plot your model fit line on top of the data.\nIdentify which species has the largest residual. Briefly write, in words, what this large residual means.\nMake a plot with the explanatory variable on the x axis and the residuals for each individual on the y axis. Does the residual distribution seem to match the assumptions of a general linear model?\nGo back to the output of summary() for the model? What was the p value associated with the estimation of the effect size of your explanatory variable? Briefly describe below what this p value tells us about the effect size. (Bonus question – how does lm() calculate p values?)\nPick an additional variable from the table that think might affect log10_diversity. Repeat the steps above to construct a linear model estimating the effect of that variable on genetic diversity.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Lab 3 -- Linear Regression</span>"
    ]
  }
]