---
title: "Intro to Linear Models"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library('DT')
```

## What are we doing here?

In this chapter, we will meet the linear model and discuss what it is exactly that we are doing when we build linear models.

We'll build the most basic type of linear model by fitting the mean of a distribution.

We'll also learn to interpret residuals and visualize them with R.

## Linear models

As biologists, we often want to understand if one thing causes another thing.

* Do traits determine an organism's fitness? In other words, are traits under selection?
* Does genotype at this locus determine phenotype?
* Do the amount of resources shape community diversity?
* **get Lauren et al to fill in some other questions**

To answer these types questions, we need a way to carefully relate two variables together. We will refer throughout this course to two types of variables:

1. Explanatory variables, also called independent variables or predictor variables.
2. Response variables, also called dependent variables.

Linear models estimate the conditional mean of the $i^{th}$ observation of a continuous response variable, $\hat{Y}_i$ from a (combination) of value(s) of the explanatory variables ($\text{explanatory variables}_i$): 

\begin{equation} 
\hat{Y}_i = f(\text{explanatory variables}_i)
\end{equation} 

These models are "linear" because we estimate of the conditional mean ($\hat{Y}_i$) by adding up all components of the model. So, each explanatory variable $y_{j,i}$ is multiplied by its effect size $b_j$. It might help to look at an example model below:

\begin{equation} 
\hat{Y}_i = a + b_1  x_{1,i} + b_2 x_{2,i} + \dots{}
\end{equation}

In this example, $\hat{Y}_i$ is estimated as the sum of the "intercept" ($a$), its value for the first explanatory variable ($y_{1,i}$) times the effect of this variable ($b_1$), its value for the second explanatory variable ($y_{2,i}$) times the effect of this variable, $b_2$, and so on for all included explanatory variables.

In practice, fitting a linear model requires picking explanatory variables and then estimating the values of $a$, $b_1$, $b_2$, and so on that best predict the response variables. These estimates then tell us how the explanatory variables relate to the response variable.

## The mean

We are going to start with the simplest linear model possible. You likely already know how to calculate the mean ($\overline{y}$) of a set of data: $\overline{y} = \frac{\sum y_i}{n}$ where $y_i$ is each data point and $n$ is the number of samples.

In the simplest linear model we can also think of the mean as the intercept ($b_0$) so 
we can predict each data point $y_i$ as simply the mean plus an error term or residual ($e_i$).

$$\hat{y}_i = b_0 + e_i$$

### Wait, what's a residual?

Observed values often differ from the predictions made by a linear model.

We define a residual ($e_i$) as the difference between an observed value($Y_i$) and its predicted value from a linear model  ($\hat{y}_i$).


$$e_i = y_i - \hat{y}_i$$

You can also rearrange this to think about it the other way around so that the observed variable ($Y_i$) is the sum of the value predicted by the model ($\hat{y}_i$) and the residual $e_i$.

$$y_i = \hat{y}_i + e_i$$

## Fitting a linear model with maximum likelihood
We will use the principals of likelihood to pick the parameters that best fit the linear model.

Throughout this section we'll be using a dataset of penguin traits. This data should be already available in your version of R, but if it isn't, use the following code to install it.

```{r, eval=F}
install.packages("palmerpenguins")
library("palmerpenguins")
```

You can read more about all the variables available in the package

```{r}
?penguins
```

We'll start by thinking about body size. We want to model each penguin's body mass as the sum of a mean body mass and a residual.

$$\text{penguin body mass} = \text{mean body mass} + e_i$$

### The distribution of residuals
A key piece of our linear model is that the residuals follow a specific distribution. 
In this part of the course we will focus on the normal distribution since it is broadly useful, but you could use any distribution you want.
Models with residuals that follow non-normal distributions are called **Generalized Linear Models** 

In math terms, we would write

$$ e_i \sim N(0, \sigma^2)$$
where $\sigma^2$ is the variance of the residuals. The mean of the residuals is 0. 

We can combine the equation for a linear model with the equation describing the distribution of the variables and get

$$y_i = \hat{y}_i + N(0, \sigma^2)$$

and this simplifies to

$$ y_i \sim N(\hat{y}_i, \sigma^2)$$

For our penguin example

$$ \text{penguin body mass} \sim N(\text{mean body mass}, \sigma^2)$$

### The likelihood of one data point given specific parameters

We can use the dnorm() function in R to calculate the probability of observing a specific penguin body mass given known parameters.

Let's pick a random penguin

```{r}
set.seed(14)
myPenguin = penguins[sample(1:nrow(penguins), 1),]

myPenguin
```

We can calculate the likelihood of our penguin's body mass if $\hat{y}_i = 4500$ and $\sigma^2$ = 40 with the following code:

```{r}
dnorm(myPenguin$body_mass, mean=4500, sd=40)
```

What happens if we change the parameters?


```{r}
dnorm(myPenguin$body_mass, mean=4600, sd=40)
```

Given that our penguin weighs 4650 grams, it is more likely to observe this data if the underlying model has a mean closer to 4650. 


## Calculating the likelihood of many datapoints

We will get better parameter estimates if we have more data -- this is a basic fact of statistics. But, how do we calculate a likelihood of many data points?

Remember from our probability rules that if you want to know the probability of observing two things, you can multiply the probability of observing the first thing times the probability of observing the second thing. And remember that likelihoods are just probabilities

So, if we wanted to calculate the likelihood of observing two penguin body masses for specific parameters, we calculate the likelihood of observing each penguin body mass and multiply those likelihoods together.

```{r}
myPenguins = penguins[sample(1:nrow(penguins), 2),]

myPenguins

dnorm(myPenguins$body_mass[1], mean=4600, sd=40)*dnorm(myPenguins$body_mass[2], mean=4600, sd=40)
```

Since the dnorm() function is vectorized, we can also write this as one line of code.
```{r}
prod(dnorm(myPenguins$body_mass, mean=4600, sd=40))

```

This new code formulation lets us calculate the likelihood for any length vector of observations. We can even calculate the likelihood of observing the entire penguin data set.

```{r}
penguins2 <- penguins |> filter(!is.na(body_mass)) ##filter out ones with NAs
prod(dnorm(penguins2$body_mass, mean=4600, sd=40))
```

Wait, what's going on? How are we getting a probability of 0?

### Log likelihood solves underflow problems

![](figs/comp_on_fire.jpg)

The product of small numbers are smaller numbers. Very very very small numbers cannot be represented in your computer's memory.
This problem is called underflow.

We often deal with underflow by using logs. You may remember from your high school or undergrad math classes that

$$\text{log}(A \times B \times C) = \text{log}(A) + \text{log}(B) + \text{log}(C)$$
\
and more generally:

$$\text{log}\left(\prod\limits_{i=1}^n X_i \right) = \sum\limits_{i=1}^n \text{log}(X_i)$$
Additionally, the log is monotonic which means that

If $X > Y$ then $\text{log}(X) > \text{log}(Y)$

In practice, instead of multiplying likelihoods together to calculate the likelihood of observing a large dataset, we can
sum log likelihoods together. 

```{r}
sum(dnorm(myPenguins$body_mass, mean=4600, sd=40, log=T))

```


### Estimating paramaters with maximum likelihood using grid search.

We aren't just interested in estimating the likelihood for one parameter -- instead we want to find the parameters that give us the highest likelihood of observing our data. One way to do this is with a grid search where we calculate the log likelihood for a grid of parameters and identify the parameter associated with the highest likelihood. We'll do this here for a grid of possible values of $\hat{y}_i$ (we won't mess with $\sigma^2$ here).

```{r}
myGrid <- seq(2000,6000,length.out=100) #make the grid

myLogLikes <- sapply(myGrid,function(m)
  {sum(dnorm(penguins2$body_mass,mean=m,sd=40,log=TRUE))}) ## function to calculate the log likeliood for each value in the grid

myGrid[which.max(myLogLikes)] ## figure out the grid value that corresponds to the maximum likelihood


plot(myGrid, myLogLikes, bty="n", xlab = "mean body weight", ylab = "log likelihood")

```


## Using R's lm() function 

In practice, grid searchers are very inefficient and it is often easier to use premade R functions to fit linear models. 

Below is code for linear model with the penguins data using R's lm() function.


```{r}
model1 = lm(body_mass ~ 1, data = penguins)
model1
```

The output gives us the estimated intercept — which, in this case with no predictors, is simply the mean

We can also use the summarize() function to look more carefully at the model

```{r}
summary(model1)
```

We can update our model from above with our new parameter estimate of the mean.

$$\text{penguin body mass} = 4201.75 + e_i$$



## More on residuals

The residual ($e_i$) for each individual penguin tells us how much that penguin's mass differs from the population mean.


For example, we can look at one specific penguin

```{r}
penguins[1,]

```

This is a male Adelie penguin from Torgersen Island. Its body mass is 3750. We can describe this penguin's mass as

$$3750 = 4201.75 + e_i$$
$e_i = 4201.75-3750 = 451.75$


Below I plot all the body mass data. Each point is a penguin. You can hover over the points to see the body mass of each penguin and the residual.

```{r, eval=F}
#| code-fold: true
#| message: false
#| warning: false
#| label: fig-plotly
#| fig-cap: "An interactive plot showing the mean body size of a penguin. Each point represents an individual penguin, and the dashed red line shows the sample mean across all penguins. Hovering over a point reveals its residual — the difference between the observed value and the mean."
#| fig-alt: "Interactive scatterplot of observed mean body mass of penguins. Points are plotted by index along the x-axis, with body mass on the y-axis. A horizontal dashed red line marks the sample mean. When hovering over a point, the residual (difference between the point’s value and the mean) is displayed."
#| cap-location: margin
library(plotly)
prop_hybrid_plot <- penguins                          |>
  filter(!is.na(body_mass))                         |>
  mutate(i = 1:n(),
         e_i = body_mass - mean(body_mass),
         e_i = round(e_i, digits = 3),
         y_hat_i = round(mean(body_mass),digits=3),
         y_i = round(body_mass, digits = 3))                         |>
  ggplot(aes(x = i, y = y_i, y_hat_i = y_hat_i, e_i = e_i))+
  geom_point(size = 4, alpha = .6)+
  scale_color_manual(values = c("black","darkgreen"))+
  geom_hline(yintercept = 4201.76,
             linetype = "dashed", color = "red", size = 2)+
  labs(y = "Body Mass", title ="This plot is interactive!! Hover over a point to see its residual")+
  theme(legend.position = "none")

ggplotly(prop_hybrid_plot)
```

## Calculating residuals

You can look at residuals and model predictions using the augment() function in the broom package. 
The code below uses augment() to make a table where each row has a penguin's body mass, the expectation 
of body mass from the fitted model, and the residual.


```{r, eval=F}
library(broom)
augment(model1) |> select(body_mass, .fitted, .resid)
                          
```

```{r, echo=F, message=F, warning=F}
library(broom)
library(DT)

  augment(model1) |> 
    select(body_mass, .fitted, .resid) |>
    mutate_all(round, digits = 2)         |>
  datatable(options = list(pageLength = 5))

```
You could also generate residuals without any additional packages by using the following code

```{r}
penguins2 <- penguins |> filter(!is.na(body_mass)) ##filter out ones with NAs
model1Residuals <-  penguins2$body_mass - model1$fitted.values

```
 or even more simply, the lm() output includes the residuals.
 
```{r}
model1Residuals <- model1$residuals 
 
```

## Calculating the $SS_{residual}$ 

TODO explain why if we're doing this 
```{r}
library(broom)
model1       |>
 augment()                |>
 mutate(sq_resid=.resid^2)|>
 summarise(SS=sum(sq_resid))

```


## The mean minimizes $SS_{residual}$


## Practice problems

1. Continuing with the penguin data, we will look at a different variable: bill length. a) write out a linear model of bill length with one paramater for the mean.


b) calculate mean bill length
```{r, echo=F, eval=F}
mean(penguins2$bill_len)

```

c) calculate the likelihood of your data given the mean from part b and a $\sigma^2$ of 5. 
```{r, echo=F, eval=F}
prod(dnorm(penguins2$bill_len, mean=43.92, sd=5, log=F))

```

d) calculate the log likelihood of your data given the mean from part b
```{r, echo=F, eval=F}

sum(dnorm(penguins2$bill_len, mean=43.92, sd=5, log=T))
```

e) calculate the log likelihood for a grid of means of 100 values ranging from 20 to 60. Make a plot showing these values.

```{r, echo=F, eval=F}
myGrid = seq(20,60, length.out=100)

myLogLikes <- sapply(myGrid,function(m)
  {sum(dnorm(penguins2$bill_len,mean=m,sd=5,log=TRUE))}) 

plot(myGrid, myLogLikes, ylab = "log likelihood", xlab = "mean bill length")
```

f) describe in words what the plot below is showing you about your data on bill length