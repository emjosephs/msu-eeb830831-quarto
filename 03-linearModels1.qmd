---
title: "Linear Models 1: The Mean"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
```

## What are we doing here?

In this chapter, we will meet the linear model and discuss what it is exactly that we are doing when we build linear models.

We'll build the most basic type of linear model by fitting the mean of a distribution.

We'll also learn to interpret residuals and visualize them with R.

## Linear models

As biologists, we often want to understand if one thing causes another thing.

* Do traits determine an organism's fitness? In other words, are traits under selection?
* Does genotype at this locus determine phenotype?
* Do the amount of resources shape community diversity?
* **get Lauren et al to fill in some other questions**

To answer these types questions, we need a way to carefully relate two variables together. We will refer throughout this course to two types of variables:

1. Explanatory variables, also called independent variables or predictor variables.
2. Response variables, also called dependent variables.

Linear models estimate the conditional mean of the $i^{th}$ observation of a continuous response variable, $\hat{Y}_i$ from a (combination) of value(s) of the explanatory variables ($\text{explanatory variables}_i$): 

\begin{equation} 
\hat{Y}_i = f(\text{explanatory variables}_i)
\end{equation} 

These models are "linear" because we estimate of the conditional mean ($\hat{Y}_i$) by adding up all components of the model. So, each explanatory variable $y_{j,i}$ is multiplied by its effect size $b_j$. It might help to look at an example model below:

\begin{equation} 
\hat{Y}_i = a + b_1  y_{1,i} + b_2 y_{2,i} + \dots{}
\end{equation}

In this example, $\hat{Y}_i$ is estimated as the sum of the "intercept" ($a$), its value for the first explanatory variable ($y_{1,i}$) times the effect of this variable ($b_1$), its value for the second explanatory variable ($y_{2,i}$) times the effect of this variable, $b_2$, and so on for all included explanatory variables.

In practice, fitting a linear model requires picking explanatory variables and then estimating the values of $a$, $b_1$, $b_2$, and so on that best predict the response variables. These estimates then tell us how the explanatory variables relate to the response variable.

## The mean

We are going to start with the simplest linear model possible. You likely already know how to calculate the mean ($\overline{y}$) of a set of data: $\overline{y} = \frac{\sum y_i}{n}$ where $y_i$ is each data point and $n$ is the number of samples.

In the simplest linear model we can also think of the mean as the intercept ($b_0$) so 
we can predict each data point $y_i$ as simply the mean plus an error term or residual ($e_i$).

$$\hat{y}_i = b_0 + e_i$$

### Wait, what's a residual?

Observed values often differ from the predictions made by a linear model.

We define a residual ($e_i$) as the difference between an observed value($Y_i$) and its predicted value from a linear model  ($\hat{y}_i$).


$$e_i = y_i - \hat{y}_i$$

You can also rearrange this to think about it the other way around so that the observed variable ($Y_i$) is the sum of the value predicted by the model ($\hat{y}_i$) and the residual $e_i$.

$$y_i = \hat{y}_i + e_i$$

## Using R's lm() function 

