---
title: "Linear models with continuous variables"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library('DT')
library(LaCroixColoR)
palette(lacroix_palette("Mango", type = "discrete"))
```

## What are we doing here?

Many of the variables you are interested are continuous, not categorical. In this section we'll extend our linear models to incorporate continuous predictor variables.

## Motivating example

Continuing with the penguin example data, imagine that we are interested in understanding how bill length determines body mass -- do penguins with longer bills also have larger bodies?

```{r}
penguins2 <- penguins |> filter(!is.na(body_mass) & !is.na(bill_len)) ##filter out ones with NAs
plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])

```

We can plot out the data and see that there is potentially a relationship between bill length and body mass. But, imagine we have a biological reason for wanting to model body mass as a function of bill length. How would we do that?

## Linear models with continuous predictors

Instead of letting the mean of the model vary between categories, we now build our model with the mean as a linear function of the predictor variable. This model is called **linear regression** and it has two components: the **deterministic function** and the **stochastic function**

### The deterministic function

The deterministic function describes how the explanatory variables relate to the conditional mean of the response variable. In this case, since this is a linear model, we are modelling the response variable as a line:

$$\hat{y}_i = b_0 + b_1 \times x_i$$
As before, $y_i$ is the conditional mean of the response variable for an individual with value $x_i$, $b_0$ is the intercept or the conditional mean of an individual with a value of $x_i = 0$ (although this value will not be meaningful if it lies far outside the range of the data). 

However, we can think of $b_1$ now as the slope of the relationship between the predictor and response variable. This means that for every increase of 1 unit in $x$, $y$ increases by $b_1$.

### The stochastic function

The **stochastic function** explains how the residuals are distributed around the conditional means predicted by the deterministic function. Since we are working with regular linear models, we will assume that the residuals are normally distributed.

$$ e_i \sim N(0, \sigma^2)$$

We can combine the deterministic and stochastic functions into one model:

$$y_i = \hat{y}_i + N(b_0 + b_1 \times x_i, \sigma^2)$$

## Estimating parameters of the determinstic function

Here is some math for estimating these parameters.

First, we start with the *covariance* between our response variable $y$ and our explanatory variable $x$. The covariance tells us about how much $x$ and $y$ jointly deviate from their means -- a high covariance means that an individual with a high value of $x$ likely also has a high value of $y$ while a low covariance tells us that an individual with a high value of $x$ could have any value of $y$.

We write the covariance of $x$ and $y$ as $\text{cov}_{x,y}$ and calculate it using the following equation:

$$ \text{cov}_{x,y} = \frac{1}{n-1} \sum(x_i - \bar{x})\times(y_i-\bar{y})$$

where $n$ is the number of individuals, $x_i$ and $y_i$ are the values for the $i^{th}$ individual, and $\bar{x}$ and $\bar{y}$ are the mean values of $x$ and $y$. 

Notice that for a covariance, which variable is the explanatory variable and which is the response variable doesn't matter -- you could switch $x$ and $y$ and the equation would remain the same.

Covariance is related to a correlation ($r_{x,y}$), which tells us how reliably $x$ and $y$ covary by standardizing the covariance by how much $x$ and $y$ themselves vary, quantified using the standard deviations of $x$ ($s_x$) and $y$ ($s_y$. 

$$ r_{x,y} = \frac{cov_{x,y}}{s_{x} \times s_{y}}$$

Again, the order of variables doesn't matter. $r_{x,y} = r_{y,x}$

In contrast, linear regression distinguishes between the response and explanatory variables because, here, the goal is to build a model that reduces residuals.

However, the equation for calculating the slope of the linear regression line ($b_1$) looks somewhat similar to the correlation but this time we only standardize by $s_x$.

$$b_1 = \frac{cov_{x,y}}{s^2_x}  =  \frac{\frac{1}{(n-1)}\sum(x_i-\bar{x})(y_i-\bar{y})}{\frac{1}{(n-1)}\sum(x_i-\bar{x})^2}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}$$ 

## Fitting with lm()

Let's try applying a linear model to our penguin data. We'll use the lm() function in R.

$$\text{Body mass}_i = b_0 + b_1 \times \text{Bill length}_i $$

```{r}
model1 <- lm(body_mass ~ bill_len, data=penguins2)
model1
```

Based on these estimates, we can now write out our model as


$$\text{Body mass}_i = 362.31 + 87.42 \times \text{Bill length}_i $$

We can interpret this as for every mm of bill length increase, body mass increases by 87 grams.

We can also estimate $\sigma^2$, the variance of the residuals.

```{r}
sigma(model1)

```

This allows us to write our model including the stochastic function as:

$$ \text{Body mass}_i \sim N(362.31 + 87.42 \times \text{Bill length}_i, 645.43) $$ 

We can also plot the predictions from this model over the raw data.

```{r}
plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])
abline(model1, col=palette()[3], lwd=2)

```


### Residuals

Often we assume that the response data needs to be normally distributed to use a linear model. However, the real assumption of the model is not that the response data is normally distributed, but that the residuals are normally distributed.

Let's look at the residuals from this model
```{r}
library(broom)
myResiduals <- augment(model1) |> select(body_mass, .fitted, .resid)

hist(myResiduals$.resid, main="", xlab = "residuals", border="white", col=palette()[2])
```

It helps me to imagine that the values themselves follow a normal distribution centered around a mean that follows the model predictions.

```{r, echo=F}

par(mfrow=c(1,2))

plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])
abline(model1, col=palette()[3], lwd=2)
abline(v=40, col=palette()[2], lty=2, lwd=2)
abline(v=47, col=palette()[4], lty=2, lwd=2)
abline(v=54, col=palette()[5], lty=2, lwd=2)



#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*40, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[2], freq=F, xlim=c(2000, 7000))

#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*47, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[4], freq=F, xlim=c(2000, 7000))

#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*54, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[5], freq=F, xlim=c(2000, 7000))


### OR

plot(-5,-5, xlim = c(2000,7000), ylim = c(0,0.0006), xlab = "body mass", ylab = "", bty="n")
lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*40,
									sd=sigma(model1)),
                  col = palette()[2], lwd=2, lty=2)

lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*47,
									sd=sigma(model1)),
                  col = palette()[4], lwd=2, lty=2)

lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*54,
									sd=sigma(model1)),
                  col = palette()[5], lwd=2, lty=2)

```


## Review questions

```{r, echo=F, eval=F}
## some noodling to convince myself of stuff

myExp = runif(500)*100
myResp = myExp + rnorm(500, sd=10) 

plot(myExp, myResp)

myModel <- lm(myResp ~ myExp)

sigma(myModel)

```

