---
title: "Linear models with continuous variables"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library('DT')
library(LaCroixColoR)
palette(lacroix_palette("Mango", type = "discrete"))
library(broom)
```

## What are we doing here?

Many of the variables you are interested are continuous, not categorical. In this section we'll extend our linear models to incorporate continuous predictor variables.

## Motivating example

Continuing with the penguin example data, imagine that we are interested in understanding how bill length determines body mass -- do penguins with longer bills also have larger bodies?

```{r}
penguins2 <- penguins |> filter(!is.na(body_mass) & !is.na(bill_len)) ##filter out ones with NAs
plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])

```

We can plot out the data and see that there is potentially a relationship between bill length and body mass. But, imagine we have a biological reason for wanting to model body mass as a function of bill length. How would we do that?

## Linear models with continuous predictors

Instead of letting the mean of the model vary between categories, we now build our model with the mean as a linear function of the predictor variable. This model is called **linear regression** and it has two components: the **deterministic function** and the **stochastic function**

### The deterministic function

The deterministic function describes how the explanatory variables relate to the conditional mean of the response variable. In this case, since this is a linear model, we are modelling the response variable as a line:

$$\hat{y}_i = b_0 + b_1 \times x_i$$
As before, $y_i$ is the conditional mean of the response variable for an individual with value $x_i$, $b_0$ is the intercept or the conditional mean of an individual with a value of $x_i = 0$ (although this value will not be meaningful if it lies far outside the range of the data). 

However, we can think of $b_1$ now as the slope of the relationship between the predictor and response variable. This means that for every increase of 1 unit in $x$, $y$ increases by $b_1$.

### The stochastic function

The **stochastic function** explains how the residuals are distributed around the conditional means predicted by the deterministic function. Since we are working with regular linear models, we will assume that the residuals are normally distributed.

$$ e_i \sim N(0, \sigma^2)$$

We can combine the deterministic and stochastic functions into one model:

$$y_i = \hat{y}_i + N(b_0 + b_1 \times x_i, \sigma^2)$$

## Estimating parameters of the determinstic function

Here is some math for estimating these parameters.

First, we start with the *covariance* between our response variable $y$ and our explanatory variable $x$. The covariance tells us about how much $x$ and $y$ jointly deviate from their means -- a high covariance means that an individual with a high value of $x$ likely also has a high value of $y$ while a low covariance tells us that an individual with a high value of $x$ could have any value of $y$.

We write the covariance of $x$ and $y$ as $\text{cov}_{x,y}$ and calculate it using the following equation:

$$ \text{cov}_{x,y} = \frac{1}{n-1} \sum(x_i - \bar{x})\times(y_i-\bar{y})$$

where $n$ is the number of individuals, $x_i$ and $y_i$ are the values for the $i^{th}$ individual, and $\bar{x}$ and $\bar{y}$ are the mean values of $x$ and $y$. 

Notice that for a covariance, which variable is the explanatory variable and which is the response variable doesn't matter -- you could switch $x$ and $y$ and the equation would remain the same.

Covariance is related to a correlation ($r_{x,y}$), which tells us how reliably $x$ and $y$ covary by standardizing the covariance by how much $x$ and $y$ themselves vary, quantified using the standard deviations of $x$ ($s_x$) and $y$ ($s_y$. 

$$ r_{x,y} = \frac{cov_{x,y}}{s_{x} \times s_{y}}$$

Again, the order of variables doesn't matter. $r_{x,y} = r_{y,x}$

In contrast, linear regression distinguishes between the response and explanatory variables because, here, the goal is to build a model that reduces residuals.

However, the equation for calculating the slope of the linear regression line ($b_1$) looks somewhat similar to the correlation but this time we only standardize by $s_x$.

$$b_1 = \frac{cov_{x,y}}{s^2_x}  =  \frac{\frac{1}{(n-1)}\sum(x_i-\bar{x})(y_i-\bar{y})}{\frac{1}{(n-1)}\sum(x_i-\bar{x})^2}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}$$ 

Note that $x$ and $y$ are no longer interchangeable.

### Some notes on intuition

While the equations for correlation and for the regression coefficient are similar, it's possible to have a large correlation and a small regression slope (and vice versa). 

In general, lots of variation within $x$ will increase $s_x$ and reduce the regression coefficient.

```{r, echo=F}
set.seed(4)
x <- runif(100) #simulate the starting variable
y <- x # simulate the response, slope is 1

y1 <- y + rnorm(n=100,sd=.3) ## add in noise
y2 <- 1.6*x + rnorm(n=100,sd=.36)

#lm(y1~x)
#lm(y~x)
#cor.test(x,y1)
#cor.test(x,y2)
#lm(y2~x)

par(mfrow=c(1,3))
plot(x, y, bty="n", ylab="y", 
     main="(A) r=1, b=1", ylim=c(0,2.5), col="gray"
     )
abline(lm(y~x), col=palette()[2], lwd=2, lty=2)

plot(x, y1, bty="n", ylab="y", 
     main="(B) r=.75, b=1", ylim=c(0,2.5), col="gray"
     )
abline(lm(y1~x), col=palette()[2], lwd=2, lty=2)


plot(x, y2, bty="n", ylab="y", 
     main="(C) r=.75, b=1.5", ylim=c(0,2.5), col="gray"
     )
abline(lm(y2~x), col=palette()[2], lwd=2, lty=2)

```


### Fitting with lm()

Let's try applying a linear model to our penguin data. We'll use the lm() function in R.

$$\text{Body mass}_i = b_0 + b_1 \times \text{Bill length}_i $$

```{r}
model1 <- lm(body_mass ~ bill_len, data=penguins2)
model1
```

Based on these estimates, we can now write out our model as


$$\text{Body mass}_i = 362.31 + 87.42 \times \text{Bill length}_i $$

We can interpret this as for every mm of bill length increase, body mass increases by 87 grams.

We can also estimate $\sigma^2$, the variance of the residuals.

```{r}
sigma(model1)

```

This allows us to write our model including the stochastic function as:

$$ \text{Body mass}_i \sim N(362.31 + 87.42 \times \text{Bill length}_i, 645.43) $$ 

We can also plot the predictions from this model over the raw data.

```{r}
plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])
abline(model1, col=palette()[3], lwd=2)

```


## Residuals

Often we assume that the response data needs to be normally distributed to use a linear model. However, the real assumption of the model is not that the response data is normally distributed, but that the residuals are normally distributed.

Let's look at the residuals from this model
```{r}
library(broom)
myResiduals <- augment(model1) |> select(body_mass, .fitted, .resid)

hist(myResiduals$.resid, main="", xlab = "residuals", border="white", col=palette()[2])
```

It helps me to imagine that the values themselves follow a normal distribution centered around a mean that follows the model predictions.

```{r, echo=F}

par(mfrow=c(1,2))

plot(penguins2$bill_len, penguins2$body_mass, bty="n", ylab = "body mass", xlab = "bill length", lwd=2, col = palette()[1])
abline(model1, col=palette()[3], lwd=2)
abline(v=40, col=palette()[2], lty=2, lwd=2)
abline(v=47, col=palette()[4], lty=2, lwd=2)
abline(v=54, col=palette()[5], lty=2, lwd=2)



#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*40, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[2], freq=F, xlim=c(2000, 7000))

#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*47, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[4], freq=F, xlim=c(2000, 7000))

#hist(rnorm(mean=model1$coefficients[1]+model1$coefficients[2]*54, sd = sigma(model1), n=1000), main="", xlab = "body mass", border="white", col=palette()[5], freq=F, xlim=c(2000, 7000))


### OR

plot(-5,-5, xlim = c(2000,7000), ylim = c(0,0.0006), xlab = "body mass", ylab = "", bty="n")
lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*40,
									sd=sigma(model1)),
                  col = palette()[2], lwd=2, lty=2)

lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*47,
									sd=sigma(model1)),
                  col = palette()[4], lwd=2, lty=2)

lines(seq(2000,7000,length.out=100),
									dnorm(seq(2000,7000,length.out=100),
									mean=model1$coefficients[1]+model1$coefficients[2]*54,
									sd=sigma(model1)),
                  col = palette()[5], lwd=2, lty=2)

```

## Assessing significance

We have different tools for quantifying how confident we can be in the predictions made by a regression.
They vary based on what type of prediction they are making.
We won't work through the math here but it is useful to know what is actually being predicted by these tools.

### Confidence intervals

Confidence intervals relate to predicting the mean value of the response variable $y$ for individuals with a specific value of the explanatory variable $x$. When we calculate *95% confidence intervals* we are bracketing the true regression line 95% of the time.

### Prediction intervals

Prediction intervals give us an estimate of our confidence in predicting a value for the response variable $y$ for *one* individual with a specific value of the explanatory variable $x$.
When we generate *95% prediction intervals* we are bracketing 95% of potential individuals.

Prediction intervals will be much wider than confidence intervals because they include more variation.

### Extrapolation

*Extrapolation* is the prediction of $y$ outside the measured interval of $x$. It's generally a bad idea because we have no way of knowing the relationship between $x$ and $y$ outside of what we've observed.

 ![](figs/extrapolating.png){fig-alt="A carton from xkcd showing a plot with time on the x axis and number of husbands on the y axis with values for 0 and 1 and a line extrapolating. The title reads 'my hobby: extrapolating' and one person tells another person dressed as a bride 'As you can see, by late next month you'll have over four dozen husbands. Better get a bulk rate on a wedding cake'"}

## Linear regression as an ANOVA

We can partition variation in our regression the same way we did in ANOVA. The concepts are similar but the math is slightly different.

```{r varpartregresion, fig.cap = "An ANOVA framework for a linear regression. **A** Shows the difference between each observation, $Y_i$, and the  mean, $\\overline{Y}$. This is the basis for calculating $MS_{total}$.  **B** Shows the difference between each predicted value $\\widehat{Y_i}$ and the mean, $\\overline{Y}$. This is the basis for calculating $MS_{model}$. **C** Shows the difference between each observation, $Y_i$, and its predicted value  $\\widehat{Y_i}$. This is the basis for calculating $MS_{error}$.", fig.height=2.3, fig.width=7.6, echo=FALSE, warning=F, message=F}
library(cowplot)
a <- ggplot(mutate(penguins2, mean_mass = mean(body_mass)) , 
            aes(x = bill_len, y = body_mass))+
  geom_point(alpha = .5)+
  geom_hline(aes(yintercept = mean_mass))+
  geom_segment(aes(xend = bill_len, yend = mean_mass), 
               color = "black", alpha = .5)+
  theme_classic()+ 
  labs(title = "Total deviation               =")

b<-ggplot(mutate(augment(model1), mean_mass = mean(body_mass)) ,
          aes(x = bill_len, y = body_mass))+
  geom_point(alpha = .2)+
  geom_hline(aes(yintercept = mean_mass))+
  geom_segment(aes( xend = bill_len,y = .fitted,  yend = mean_mass ), 
               color = "black", alpha = .5)+
  geom_line(aes(y = .fitted), alpha  = 2, color = palette()[2])+
 theme_classic()+ 
  labs(title = "Model deviation             +")

c<-ggplot(augment(model1) ,
          aes(x = bill_len, y = body_mass))+
  geom_point(alpha = .5)+
  geom_segment(aes( xend = bill_len,  yend = .fitted), 
               color = "black", alpha = .5)+
    geom_line(aes(y = .fitted), alpha  = 2, color = palette()[2])+ 
  theme_classic()+ 
  labs(title = "Error (residual) deviation")

plot_grid(plot_grid(a + theme(legend.position = "none"),
          b + theme(legend.position = "none"),
          c + theme(legend.position = "none"), ncol = 3, labels = c("a","b","c"), label_y = .9),
           get_legend(a+theme(legend.position = "bottom")), 
                      ncol = 1, rel_heights = c(1,.1))
```

As before, we can calculate the total sum of squares ($SS_{total}$, panel A) as the sum of the squared differences between the model and the mean ($SS_{model}$, panel B) and the squared differences between each individual and the model ($SS_{residual}$). Better fitting models will have higher $SS_{model}$ relative to $SS_{residual}$.

We calculate all of these values as follows:
$$SS_{total} = \sum{(y_i - \bar{y})^2}$$
$$SS_{model} = \sum{(\hat{y_i} - \bar{y})^2}$$
$$SS_{residual} = \sum{(y_i - \bar{y})^2} $$
$$MS_{model} = SS_{model}/df_{model}$$
$$MS_{residual} =  SS_{residual}/df_{residual}$$
$$df_{model} = 1 $$
$$df_{residual} = n-2$$

where $\bar{y}$ is the mean response variable for the whole sample and $\hat{y_i}$ is the model predicted value for individual $i$ and there are $n$ individuals in the sample.

We can calculate these values by hand or get them from the anova function

```{r, message=F, warning=F}
library(car)

```

## Review questions

```{r, echo=F, eval=F}
## some noodling to convince myself of stuff

myExp = runif(500)*100
myResp = myExp + rnorm(500, sd=10) 

plot(myExp, myResp)

myModel <- lm(myResp ~ myExp)

sigma(myModel)

```

