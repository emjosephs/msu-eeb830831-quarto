---
title: "Intro to Linear Models"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library('DT')
library(LaCroixColoR)
palette(lacroix_palette("Mango", type = "discrete"))
```

## What are we doing here?

In this chapter, we will meet the linear model and discuss what it is exactly that we are doing when we build linear models.

We'll build the most basic type of linear model by fitting the mean of a distribution.

We'll also learn to interpret residuals and visualize them with R.

## Linear models

As biologists, we often want to understand if one thing causes another thing.

* Do traits determine an organism's fitness? In other words, are traits under selection?
* Does genotype at this locus determine phenotype?
* Do the amount of resources shape community diversity?

To answer these types questions, we need a way to carefully relate two variables together. We will refer throughout this course to two types of variables:

1. Explanatory variables, also called independent variables or predictor variables.
2. Response variables, also called dependent variables.

Linear models estimate the conditional mean of the $i^{th}$ observation of a continuous response variable, $\hat{Y}_i$ from a (combination) of value(s) of the explanatory variables ($\text{explanatory variables}_i$): 

\begin{equation} 
\hat{Y}_i = f(\text{explanatory variables}_i)
\end{equation} 

These models are "linear" because we estimate of the conditional mean ($\hat{Y}_i$) by adding up all components of the model. So, each explanatory variable $x_{j,i}$ is multiplied by its effect size $b_j$. It might help to look at an example model below:

\begin{equation} 
\hat{Y}_i = b_0 + b_1  x_{1,i} + b_2 x_{2,i} + \dots{}
\end{equation}

In this example, $\hat{Y}_i$ is estimated as the sum of the "intercept" ($b_0$), its value for the first explanatory variable ($x_{1,i}$) times the effect of this variable ($b_1$), its value for the second explanatory variable ($x_{2,i}$) times the effect of this variable, $b_2$, and so on for all included explanatory variables.

In practice, fitting a linear model requires picking explanatory variables and then estimating the values of $a$, $b_1$, $b_2$, and so on that best predict the response variables. These estimates then tell us how the explanatory variables relate to the response variable.

## The mean

We are going to start with the simplest linear model possible. You likely already know how to calculate the mean ($\overline{y}$) of a set of data: $\overline{y} = \frac{\sum y_i}{n}$ where $y_i$ is each data point and $n$ is the number of samples.

In the simplest linear model we can also think of the mean as the intercept ($b_0$) so 
we can predict each data point $y_i$ as simply the mean plus an error term or residual ($e_i$).

$$\hat{y}_i = b_0 + e_i$$

### Wait, what's a residual?

Observed values often differ from the predictions made by a linear model.

We define a residual ($e_i$) as the difference between an observed value($Y_i$) and its predicted value from a linear model  ($\hat{y}_i$).


$$e_i = y_i - \hat{y}_i$$

You can also rearrange this to think about it the other way around so that the observed variable ($Y_i$) is the sum of the value predicted by the model ($\hat{y}_i$) and the residual $e_i$.

$$y_i = \hat{y}_i + e_i$$

## Fitting a linear model with maximum likelihood
We will use the principals of likelihood to pick the parameters that best fit the linear model.

Throughout this section we'll be using a dataset of penguin traits. This data should be already available in your version of R, but if it isn't, use the following code to install it.

```{r, eval=F}
install.packages("palmerpenguins")
library("palmerpenguins")
```

You can read more about all the variables available in the package

```{r, eval=F}
?penguins
```

```{r}
#| echo: false
#| column: margin
#| fig-alt: "A cartoon of a penguin flying as other penguins watch on says 'we just haven't been flapping them hard enough"
#| fig-cap: "A cartoon by Sam Gross" 
library(knitr)
include_graphics("figs/penguincartoon.png")
```


We'll start by thinking about body size. We want to model each penguin's body mass as the sum of a mean body mass and a residual.

$$\text{penguin body mass} = \text{mean body mass} + e_i$$

### The distribution of residuals
A key piece of our linear model is that the residuals follow a specific distribution. 
In this part of the course we will focus on the normal distribution since it is broadly useful, but you could use any distribution you want.
Models with residuals that follow non-normal distributions are called **Generalized Linear Models** 

In math terms, we would write

$$ e_i \sim N(0, \sigma^2)$$
where $\sigma^2$ is the variance of the residuals. The mean of the residuals is 0.

:::fyi
Remember that the standard deviation is the squsre root of the variance and the standard error is the standard deviation divided by the square root of the sample size: $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$

:::


We can combine the equation for a linear model with the equation describing the distribution of the variables and get

$$y_i = b_0 + N(0, \sigma^2)$$

and this simplifies to

$$ y_i \sim N(b_0, \sigma^2)$$

For our penguin example

$$ \text{penguin body mass} \sim N(\text{mean body mass}, \sigma^2)$$

Note that a linear model has two components: a deterministic component and a stochastic component. The deterministic component tells us about how the explanatory variable relates to the model prediction for the response variable. The stochastic component tells us about how the residuals are distributed.

In this case, the deterministic equation tells us that the predicted value of body mass is mean body mass ($\hat{y} = b_0$) and the stochastic equation tells us that the residuals are normally distributed ($e_i \sim N(0,\sigma^2$)

## Using maximum likelihood to estimate parameters in a linear model

Remember from our earlier lectures that we can use likelihood to estimate the parameters of a distribution. The code below calculates the log likelihood of observing our data on penguin body mass for specific parameters of the normal distribution.

```{r}
penguins2 <- penguins |> filter(!is.na(body_mass)) ##filter out ones with NAs
sum(dnorm(penguins2$body_mass, mean=4600, sd=40, log=T))

```

### Estimating paramaters with maximum likelihood using grid search.

As we learned in our lecture on likelihoods, we can use a grid search to find the MLE. We'll do this here for a grid of possible values of $\hat{y}_i$ (we won't mess with $\sigma^2$ here).

```{r}
myGrid <- seq(2000,6000,length.out=100) #make the grid

myLogLikes <- sapply(myGrid,function(m)
  {sum(dnorm(penguins2$body_mass,mean=m,sd=40,log=TRUE))}) ## function to calculate the log likeliood for each value in the grid

myGrid[which.max(myLogLikes)] ## figure out the grid value that corresponds to the maximum likelihood


plot(myGrid, myLogLikes, bty="n", xlab = "mean body weight", ylab = "log likelihood")

```


## Using R's lm() function 

In practice, grid searchers are very inefficient and it is often easier to use premade R functions to fit linear models. 

Below is code for linear model with the penguins data using R's lm() function.


```{r}
model1 = lm(body_mass ~ 1, data = penguins)
model1
```

The output gives us the estimated intercept — which, in this case with no predictors, is simply the mean

We can also use the summarize() function to look more carefully at the model

```{r}
summary(model1)
```

We can update our model from above with our new parameter estimate of the mean.

$$\text{penguin body mass} = 4201.75 + e_i$$



## Residuals

The residual ($e_i$) for each individual penguin tells us how much that penguin's mass differs from the population mean.


For example, we can look at one specific penguin

```{r}
penguins[1,]

```

This is a male Adelie penguin from Torgersen Island. Its body mass is 3750. We can describe this penguin's mass as

$$3750 = 4201.75 + e_i$$
$e_i = 4201.75-3750 = 451.75$


Below I plot all the body mass data. Each point is a penguin. You can hover over the points to see the body mass of each penguin and the residual.

```{r, eval=T}
library(plotly)
#| code-fold: true
#| message: false
#| warning: false
#| label: fig-plotly
#| fig-cap: "An interactive plot showing the mean body size of a penguin. Each point represents an individual penguin, and the dashed red line shows the sample mean across all penguins. Hovering over a point reveals its residual — the difference between the observed value and the mean."
#| fig-alt: "Interactive scatterplot of observed mean body mass of penguins. Points are plotted by index along the x-axis, with body mass on the y-axis. A horizontal dashed red line marks the sample mean. When hovering over a point, the residual (difference between the point’s value and the mean) is displayed."
#| cap-location: margin
resid_plot <- penguins                          |>
  filter(!is.na(body_mass))                         |>
  mutate(i = 1:n(),
         e_i = body_mass - mean(body_mass),
         e_i = round(e_i, digits = 3),
         y_hat_i = round(mean(body_mass),digits=3),
         y_i = round(body_mass, digits = 3))                         |>
  ggplot(aes(x = i, y = y_i, y_hat_i = y_hat_i, e_i = e_i))+
  geom_point(size = 4, alpha = .6)+
  scale_color_manual(values = c("black","darkgreen"))+
  geom_hline(yintercept = 4201.76,
             linetype = "dashed", color = "red", size = 2)+
  labs(y = "Body Mass", title ="This plot is interactive!! Hover over a point to see its residual")+
  theme(legend.position = "none")

ggplotly(resid_plot)
```

### Calculating residuals

You can look at residuals and model predictions using the augment() function in the broom package. 
The code below uses augment() to make a table where each row has a penguin's body mass, the expectation 
of body mass from the fitted model, and the residual.


```{r, eval=F}
library(broom)
augment(model1) |> select(body_mass, .fitted, .resid)
                          
```

```{r, echo=F, message=F, warning=F}
library(broom)
library(DT)

  augment(model1) |> 
    select(body_mass, .fitted, .resid) |>
    mutate_all(round, digits = 2)         |>
  datatable(options = list(pageLength = 5))

```
You could also generate residuals without any additional packages by using the following code

```{r}
model1Residuals <-  penguins2$body_mass - model1$fitted.values

```
 or even more simply, the lm() output includes the residuals.
 
```{r}
model1Residuals <- model1$residuals 
 
```

### Calculating the $\text{SS}_{\text{residual}}$

We can quantify how well our model fits the data using the residuals, in particular by calculating the sum of squared residuals ($\text{SS}_{\text{residual}}$)
```{r}
library(broom)
ss_resid <-model1       |> 
 augment()                |> #use augment to calculate residuals
 mutate(sq_resid=.resid^2)|> # use mutate to calculate squared residuals
 summarise(SS=sum(sq_resid)) #use summarise to sum the residuals

ss_resid
```


### The mean minimizes $\text{SS}_{\text{residual}}$

We know how to calculate the mean of our data. But, imagine that we didn't and we wanted to figure out a way to 'best' summarize our data. One way to do this is to find a measure that minimizes the distance between our observed data and the predictions made by the model, which means minimizing the residuals. In practice, this means minimizing $\text{SS}_{\text{residual}}$

Above, we calculated $\text{SS}_{\text{residual}}$ for a mean of 4202. The following code calculates $\text{SS}_{\text{residual}}$ for a model with a mean of 5000.

```{r}
ss_resid2 <- model1 |>
  augment() |>
mutate(sq_resid2 = (5000-body_mass)^2) |> #calculate new residuals based on a "mean " of 5000
summarise(SS2 = sum(sq_resid2))

ss_resid2

ss_resid2 > ss_resid
```

So the sum of squares for residuals from 5000 is much larger than the sum of squares for residuals from 4202, the mean.

The plot below shows potential means on the x axis and $\text{SS}_{\text{residual}}$ on the Y axis

```{r, echo=F}
calcSS <- function(x, m){
  m |>
  augment() |>
mutate(sq_resid2 = (x-body_mass)^2) |> 
summarise(SS2 = sum(sq_resid2))
}

myX = seq(2000,6000, length.out=100)
mySS <- sapply(myX, function(x){
  calcSS(x,model1)
})

plot(myX, mySS, bty="n", xlab = "mean", ylab = "SS_resid")
abline(v = 4202, lwd=2, lty=2, col=palette())

```

## Practice problems

Continuing with the penguin data, we will look at a different variable: bill length. 

Here is an equation for a linear model:

$$ \text{Bill length}_i = b_0 + e_i\ $$

1. Describe what $b_0$ and $e_i$ represent.


Here is a different way of writing the linear model

$$ \text{Bill length}_i \sim N(b_0, \sigma^2) $$

2. Which part of the above equation is the deterministic part of the model? Which part is the stochastic part of the model?




```{r, echo=F}
model3 <- lm(bill_len~1, data=penguins)
plot(1:344, penguins$bill_len, bty="n", col = 'gray', xlab = "penguin number", ylab = "bill length", pch=20)
abline(model3, col=palette()[3], lty=2,lwd=2)
points(186, penguins$bill_len[186], col=palette()[3], pch=17, cex=2)
points(100, penguins$bill_len[100], col=palette()[3], pch=15, cex=2)
points(250, penguins$bill_len[250], col=palette()[3], pch=8, cex=2, lwd=2)

```

3.The plot above shows the data from our model. Each point represents a penguin and three penguins are highlighted in dark blue by either a square, triangle, or star. Which penguin has the largest residual?

4. Which of the following pieces of code calculates the log likelihood of the model above?
a. 
```{r, eval=F}
sum(dnorm(penguins$bill_len, mean=44, sd=5, log=T))
```
b.
```{r, eval=F}
prod(dnorm(penguins$bill_len, mean=44, sd=5))
```
c.
```{r, eval=F}
prod(dnorm(penguins$body_mass, mean=44, sd=5))
```
d.
```{r, eval=F}
sum(dnorm(penguins$bill_len, mean=500, sd=5, log=T))
```


