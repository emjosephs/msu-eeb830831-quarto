---
title: "Multiple testing issues (and opportunities)"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library('DT')
library(LaCroixColoR)
palette(lacroix_palette("Mango", type = "discrete"))
library(car)
```

## What are we doing here?



## Motivating example

Imagine that we are interested in a lot of things, so we do a lot of statistical tests. However, remember that the definition of a p value is that it is the likelihood of observing our data *if* the null hypothesis is true. If we do a lot of statistical tests, we increase our chances of observing data that looks unlikely to fit the null. 

The code below demonstrates the problem. The `doExp()` function samples 100 random numbers from a uniform distribution to generate data for an explanatory variable and another 100 random numbers from a uniform distribution to generate data for a response variable. Then it fits a linear model testing if the explanatory variable predicts the response variable. We know, from the code, that the null hypothesis should be true -- there should be no relationship between these two variables. 

However, let's look at the distribution of p values when we run the `doExp()` function 200 times

```{r}

set.seed(830)

doExp <- function(i){
  myX <- runif(100) #sample 100 random numbers
  myY <- runif(100) #sample 100 random numbers
  myModel = lm(myY ~ myX) #fit linear model
  myP = summary(myModel)$coefficients[2,4] #get p value
  return(myP)
  }

myPValues <- sapply(1:200, doExp) ## run doExp 200 times


hist(myPValues, col = palette()[4], border="white", main="", xlab = "pvalues", breaks=20)
abline(v=0.05, col=palette()[2], lwd=2,lty=2)

```

As you can see from the histogram, 10 out of our 200 samples had significant p values for the relationship between our explanatory and response variables, even though these variables were generated from independent processes. This is not just bad luck -- it is exactly what is expected for a p value cut off of 0.05! Five percent of the time, we saw a pattern indicating that the explanatory and response variables were related to each other, even though the null hypothesis was true.

## "Correcting" for multiple testing

If a specific analysis or set of analyses does a large number of statistical tests, then there is a larger chance of a **Type I** error or **false positive**. There are a number of approaches that scientists use to try to account for this inflated false positive rate.

### Bonferroni correction

### False discovery rate

## When do you need to correct?

Data exploration vs. conclusively establishing effects