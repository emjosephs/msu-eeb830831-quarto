---
title: "Multiple testing issues (and opportunities)"
format: html
editor: source
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library('DT')
library(LaCroixColoR)
palette(lacroix_palette("Mango", type = "discrete"))
library(car)
```


## Motivating example

Imagine that we are interested in a lot of things, so we do a lot of statistical tests. However, remember that the definition of a p value is that it is the likelihood of observing our data *if* the null hypothesis is true (a **type I error**). If we do a lot of statistical tests, we increase our chances of observing data that looks unlikely to fit the null even if the null hypothesis is true.

The code below demonstrates the problem. The `doExp()` function samples 100 random numbers from a uniform distribution to generate data for an explanatory variable and another 100 random numbers from a uniform distribution to generate data for a response variable. Then it fits a linear model testing if the explanatory variable predicts the response variable. We know, from the code, that the null hypothesis should be true -- there should be no relationship between these two variables. 

However, let's look at the distribution of p values when we run the `doExp()` function 200 times

```{r}

set.seed(830)

doExp <- function(i){
  myX <- runif(100) #sample 100 random numbers
  myY <- runif(100) #sample 100 random numbers
  myModel = lm(myY ~ myX) #fit linear model
  myP = summary(myModel)$coefficients[2,4] #get p value
  return(myP)
}

nTests=200

myPValues <- sapply(1:nTests, doExp) ## run doExp 200 times


hist(myPValues, col = palette()[4], border="white", main="", xlab = "pvalues", breaks=20)
abline(v=0.05, col=palette()[2], lwd=2,lty=2)

```

Ten out of our 200 samples had significant p values for the relationship between our explanatory and response variables, even though these variables were generated from independent processes. This is not just bad luck -- it is exactly what is expected for a p value cut off of 0.05! Five percent of the time, we saw a pattern indicating that the explanatory and response variables were related to each other, even though the null hypothesis was true.

## "Correcting" for multiple testing

If a specific analysis or set of analyses does a large number of statistical tests, then there is a larger chance of a **Type I** error or **false positive**. The statistical term for the chance of detecting one or more false positives across multiple tests is the **family-wise error rate**. There are a number of approaches that scientists use to try to account for this inflated false positive rate.

:::fyi
**What is a family?**
Opinions differ on the correct level to do multiple testing control on? Do you want to control family-wide error rate for a specific analysis where you conduct multiple tests? Or across all the tests contained within a single paper? I've heard some people joke that scientists themselves should have a lifetime family-wise error test rate, which, as an evolutionary genomics scientist, is a scary prospect!
:::

### Bonferroni correction

The **Bonferroni correction** divides the $\alpha$ value by the number of tests conducted to adjust for the increase in the family-wise error rate that you get by doing multiple tests. So if you're conducting 10 statistical tests with an $\alpha$ of 0.05, with a Bonferroni correction you need $p < 0.005$ to determine significance.

In our example from before, we conducted 200 test with an $\alpha$ value of 0.05. A Bonferroni correction would require a p value of 0.00025 to be considered significant. The code below checks if any of the p values from the simulations are below the Bonferroni-corrected threshold.

```{r}
newAlpha = 0.05/nTests
sum(myPValues < newAlpha)

```

None of them are. Hooray, have we solved the problem?

Some benefits of the Bonferroni correction:
- You can use it to combine all different types of tests into one family-wide error rate control, as long as they use p values.
- It's very easy to use.

However, the Bonferroni correction has a big problem: it very strongly reduces statistical power to detect true associations, essentially increasing rates of **type II error** or **false negatives**. My own PhD advisor would say "never disrespect your data by using a Bonferroni correction".

One thing that helps me think through concerns with the Bonferroni is that no matter what your distribution of p values, the Bonferroni is the same. However, in reality, our own expectations of the underlying false positive and false negative rate my change depending on whether the distribution of p values is flat, right skewed, or left skewed as in @fig-p-val-dists.

```{r, echo=FALSE}
#| label: fig-p-val-dists
#| fig-cap: "Three potential p value distributions that could result from 200 tests."
#| fig-alt: "Three panels showing histograms of values from 0 to 1 with a dotted vertical line at 0.05. On the left is a relatively flat distribution, in the center is a distribution skewed to the left so that many values are below 0.05. On the left is a right skewed distribution so that new values are below 0.05"
par(mfrow=c(1,3), mar=c(2,2,2,2))
hist(runif(200), col = palette()[4], border="white", main="", xlab = "pvalues", breaks=20)
abline(v=0.05, col=palette()[2], lwd=2,lty=2)
hist(rnorm(n=200,mean=0.05, sd=0.4), col = palette()[4], border="white", main="", xlab = "pvalues", breaks=40, xlim=c(0,1))
abline(v=0.05, col=palette()[2], lwd=2,lty=2)
hist(rnorm(n=200,mean=0.9, sd=0.4), col = palette()[4], border="white", main="", xlab = "pvalues", breaks=40, xlim=c(0,1))
abline(v=0.05, col=palette()[2], lwd=2,lty=2)

```

### False discovery rate

Instead of Bonferroni, we can use the information about our distribution of p values to estimate, for a given p value cut-off, the proportion of tests that are true positives and false positives. This estimate is our **false discovery rate** (**FDR**) and we can then modify our p-value cut-off to match a desired FDR.

```{r}
#| echo: false
#| column: margin
#| fig-alt: "President Roosevelt in his wheelchair on the porch at Top Cottage in Hyde Park, NY with a child and a dog."
#| fig-cap: "Not this FDR (FDR Presidential Library & Museum)"
library(knitr)
include_graphics("figs/FDR.jpg")
```

The idea behind an FDR is that when you conduct a bunch of tests and look at the distribution of p values, you are really looking at a mixture of two distributions: a distribution of p values from tests that fit the null hypothesis and a distribution of p values from tests that do not fit the null hypothesis. As you move your p-value cut-off closer and closer to 0, the collection of p values below that cut-off contains more and more p-values from tests that do not result from the null-hypothesis, reducing the FDR (but increasing your type II error rate.)

The plot below shows this idea, with a mix of the distribution from tests generated under the null hypothesis and an alternative hypothesis. The two vertical lines show potential p value cut-offs corresponding to potential FDRs.

```{r}
#| label: fig-fdr
#| fig-cap: "A mixture of p value distributions from tests based on the null and alternative hypotheses."
#| fig-alt: "A stacked histogram ranging from 0 to 1 of a flat distribution (tagged null) and a left-skewed distribution (tagged alternate). Two vertical lines at 0.05 and 0.01."
par(mar=c(5,5,2,2), mfrow=c(1,1))
p_null = runif(n=500)
p_alt = rgamma(n=500, shape = 1, scale=2)/20

hist(c(p_alt,p_null), border="white", col=palette()[3], breaks = seq(0,1,by=0.01), main="", xlab=c('p values'))
hist(p_null, border="white",add=TRUE, col=palette()[2],seq(0,1,by=0.01))
legend('topright',c('null','alternate'),fill=palette()[2:3],bty="n", border="white")

abline(v=0.05, col=palette()[1], lwd=2,lty=2)
abline(v=0.01, col=palette()[5], lwd=2,lty=2)


```

The FDR has been widely adopted across fields, from genetics to social science research. R has a nice build in function called `p.adjust` for calculating a false discovery rate. The code below applies it to our simulated p values from the beginning of the lesson.

```{r}
fdrs <- p.adjust(myPValues, method="fdr") #calculate FDRs
summary(fdrs)

myPValues[which.min(fdrs)]

```

The code above calcuates the FDR and finds that the smallest FDR in this set is 0.22, corresponding to a p value of 0.001. This means that in this data set, a p value of 0.001 has a 22% chance of being a false positive. 
